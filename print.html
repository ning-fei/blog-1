<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>blog</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="about.html"><strong aria-hidden="true">1.</strong> About</a></li><li class="chapter-item expanded "><a href="tensorflow/index.html"><strong aria-hidden="true">2.</strong> tensorflow inside</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tensorflow/executor.html"><strong aria-hidden="true">2.1.</strong> Executor: 执行Computation Sub Graph</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tensorflow/executor-subgraph-preprocess.html"><strong aria-hidden="true">2.1.1.</strong> SubGraph预处理：Node/NodeItem/TaggedNode</a></li><li class="chapter-item expanded "><a href="tensorflow/flow-control-op.html"><strong aria-hidden="true">2.1.2.</strong> Flow control op: switch/merge/enter/exit/nextIteration</a></li><li class="chapter-item expanded "><a href="tensorflow/executor-frame.html"><strong aria-hidden="true">2.1.3.</strong> Frame: ControlFlowInfo/FrameInfo/FrameState/IterationState</a></li></ol></li><li class="chapter-item expanded "><a href="tensorflow/direct-session.html"><strong aria-hidden="true">2.2.</strong> DirectSession: 单机执行computation graph</a></li><li class="chapter-item expanded "><a href="tensorflow/rendezvous.html"><strong aria-hidden="true">2.3.</strong> RendezVous：跨设备，跨主机通信</a></li><li class="chapter-item expanded "><a href="tensorflow/device.html"><strong aria-hidden="true">2.4.</strong> Device：计算单元抽象(CPU/GPU)</a></li></ol></li><li class="chapter-item expanded "><a href="tensorflow/optimize.html"><strong aria-hidden="true">3.</strong> tensorflow模型对接工程优化</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tensorflow/export-keras-model-as-tf-frozen-graph.html"><strong aria-hidden="true">3.1.</strong> 将keras模型导出为tf frozen graph</a></li><li class="chapter-item expanded "><a href="tensorflow/replace-placeholder-with-iterator.html"><strong aria-hidden="true">3.2.</strong> 使用dataset iterator 优化keras model预测的吞吐量</a></li><li class="chapter-item expanded "><a href="tensorflow/stat-cpu-gpu-load.html"><strong aria-hidden="true">3.3.</strong> 统计cpu/gpu 负载率脚本</a></li></ol></li><li class="chapter-item expanded "><a href="pthread/index.html"><strong aria-hidden="true">4.</strong> pthread</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="pthread/pthread-primer.html"><strong aria-hidden="true">4.1.</strong> Pthread Primer笔记</a></li><li class="chapter-item expanded "><a href="pthread/glibc-pthread-implement-thread-life-cycle.html"><strong aria-hidden="true">4.2.</strong> Pthread线程生命周期</a></li><li class="chapter-item expanded "><a href="pthread/glibc-pthread-implement-sync.html"><strong aria-hidden="true">4.3.</strong> Pthread线程同步</a></li></ol></li><li class="chapter-item expanded "><a href="react/index.html"><strong aria-hidden="true">5.</strong> react</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="react/from-jsx-to-dom.html"><strong aria-hidden="true">5.1.</strong> 从jsx到html dom的流程分析</a></li></ol></li><li class="chapter-item expanded "><a href="java/index.html"><strong aria-hidden="true">6.</strong> hotspot</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="java/hotspot-debug-under-osx.html"><strong aria-hidden="true">6.1.</strong> osx下编译调试hotspot</a></li><li class="chapter-item expanded "><a href="java/hotspot-thread-created-when-init.html"><strong aria-hidden="true">6.2.</strong> jvm的初始化时创建的线程</a></li><li class="chapter-item expanded "><a href="java/hotspot-class-file-load-and-run.html"><strong aria-hidden="true">6.3.</strong> class文件的加载和执行</a></li></ol></li><li class="chapter-item expanded "><a href="golang/index.html"><strong aria-hidden="true">7.</strong> Go</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="golang/pgm.html"><strong aria-hidden="true">7.1.</strong> Runtime PGM调度模型</a></li><li class="chapter-item expanded "><a href="golang/goroutine-stack.html"><strong aria-hidden="true">7.2.</strong> Goroutine Stack</a></li><li class="chapter-item expanded "><a href="golang/memory.html"><strong aria-hidden="true">7.3.</strong> Memory分配</a></li><li class="chapter-item expanded "><a href="golang/GC.html"><strong aria-hidden="true">7.4.</strong> GC</a></li><li class="chapter-item expanded "><a href="golang/context.html"><strong aria-hidden="true">7.5.</strong> Context</a></li><li class="chapter-item expanded "><a href="golang/defer-panic-recover.html"><strong aria-hidden="true">7.6.</strong> defer-panic-recover</a></li></ol></li><li class="chapter-item expanded "><a href="kafka/index.html"><strong aria-hidden="true">8.</strong> Kafka</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="kafka/client-producer.html"><strong aria-hidden="true">8.1.</strong> client: producer</a></li><li class="chapter-item expanded "><a href="kafka/group-coordinator.html"><strong aria-hidden="true">8.2.</strong> group coordinator</a></li><li class="chapter-item expanded "><a href="kafka/kafka-produce-fetch.html"><strong aria-hidden="true">8.3.</strong> produce and fetch</a></li><li class="chapter-item expanded "><a href="kafka/log.html"><strong aria-hidden="true">8.4.</strong> log</a></li><li class="chapter-item expanded "><a href="kafka/partition.html"><strong aria-hidden="true">8.5.</strong> Partition</a></li><li class="chapter-item expanded "><a href="kafka/controller-main.html"><strong aria-hidden="true">8.6.</strong> Controller</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="kafka/controller-channel-manager.html"><strong aria-hidden="true">8.6.1.</strong> 通信管理 channelManager</a></li><li class="chapter-item expanded "><a href="kafka/controller-elect.html"><strong aria-hidden="true">8.6.2.</strong> 选举</a></li><li class="chapter-item expanded "><a href="kafka/controller-zk.html"><strong aria-hidden="true">8.6.3.</strong> zk监听处理</a></li></ol></li><li class="chapter-item expanded "><a href="kafka/replica-assignment.html"><strong aria-hidden="true">8.7.</strong> 副本迁移</a></li><li class="chapter-item expanded "><a href="kafka/paritition-replica-statemachine.html"><strong aria-hidden="true">8.8.</strong> Partition/Replica状态机</a></li><li class="chapter-item expanded "><a href="kafka/txn_coordinator.html"><strong aria-hidden="true">8.9.</strong> 事务</a></li><li class="chapter-item expanded "><a href="kafka/stream.html"><strong aria-hidden="true">8.10.</strong> Stream</a></li></ol></li><li class="chapter-item expanded "><a href="leveldb/index.html"><strong aria-hidden="true">9.</strong> LevelDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="leveldb/draft.html"><strong aria-hidden="true">9.1.</strong> draft</a></li><li class="chapter-item expanded "><a href="leveldb/code-struct.html"><strong aria-hidden="true">9.2.</strong> 代码模块间关系</a></li><li class="chapter-item expanded "><a href="leveldb/write.html"><strong aria-hidden="true">9.3.</strong> Write 流程</a></li><li class="chapter-item expanded "><a href="leveldb/read.html"><strong aria-hidden="true">9.4.</strong> Read 流程</a></li><li class="chapter-item expanded "><a href="leveldb/table-format.html"><strong aria-hidden="true">9.5.</strong> SSTable 文件格式和读写</a></li><li class="chapter-item expanded "><a href="leveldb/versionset.html"><strong aria-hidden="true">9.6.</strong> versionset和Manifest</a></li><li class="chapter-item expanded "><a href="leveldb/table-compact.html"><strong aria-hidden="true">9.7.</strong> Do Compact</a></li><li class="chapter-item expanded "><a href="leveldb/iterator.html"><strong aria-hidden="true">9.8.</strong> Iterator迭代器</a></li><li class="chapter-item expanded "><a href="leveldb/bloom-filter.html"><strong aria-hidden="true">9.9.</strong> Bloom filter</a></li></ol></li><li class="chapter-item expanded "><a href="rocksdb/index.html"><strong aria-hidden="true">10.</strong> RocksDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rocksdb/draft.html"><strong aria-hidden="true">10.1.</strong> draft</a></li><li class="chapter-item expanded "><a href="rocksdb/column-family.html"><strong aria-hidden="true">10.2.</strong> 主要struct引用关系</a></li><li class="chapter-item expanded "><a href="rocksdb/wal.html"><strong aria-hidden="true">10.3.</strong> Write Ahead Log</a></li><li class="chapter-item expanded "><a href="rocksdb/write.html"><strong aria-hidden="true">10.4.</strong> write 并发控制</a></li><li class="chapter-item expanded "><a href="rocksdb/flush-and-compact.html"><strong aria-hidden="true">10.5.</strong> 后台flush和compact线程</a></li><li class="chapter-item expanded "><a href="rocksdb/leveled-compaction-picker.html"><strong aria-hidden="true">10.6.</strong> Leveled Compaction Picker</a></li><li class="chapter-item expanded "><a href="rocksdb/read.html"><strong aria-hidden="true">10.7.</strong> read 流程</a></li><li class="chapter-item expanded "><a href="rocksdb/blob.html"><strong aria-hidden="true">10.8.</strong> Blob</a></li><li class="chapter-item expanded "><a href="rocksdb/transaction.html"><strong aria-hidden="true">10.9.</strong> 事务</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rocksdb/optimistic-transaction.html"><strong aria-hidden="true">10.9.1.</strong> Optimistic Transaction</a></li><li class="chapter-item expanded "><a href="rocksdb/transaction-lock-mgr.html"><strong aria-hidden="true">10.9.2.</strong> Transaction lock mgr</a></li><li class="chapter-item expanded "><a href="rocksdb/two-phase-commit.html"><strong aria-hidden="true">10.9.3.</strong> two phase commit</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="tikv/index.html"><strong aria-hidden="true">11.</strong> TiKV</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tikv/draft/index.html"><strong aria-hidden="true">11.1.</strong> draft</a></li></ol></li><li class="chapter-item expanded "><a href="raft/index.html"><strong aria-hidden="true">12.</strong> Raft</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="raft/raft.html"><strong aria-hidden="true">12.1.</strong> raft paper</a></li><li class="chapter-item expanded "><a href="raft/raft-lab.html"><strong aria-hidden="true">12.2.</strong> pingcap talent plan raft lab</a></li></ol></li><li class="chapter-item expanded "><a href="mit6-824/index.html"><strong aria-hidden="true">13.</strong> mit6.824: 分布式系统</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="mit6-824/gfs.html"><strong aria-hidden="true">13.1.</strong> GFS</a></li><li class="chapter-item expanded "><a href="mit6-824/raft.html"><strong aria-hidden="true">13.2.</strong> Raft</a></li></ol></li><li class="chapter-item expanded "><a href="tidb/index.html"><strong aria-hidden="true">14.</strong> TiDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tidb/note.html"><strong aria-hidden="true">14.1.</strong> tidb学习资料整理</a></li><li class="chapter-item expanded "><a href="tidb/main.html"><strong aria-hidden="true">14.2.</strong> Server Main Loop</a></li><li class="chapter-item expanded "><a href="tidb/schema.html"><strong aria-hidden="true">14.3.</strong> Schema</a></li><li class="chapter-item expanded "><a href="tidb/plan.html"><strong aria-hidden="true">14.4.</strong> Plan</a></li><li class="chapter-item expanded "><a href="tidb/hash-join.html"><strong aria-hidden="true">14.5.</strong> Hash Join (draft)</a></li><li class="chapter-item expanded "><a href="tidb/ddl.html"><strong aria-hidden="true">14.6.</strong> ddl</a></li><li class="chapter-item expanded "><a href="tidb/table.html"><strong aria-hidden="true">14.7.</strong> table</a></li></ol></li><li class="chapter-item expanded "><a href="clickhouse/index.html"><strong aria-hidden="true">15.</strong> ClickHouse</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="clickhouse/server-main.html"><strong aria-hidden="true">15.1.</strong> Server main</a></li><li class="chapter-item expanded "><a href="clickhouse/block.html"><strong aria-hidden="true">15.2.</strong> block</a></li><li class="chapter-item expanded "><a href="clickhouse/blockio.html"><strong aria-hidden="true">15.3.</strong> blockio</a></li></ol></li><li class="chapter-item expanded "><a href="godot/index.html"><strong aria-hidden="true">16.</strong> Godot</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="godot/learning-note.html"><strong aria-hidden="true">16.1.</strong> godot 学习笔记</a></li></ol></li><li class="chapter-item expanded "><a href="tokio/index.html"><strong aria-hidden="true">17.</strong> tokio</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tokio/executor.html"><strong aria-hidden="true">17.1.</strong> Executor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="tokio/park.html"><strong aria-hidden="true">17.1.1.</strong> Park</a></li><li class="chapter-item expanded "><a href="tokio/thread-pool.html"><strong aria-hidden="true">17.1.2.</strong> thread pool</a></li></ol></li><li class="chapter-item expanded "><a href="tokio/driver.html"><strong aria-hidden="true">17.2.</strong> driver</a></li><li class="chapter-item expanded "><a href="tokio/io.html"><strong aria-hidden="true">17.3.</strong> io</a></li><li class="chapter-item expanded "><a href="tokio/codec.html"><strong aria-hidden="true">17.4.</strong> codec</a></li><li class="chapter-item expanded "><a href="tokio/channel.html"><strong aria-hidden="true">17.5.</strong> channel</a></li><li class="chapter-item expanded "><a href="tokio/waker.html"><strong aria-hidden="true">17.6.</strong> waker</a></li></ol></li><li class="chapter-item expanded "><a href="python/index.html"><strong aria-hidden="true">18.</strong> python</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="python/records/records.html"><strong aria-hidden="true">18.1.</strong> records</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">blog</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#about" id="about">About</a></h1>
<h1><a class="header" href="#tensorflow" id="tensorflow">tensorflow</a></h1>
<p>some notes on reading tensorflow source code</p>
<h2><a class="header" href="#tensorflow-graph-executor草稿" id="tensorflow-graph-executor草稿">Tensorflow Graph Executor(草稿)</a></h2>
<h2><a class="header" href="#摘要" id="摘要">摘要</a></h2>
<p>Tensorflow中单机版的(direct session)会按照device将graph先划分成子图subgraph, 然后每个subgraph会交给一个execturo去执行，分布式的（GrpSession) 首先会将graph按照worker划分，每个worker划分成一个子图，然后注册到每个worker的graph_mgr, 并在graph_mgr中再按照device将worker_subgraph划分成device的subgraph, 最后每个device对应的subgraph会由executor去执行，Tensorflow中的graph执行示意图如下(图片来自<a href="https://wookayin.github.io/tensorflow-talk-debugging/#1">tensorflow-talk-debugging</a>)。</p>
<p><img src="tensorflow/./images/tensors_flowing.gif" alt="tensors_flowing" /></p>
<p>本文主要分析了executor在执行graph时，Node的执行调度以及node的输入输出数据, 执行状态是如何保存的，最后结合代码和<a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">Tensorflow control flow implemention</a>这部分文档分析了的control flow的具体实现。主要涉及的代码为common_runtime/executor.cc</p>
<h3><a class="header" href="#executor中主要类" id="executor中主要类">Executor中主要类</a></h3>
<h4><a class="header" href="#executor" id="executor">Executor</a></h4>
<p>Executor为基类，对外提供了两个接口Run和RunAsync, 其中Run是对RunAsync简单的一层包装。</p>
<pre><code class="language-cpp">
  // Synchronous wrapper for RunAsync().
  Status Run(const Args&amp; args) {
    Status ret;
    Notification n;
    RunAsync(args, [&amp;ret, &amp;n](const Status&amp; s) {
      ret = s;
      n.Notify();
    });
    n.WaitForNotification();
    return ret;
  }
</code></pre>
<p>Executor基类只要去实现RunAsync就行。</p>
<pre><code class="language-cpp">  virtual void RunAsync(const Args&amp; args, DoneCallback done) = 0;
</code></pre>
<h4><a class="header" href="#executorimpl" id="executorimpl">ExecutorImpl</a></h4>
<p>ExecutorImpl继承实现了Executor，它的RunAsync实现转发给了ExecutorState::RunAsync, ExecutorImpl主要的工作是从Graph中解析出一些静态信息，比如FrameInfo, GraphView, 由后面的ExecutorState执行的时候使用。</p>
<pre><code class="language-cpp">void ExecutorImpl::RunAsync(const Args&amp; args, DoneCallback done) {
  (new ExecutorState(args, this))-&gt;RunAsync(std::move(done));
}
</code></pre>
<h4><a class="header" href="#executorstate" id="executorstate">ExecutorState</a></h4>
<h3><a class="header" href="#executor中的调用关系" id="executor中的调用关系">Executor中的调用关系</a></h3>
<h3><a class="header" href="#executorimpl-call-flow" id="executorimpl-call-flow">ExecutorImpl call flow</a></h3>
<p>Executor被调用的入口为NewLocalExecutor, 在DirectSesion中会为每个subgraph创建一个executor, 然后交给ExecutorBarrier同时执行多个Executor。NewLocalExecutor在ExecutorImpl成员函数中的调用过程如下：</p>
<p><img src="tensorflow/./images/executor_impl_call.jpeg" alt="executor impl call flow" /></p>
<p>Exector::RunAsync这个会被转发给ExecutorState::RunAsync（这个函数的执行逻辑见下文）</p>
<h3><a class="header" href="#executorimplinitialize" id="executorimplinitialize">ExecutorImpl::Initialize</a></h3>
<p>在ExecutorImpl::Initialize中，对于graph中的每个node, 创建对应的NodeItem, 主要包含了三块：</p>
<ol>
<li>调用params.create_kernal, 创建nodeItem-&gt;kernal.</li>
<li>记录nodeItem.input_start, input_start 是该node在它所属frame的input_tensors中的偏移index, 这个在后面的ProcessInputs和ProcessOutputs中会用到。</li>
<li>创建node对应的pending_id， pending_id用于找到记录它执行状态的pendingCount, 这个在后面的ActiveNode中会用到.</li>
</ol>
<p>在BuildCtronlFlow中会建立好framename之间的父子关系, frameInfo是frame的静态信息（对应着执行时候的FrameState动态信息），并且建立了从node id找到node所属frame name的映射关系，包含了frame中的total inputs, 这个frame所包含的node.</p>
<p><img src="tensorflow/./images/tf-executor-init.jpeg" alt="image" /></p>
<h2><a class="header" href="#executorstaterunasync" id="executorstaterunasync">ExecutorState::RunAsync</a></h2>
<p><img src="tensorflow/./images/tf-executor-call-flow.jpeg" alt="image" /></p>
<h3><a class="header" href="#executorimplprocess" id="executorimplprocess">ExecutorImpl::Process</a></h3>
<p><img src="tensorflow/./images/tf-executor-data-flow.jpeg" alt="image" /></p>
<h2><a class="header" href="#control-flow" id="control-flow">Control Flow</a></h2>
<p>后来在[1]中发现节点还有Switch, Merge, IterNext, Enter, Exit 五个flow control node，用来实现while循环，为此tensorflowe引入了frame的概念，可以粗略的认为和函数调用一样吧, 在遇到Enter node的时候，就新建一个child frame，把inputs(类似于函数函数调用时候参数入栈)一样，forward到child frame中，在遇到Exit node，就把输出放到parent frame 中(类似于将函数的return值入栈)。</p>
<p>未完待续</p>
<h2><a class="header" href="#executor中数据流程" id="executor中数据流程">Executor中数据流程</a></h2>
<h2><a class="header" href="#参考" id="参考">参考</a></h2>
<ol>
<li>[Tensorflow control flow implemention]</li>
<li><a href="https://wookayin.github.io/tensorflow-talk-debugging/#1">tensorflow-talk-debugging</a></li>
</ol>
<h2><a class="header" href="#sub-graph-预处理-node--nodeitem--taggednode-draft" id="sub-graph-预处理-node--nodeitem--taggednode-draft">Sub Graph 预处理: Node =&gt; NodeItem =&gt; TaggedNode (Draft)</a></h2>
<h3><a class="header" href="#引言" id="引言">引言</a></h3>
<p>下图是一个graph中每个node被处理的过程，首先在ExecutorImpl::Initialize的时候，将node 处理成NodeItem，创建node对应的kernal, 然后在node ready可执行的时候，会创建一个TaggedNode(TaggedNode主要多了个frame指针，记录了当前执行的frame), 并将它放入Ready队列中，最后交给ExecutorState::Process去执行这个Node。
<img src="tensorflow/./images/node_process_flow.jpeg" alt="node process flow" /></p>
<h3><a class="header" href="#nodeitem" id="nodeitem">NodeItem</a></h3>
<p>NodeItem的主要作用是将Graph中每个node的op，转换成可以在device上执行的kernal, 另一方面，记录该node输入tensor的位置，并且使用PendingCount来记录Node的执行状态。 Gview可以看成是NodeItem的容器，根据node的id就可以找到相应的NodeItem, 对于graph中的每个node, 在ExecutorImpl::Initialize中都会创建一个NodeItem，放到Gview中。</p>
<h4><a class="header" href="#nodeitem-主要包含的字段" id="nodeitem-主要包含的字段">NodeItem 主要包含的字段</a></h4>
<ol>
<li><b>kernel</b>:  由params.create_kernel创建，kernel是在device上执行的主要对象，kerenl 并将在ExecutorImpl的析构函数被params.delete_kernel删除。</li>
</ol>
<pre><code class="language-cpp">  // The kernel for this node.
  OpKernel* kernel = nullptr;
</code></pre>
<ol start="2">
<li><b>input_start</b>：纪录了在当前IteratorState的input_tensors中开始的index。这个node的输入为：input_tensors[input_start: input_start + num_inputs]这部分对应的Tensors。</li>
</ol>
<pre><code class="language-cpp">  // Cached values of node-&gt;num_inputs() and node-&gt;num_outputs(), to
  // avoid levels of indirection.
  int num_inputs;
  int num_outputs;

  // ExecutorImpl::tensors_[input_start] is the 1st positional input
  // for this node.
  int input_start = 0;

  // Number of output edges.
  size_t num_output_edges;

</code></pre>
<ol start="3">
<li><b>pending_id</b>: 根据这个id在当前的IteratorState中找到对应的PendingCount，从而找到这个nodeItem的执行状态。</li>
</ol>
<pre><code class="language-cpp">  PendingCounts::Handle pending_id;
</code></pre>
<ol start="4">
<li><b>expensive/async kernel</b>: 标志表明kernel是否是Async的和expensive的。</li>
</ol>
<pre><code>  bool kernel_is_expensive : 1;  // True iff kernel-&gt;IsExpensive()
  bool kernel_is_async : 1;      // True iff kernel-&gt;AsAsync() != nullptr
</code></pre>
<ol start="5">
<li><b>control node</b> ，标志该node是否是Control flow node, 以及类型</li>
</ol>
<pre><code class="language-cpp">
  bool is_merge : 1;             // True iff IsMerge(node)
  bool is_enter : 1;             // True iff IsEnter(node)
  bool is_exit : 1;              // True iff IsExit(node)
  bool is_control_trigger : 1;   // True iff IsControlTrigger(node)
  bool is_sink : 1;              // True iff IsSink(node)
  // True iff IsEnter(node) || IsExit(node) || IsNextIteration(node)
  bool is_enter_exit_or_next_iter : 1;
</code></pre>
<ol start="6">
<li><b>allocate attribute</b>: 影响device所返回的allocator,从而影响kernal执行时候，申请内存时候的处理行为。</li>
</ol>
<pre><code class="language-cpp">  // Return array of per-output allocator attributes.
  const AllocatorAttributes* output_attrs() const { return output_attr_base(); }
</code></pre>
<p>InferAllocAttr主要根据device, send, recv等节点, 来设置是否是gpu_compatible的，</p>
<pre><code class="language-cpp">      attr-&gt;set_nic_compatible(true);
      attr-&gt;set_gpu_compatible(true);
</code></pre>
<p>其中AllocatorAttributes主要影响GpuDevice所返回的allocator上。</p>
<pre><code class="language-cpp">//common_runtime/gpu/gpu_device_factory.cc

  Allocator* GetAllocator(AllocatorAttributes attr) override {
    if (attr.on_host()) {
      if (attr.gpu_compatible() || force_gpu_compatible_) {
        ProcessState* ps = ProcessState::singleton();
        return ps-&gt;GetCUDAHostAllocator(0);
      } else {
        return cpu_allocator_;
      }
    } else {
      return gpu_allocator_;
    }
}
</code></pre>
<h3><a class="header" href="#taggednode" id="taggednode">TaggedNode</a></h3>
<p>TaggedNode 增加了了一个FrameState指针，指向了Node将要执行的FrameState, input_iter, input_frame加上input_iter可以确定了</p>
<pre><code class="language-cpp">  struct TaggedNode {
    const Node* node = nullptr;
    FrameState* input_frame = nullptr;
    int64 input_iter = -1;
    bool is_dead = false;

    TaggedNode(const Node* t_node, FrameState* in_frame, int64 in_iter,
               bool dead) {
      node = t_node;
      input_frame = in_frame;
      input_iter = in_iter;
      is_dead = dead;
    }
</code></pre>
<p>在node处于ready 可执行状态的时候，会创建一个TaggedNode, 并放到TaggedNodeSeq队列中，等待调度执行。</p>
<pre><code>ExecutorState::FrameState::ActivateNodes ==&gt;
    ready-&gt;push_back(TaggedNode(dst_item-&gt;node, this, iter, dst_dead));

ExecutorState::RunAsync ==&gt;
    for (const Node* n : impl_-&gt;root_nodes_) {
      DCHECK_EQ(n-&gt;in_edges().size(), 0);
      ready.push_back(TaggedNode{n, root_frame_, 0, false});
    }
</code></pre>
<h3><a class="header" href="#获取node输入tensors指针" id="获取node输入tensors指针">获取node输入tensors指针</a></h3>
<p>首先根据TaggedNode中的input_frame，input_iter获取node的输入tensors</p>
<pre><code>  Entry* GetInputTensors(FrameState* input_frame,
                         int64 input_iter) const NO_THREAD_SAFETY_ANALYSIS {
    return input_frame-&gt;GetIteration(input_iter)-&gt;input_tensors;
  }
</code></pre>
<p>然后根据NodeItem中定义的input_start获取first_input tensor的指针</p>
<pre><code>//在ExecutorState::Process中:
    Entry* input_tensors = GetInputTensors(input_frame, input_iter);
    Entry* first_input = input_tensors + item.input_start;
</code></pre>
<h2><a class="header" href="#flow-control-op" id="flow-control-op">Flow Control op</a></h2>
<p>在Tensorflow中，graph中每个node的op，都在一个execution Frame中执行，Enter/Exit分别负责execution Frame的创建和删除，如果把execution frame和函数调用做类比的话，那么Enter有点类似于传参，而Exit则类似于return 返回值。
而switch/merge/nextIteration 则用于实现类似于while/if之类的分支跳转和循环。本节主要参照 <a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">1</a> 这篇文章。</p>
<h3><a class="header" href="#flow-control-op-1" id="flow-control-op-1">flow control op</a></h3>
<p><img src="tensorflow/./images/control-flow-op.jpeg" alt="control flow op" /></p>
<p>Tensorflow中control flow op对应具体定义如下</p>
<h4><a class="header" href="#switch" id="switch">switch</a></h4>
<blockquote>
<p>A Switch operator forwards the input tensor d to one of its outputs depending on the boolean tensor of the control input p. A Switch is enabled for execution when both its inputs are available.</p>
</blockquote>
<p>Switch 根据predict将输入tensor导出到相应的true/false输出。没获得输出的分支会被标记为dead状态(有点类似于if/else中没被执行到的代码), 这个dead状态会往下传播。</p>
<h4><a class="header" href="#merge" id="merge">Merge</a></h4>
<blockquote>
<p>A Merge operator forwards one of its available inputs to its output. A Merge is enabled for execution when any of its inputs is available. It is unspecified which available input it outputs if there are multiple inputs available.</p>
</blockquote>
<p>Merge 将输入tensor中的一个导出到输出（先到先得）,一般配合switch用</p>
<h4><a class="header" href="#enter" id="enter">Enter</a></h4>
<blockquote>
<p>An Enter operator forwards its input to the execution frame that is uniquely identified by the given name. This Enter op is used to pass a tensor in one execution frame to a child execution frame. There can be multiple Enter ops to the same child execution frame, each making a tensor available (asynchronously) in that child execution frame. An Enter is enabled for execution when its input is available. A new execution frame is instantiated in the TensorFlow runtime when the first Enter op to that frame is executed </p>
</blockquote>
<p>Enter node将输入tensor导入到一个frame中。frame name是唯一的，可以根据frame name来找到对应的frame， 在执行的时候，如果frame不存在的话，Enter会创建相应的子frame, Enter node所在的frame是该frame的parent frame.</p>
<h4><a class="header" href="#exit" id="exit">Exit</a></h4>
<blockquote>
<p>An Exit operator forwards a value from an execution frame to its parent execution frame.  This Exit op is used to return a tensor computed in a child execution frame back to its parent frame. There can be multiple Exit ops to the parent frame, each asynchronously passing a tensor back to the parent frame. An Exit is enabled when its input is available.</p>
</blockquote>
<p>Exit node 从Frame中导出一个tensor到parent frame中。</p>
<h4><a class="header" href="#nextiteration" id="nextiteration">NextIteration</a></h4>
<blockquote>
<p>A NextIteration operator forwards its input to the next iteration in the current execution frame. The TensorFlow runtime keeps track of iterations in an execution frame. Any op executed in an execution frame has a unique iteration id, which allows us to uniquely identify different invocations of the same op in an iterative computation. Note that there can be multiple NextIteration ops in an execution frame. The TensorFlow runtime starts iteration N+1 when the first NextIteration op is executed at iteration N. As more tensors enter an iteration by executing NextIteration ops, more ops in that iteration will be ready for execution. A NextIteration is enabled when its input is available.</p>
</blockquote>
<p>NextIteration将输入导出到下个iteration, NextIteration导出的应该是循环变量，比如下面代码中的j和sum</p>
<pre><code class="language-cpp">for(int j=0, sum=0; j &lt; 100;){
    int tmp = i + 1;
    j * = 2
    sum += j;
}
</code></pre>
<h3><a class="header" href="#while-loop" id="while-loop">While loop</a></h3>
<p>可以通过上述的五个flow control node来实现tensorflow中的while loop</p>
<pre><code class="language-python">tf.while_loop(lambda i: i &lt; 10, lambda i: tf.add(i, 1), [0])
</code></pre>
<p><img src="tensorflow/./images/while_loop.jpeg" alt="while loop" /></p>
<p>可以看到NextIteration导入导出的是循环变量i，merge node可以用来初始化变量, 类似于 <code>i= i || 0</code>的效果, switch控制是否结束循环，Exit跳出循环。</p>
<p>在文献<a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">1</a>中还讲述了dead传播，分布式的whileloop，以及while loop对应的gradient op.讲的比较深，后面再补上吧。</p>
<h3><a class="header" href="#参考文献" id="参考文献">参考文献：</a></h3>
<ol>
<li><a href="http://download.tensorflow.org/paper/white_paper_tf_control_flow_implementation_2017_11_1.pdf">Tensorflow control flow implemention</a></li>
</ol>
<h1><a class="header" href="#executor-frame" id="executor-frame">Executor Frame</a></h1>
<h2><a class="header" href="#引言-1" id="引言-1">引言</a></h2>
<p>在Executor 执行Graph的时候，会首先分析Graph, 创建关于Graph中frame的静态信息，比如ControlFlowInfo和FrameInfo，对于graph中的每个node, 可以根据ControlFlowInfo去得到它对应的frame_name, 然后根据frame_name可以得到FrameInfo的一些信息。</p>
<p>而FrameState和IterationState这两个是动态的状态，由Executor在执行Graph时候动态创建的。FrameState对应着整个while loop，而IterationState则对应着while loop中的某个迭代。 FrameState中包了total_input(frame中所有node input个数等信息），IterationState中有个EntryVec用于保存某次迭代时候，node之间输入输出的Entry。</p>
<p>本文主要分析了Executor中ControlFlowInfo， FrameInfo, FrameState, IterationState，这几个和Executor Frame相关的struct， 以及它们之间的关系。</p>
<h2><a class="header" href="#executorimplcontrolflowinfo" id="executorimplcontrolflowinfo">ExecutorImpl::ControlFlowInfo</a></h2>
<p>ControlFlowInfo里面<code>unique_frame_names</code>保存了computation graph中所有frame的名字，frame_names则是个倒查表，索引对应于<code>node-&gt;id</code>, 可以根据<code>frame_names[node-&gt;id()]</code>找到node对应的frame_name.</p>
<pre><code class="language-cpp">struct ControlFlowInfo {
  gtl::FlatSet&lt;string&gt; unique_frame_names;
  std::vector&lt;string&gt; frame_names;
};
</code></pre>
<h3><a class="header" href="#controlflowinfo的创建" id="controlflowinfo的创建">ControlFlowInfo的创建</a></h3>
<p>BuildControlFlowInfo 会遍历整个graph, 然后处理Enter/Exit node, 填充好ControlFlowInfo中的字段, </p>
<ol>
<li>如果遇到Enter node, 则进入子Frame, Enter node的每个输出node对应的frame_name都是EnterNode对应的 &quot;frame_node&quot;属性</li>
</ol>
<pre><code class="language-cpp">//Enter node包含了frame_name 属性，
GetNodeAttr(curr_node-&gt;attrs(), &quot;frame_name&quot;, &amp;frame_name));
</code></pre>
<ol start="2">
<li>如果是Exit node, 则退出子Frame, Exit node的每个输出node对应的frame_name都是Exit node parent node的 frame_name</li>
</ol>
<pre><code class="language-cpp">//other code
else if (IsExit(curr_node)) {
    parent = parent_nodes[curr_id];
    frame_name = cf_info-&gt;frame_names[parent-&gt;id()];
    parent = parent_nodes[parent-&gt;id()];
}
</code></pre>
<ol start="3">
<li>如果是其他类型的node, 则node的每个输出node frame和当前node一致</li>
</ol>
<pre><code class="language-cpp"> parent = parent_nodes[curr_id];
 frame_name = cf_info-&gt;frame_names[curr_id];
</code></pre>
<h4><a class="header" href="#controlflow-info被用到的地方" id="controlflow-info被用到的地方">controlflow info被用到的地方</a></h4>
<p>在executor中首先会根据node-&gt;id找到frame_name, 然后根据frame_name找到对应的FrameInfo</p>
<pre><code class="language-cpp">    const string&amp; frame_name = cf_info.frame_names[id];
    FrameInfo* frame_info = EnsureFrameInfo(frame_name);
</code></pre>
<h2><a class="header" href="#executorimplframeinfo" id="executorimplframeinfo">ExecutorImpl::FrameInfo</a></h2>
<p>FrameInfo包含的主要字段如下:</p>
<pre><code class="language-cpp">    // The total number of inputs to a frame.
    int input_count;

    int total_inputs;

    PendingCounts::Layout pending_counts_layout;
    PendingCounts* pending_counts;  // Owned
</code></pre>
<h3><a class="header" href="#input_count" id="input_count">input_count</a></h3>
<p>input_count 代表graph中Enter到该frame的Enter Node个数, 统计个数的代码如下：</p>
<pre><code class="language-cpp">//ExecutorImpl::Initialize
  for (const Node* n : graph_-&gt;nodes()) {
    //other code..

    if (IsEnter(n)) {
      string enter_name;
      TF_RETURN_IF_ERROR(GetNodeAttr(n-&gt;attrs(), &quot;frame_name&quot;, &amp;enter_name));
      EnsureFrameInfo(enter_name)-&gt;input_count++;
    }
  }
</code></pre>
<h3><a class="header" href="#total_inputs" id="total_inputs">total_inputs</a></h3>
<p>total_inputs会在ExecutorState::IteratorState中用到，它的值为frame中所有node的inputs个数的总和。</p>
<pre><code class="language-cpp">// The total number of input tensors of a frame.
// == sum(nodes[*].num_inputs()) where nodes are the nodes in the frame.
int total_inputs;
</code></pre>
<p>total_inputs在后面的影响如下：</p>
<pre><code>FrameInfo.total_inputs ==&gt; FrameState.total_input_tensors ==&gt; IterationsState.input_tensors(new Entry[total_input_tensors])
</code></pre>
<h3><a class="header" href="#pendingcounts" id="pendingcounts">PendingCounts</a></h3>
<ol start="3">
<li>PendingCounts相关，pending_counts_layout在后面会用来创建Node的PendingCount, pending count会用来跟踪Node的状态（比如是否所有的input都已ready, Node是否已经执行过了，Node是否在Dead path)，</li>
</ol>
<p>struct FrameInfo由EnsureFrameInfo这个函数lazy创建，并在Intialize填充好它的字段。</p>
<pre><code class="language-cpp">  FrameInfo* EnsureFrameInfo(const string&amp; fname) {
    auto slot = &amp;frame_info_[fname];
    if (*slot == nullptr) {
      *slot = new FrameInfo;
    }
    return *slot;
  }
</code></pre>
<p>FrameInfo将在ExecutorImpl的析构函数中被删掉。</p>
<pre><code class="language-cpp">  ~ExecutorImpl() override {
    //other code
    for (auto fiter : frame_info_) {
      delete fiter.second;
    }
</code></pre>
<h2><a class="header" href="#executorstateframestate" id="executorstateframestate">ExecutorState::FrameState</a></h2>
<p>前面两个ControlFlowInfo/FrameInfo都是静态的信息(所以叫XXXInfo)，而FrameState和IterationState都是动态信息，会在Graph执行的时候动态创建。</p>
<h4><a class="header" href="#创建framestate-findorcreatechildframe" id="创建framestate-findorcreatechildframe">创建FrameState: FindOrCreateChildFrame</a></h4>
<p>在FindOrCreateChildFrame中，会调用InitializeFrameInfo从FrameInfo中抽取有用的字段</p>
<pre><code class="language-cpp">    void InitializeFrameInfo(const string&amp; enter_name) {
      auto it_frame_info = executor-&gt;frame_info_.find(enter_name);
      DCHECK(it_frame_info != executor-&gt;frame_info_.end());
      ExecutorImpl::FrameInfo* finfo = it_frame_info-&gt;second;
      pending_counts = finfo-&gt;pending_counts;
      total_input_tensors = finfo-&gt;total_inputs;
      num_pending_inputs = finfo-&gt;input_count;
      nodes = finfo-&gt;nodes;
    }
</code></pre>
<p>FindOrCreateChildFrame被调用的stack</p>
<pre><code>Process -&gt; PropagationOutputs -&gt; FindOrCreateChildFrame
</code></pre>
<h3><a class="header" href="#删除framestate-deleteframe" id="删除framestate-deleteframe">删除FrameState: DeleteFrame</a></h3>
<p>1.在PropgateOutputs中，如果is_frame_done，就会调用DeleteFrame, DeleteFrame会向parent frame传播dead_exits（TODO: 这部分描述细化）</p>
<p>IterationState删除的地方</p>
<ol>
<li>CleanupFrameIterations</li>
<li>frame-&gt;CleanupIterations</li>
</ol>
<h2><a class="header" href="#executorstateiterationstate" id="executorstateiterationstate">ExecutorState::IterationState</a></h2>
<pre><code class="language-cpp">    Entry* input_tensors;
    // The number of outstanding ops for each iteration.
    size_t outstanding_ops;
    int outstanding_frame_count;
    PendingCounts counts_;
</code></pre>
<h6><a class="header" href="#framestate和iterationstate创建地方" id="framestate和iterationstate创建地方">FrameState和IterationState创建地方:</a></h6>
<ol>
<li>
<p>在ExecutorState的构造函数中会创建一个FrameState作为rootframe, 同时也会创建该frameState的第一个IterationState。</p>
</li>
<li>
<p>在执行完一个Node之后，PropagateOutputs在遇到Enter节点的时候，会调用FindOrCreateChildFrame来创建一个新的FrameState,以及该FrameState的第一个IterationState</p>
</li>
<li>
<p>在PropgateOutputs的时候，遇到NextIteration Node 会去调用FrameState::IncreatementIteration新增一个IterationState</p>
</li>
<li>
<p>所有的framesate都放在了outstanding_frames 这个map中，新建的framestate会插到这个map中，删除的时候会从这个map中去掉。</p>
</li>
</ol>
<h2><a class="header" href="#tensorflow-direct-session-draft" id="tensorflow-direct-session-draft">Tensorflow Direct Session (Draft)</a></h2>
<h3><a class="header" href="#摘要-1" id="摘要-1">摘要</a></h3>
<p>本文主要分析了tensorflow 中DirectSession部分的代码。如果把executor 执行graph当成一个函数的话，那么Tensorflow中Session主要功能是把用户传过来的一些参数Feeds到compute graph中，然后运行到graph target node，最后在graph computation完成之后，取出用户指定名字的一些tensor。</p>
<p>DirectSession 则主要工作以下几方面：</p>
<ol>
<li>Rewrite Graph： 将FeedInputs和FetchOutputs节点加到graph中，然后去掉graph中运行不到的节点，最后采用并查集的方式，给graph中每个node分配一个device。</li>
<li>Graph partition：根据每个node所device，将node划分成不同的subgraph, subgraph之间添加send和recv节点做不同device之间的通信。</li>
<li>CeateExecutors：每个device的subgraph会创建一个Executor来执行graph computation。</li>
<li>Fetch outputs：对于DirectSession来说，FeedInputs和FetchOutputs 所添加的节点是<code>_Arg</code>和<code>_RetVal</code>，这两个节点会通过directSession的callframe来读写input，output。</li>
</ol>
<h3><a class="header" href="#rewritegraph" id="rewritegraph">RewriteGraph</a></h3>
<p>RewriteGraph这块的callstack如下图所示，主要主要涉及到 GraphExecutionState, SubGraph, Placer这三块。</p>
<p>GraphExecutionState据文档所说(graph_execution_state.h)，其主要作用是按照BuildGraphOptions选项将Graph转换成可执行的Client Graph。</p>
<blockquote>
<p>GraphExecutionState is responsible for generating an
executable ClientGraph from the original GraphDef that specifies
the complete graph and from BuildGraphOptions which specifies
input/output nodes.</p>
</blockquote>
<p>ClientGraph与GraphDef的区别是： ClientGraph中每个node都被Assign了某个Device，这部分由Placer完成；另外添加了input/output nodes, 去掉了执行不到的node, 这部分由subgraph完成。</p>
<blockquote>
<p>An executable Graph differs from a GraphDef by being Placed,
meaning that each Node is assigned to a single Device in the
available set.</p>
</blockquote>
<p><img src="tensorflow/./images/direct_session_rewrite_graph.jpeg" alt="rewrite graph" /></p>
<h3><a class="header" href="#call-frame-feed-and-fetch" id="call-frame-feed-and-fetch">Call frame: feed and fetch</a></h3>
<p>DirectSession中采用了call frame的方式读写compution graph中的inputs/outputs </p>
<p><img src="tensorflow/./images/direct_session_call_frame.jpeg" alt="direct session call frame" /></p>
<p>DirectSession::Run的时候，首先会创建一个FunctionCallFrame, 把要feed的tensor填充到<code>FunctionCallFrame::args_</code>。</p>
<pre><code class="language-cpp">// In DirectSession::Run

  FunctionCallFrame call_frame(executors_and_keys-&gt;input_types,
                               executors_and_keys-&gt;output_types);
  gtl::InlinedVector&lt;Tensor, 4&gt; feed_args(inputs.size());
  for (const auto&amp; it : inputs) {
    if (it.second.dtype() == DT_RESOURCE) {
      Tensor tensor_from_handle;
      TF_RETURN_IF_ERROR(
          ResourceHandleToInputTensor(it.second, &amp;tensor_from_handle));
      feed_args[executors_and_keys-&gt;input_name_to_index[it.first]] =
          tensor_from_handle;
    } else {
      feed_args[executors_and_keys-&gt;input_name_to_index[it.first]] = it.second;
    }
  }
  const Status s = call_frame.SetArgs(feed_args);
</code></pre>
<p>在创建Executor的时候，通过Executor::Args.call_frame把call_frame放到OpkernalContext中。</p>
<pre><code class="language-cpp">//## DirectSessioin::Runinternal

  Executor::Args args;
  args.step_id = step_id;
  args.call_frame = call_frame;

  //other code...
  //每个device subgraph对应一个item, item.executor为这个subgraph的exeuctor.
  item.executor-&gt;RunAsync(args, barrier-&gt;Get());


//## ExecutorState::Process
  OpKernelContext::Params params;
  params.step_id = step_id_;
  params.call_frame = call_frame_;

  //other code ...
  // Synchronous computes.
  OpKernelContext ctx(&amp;params, item.num_outputs);
  nodestats::SetOpStart(stats);
  device-&gt;Compute(CHECK_NOTNULL(op_kernel), &amp;ctx);

</code></pre>
<p>当所有的subgraph Executor执行完毕后，通过FunctionCallFrame::ConsumeRetVals的方式把输出的tensor取出来。</p>
<pre><code class="language-cpp">// DirectSession::Run

  if (outputs) {
    std::vector&lt;Tensor&gt; sorted_outputs;
    const Status s = call_frame.ConsumeRetvals(&amp;sorted_outputs);
    if (errors::IsInternal(s)) {
 //other code
</code></pre>
<h3><a class="header" href="#device-placer" id="device-placer">Device placer</a></h3>
<p>Placer 在初始的时候，用户会指定某些节点的device, 比如有的节点是gpu:0, 有的cpu:0, 有的node是gpu:1, 然后将有相同<code>class_</code>属性<code>@loc:xxx</code>的node节点放到一个集合里面，随后根据以下约束, 采用并查集的方式，对node集合进行进一步的划分:</p>
<ol>
<li>用户指定了device，就将node放到用户指定的device上</li>
<li>Generateo node 和output node放到同一个device上</li>
<li>Meta node（比如cast操作) 和input node放到同一个device上</li>
<li>Reftype 的Input, input和output节点尽量放到同一个device上</li>
<li>采用并查集的方式将node place给device</li>
<li>对于stateful的node, 不改变它的device assign。</li>
</ol>
<p>stateful node 在placed之后，就不能移到别的device上了, 对于这种node，GraphExecutionState的做法是在placer run之前将stateful node的device assign保存以下，在placer run 之后再恢复回去。</p>
<blockquote>
<p>Map of placed stateful nodes, i.e. nodes for which is_stateful()
is true, such as &quot;params&quot; and &quot;queue&quot; nodes.  Once placed these
nodes can not be moved to a different device.  Maps node names to
device names.</p>
</blockquote>
<p>可以通过打开<code>log_device_placement</code>的方式让placer在stderr中把node的device place情况打出来:</p>
<pre><code class="language-py">config=tf.ConfigProto(log_device_placement=True)
sess = tf.Session(config=config)
</code></pre>
<h3><a class="header" href="#graph-partition" id="graph-partition">Graph partition</a></h3>
<p>Graph partition根据上面Placemnet的结果，将graph partition成不同的子图，子图之间添加send 和recv节点，send和recv节点会用rendzvous来传送tensor。有时候除了send和recv node还需要添加一些control flow node。</p>
<p><img src="tensorflow/./images/distributed_graph.png" alt="graph partition" /></p>
<p>（这个地方需要看下tf implement那个文档，了解下具体情况）</p>
<h3><a class="header" href="#executor-cache" id="executor-cache">Executor Cache</a></h3>
<p>提交给DirectSessoin在经过Graph Partition之后，会划分成不同的子图，比如下图将一个大的graph划分成了3个subgraph分别放置在了在CPU, GPU1, GPU2上，device之间通过rendezvous来通信，每个subgraph都会创建一个executor去执行。</p>
<p><img src="tensorflow/./images/direct_session_create_executors.png" alt="Graph Executors" /></p>
<p>在模型的训练通常会多次迭代run, 因此要加一层cache避免多次做graph的parition，多次创建executor。</p>
<pre><code class="language-py">with tf.Session(config=config) as sess:
    sess.run([merge, gd_step], feed_dict={x: batch_xs, y_label: batch_ys})
</code></pre>
<p>cache的key为input, output，target tensor的names 连起来的。还有一个key是吧input, output, target的names分别sort之后再连起来。</p>
<p>DirectSession::Run中cache的key很有意思，有两个key,  首先去是未排序的，另外一个是排序的。未排序的为了快速查找，而排序的key是为了避免由于input_names中names顺序不一样导致cache miss。</p>
<pre><code>  // Fast lookup path, no sorting.
  // Fast查询的key, 没排序
  const string key = strings::StrCat(
      str_util::Join(inputs, &quot;,&quot;), &quot;-&gt;&quot;, str_util::Join(outputs, &quot;,&quot;), &quot;/&quot;,
      str_util::Join(target_nodes, &quot;,&quot;), &quot;/&quot;, run_state_args-&gt;is_partial_run,
      &quot;/&quot;, debug_tensor_watches_summary);


 // 将names分别排序然后concat起来.
  std::vector&lt;string&gt; inputs_sorted(inputs.begin(), inputs.end());
  std::sort(inputs_sorted.begin(), inputs_sorted.end());
  std::vector&lt;string&gt; outputs_sorted(outputs.begin(), outputs.end());
  std::sort(outputs_sorted.begin(), outputs_sorted.end());
  std::vector&lt;string&gt; tn_sorted(target_nodes.begin(), target_nodes.end());
  std::sort(tn_sorted.begin(), tn_sorted.end());

  const string sorted_key = strings::StrCat(
      str_util::Join(inputs_sorted, &quot;,&quot;), &quot;-&gt;&quot;,
      str_util::Join(outputs_sorted, &quot;,&quot;), &quot;/&quot;, str_util::Join(tn_sorted, &quot;,&quot;),
      &quot;/&quot;, run_state_args-&gt;is_partial_run, &quot;/&quot;, debug_tensor_watches_summary);
  // Set the handle, if its needed to log memory or for partial run.
</code></pre>
<h2><a class="header" href="#tensorflow-rendezvous" id="tensorflow-rendezvous">Tensorflow Rendezvous</a></h2>
<h3><a class="header" href="#摘要-2" id="摘要-2">摘要</a></h3>
<p>Rendezvous负责在Send和Recv node之间传递tensor, tensor的传递可能会跨设备(cross device), 也可能跨主机(GRPC，MPI，Rdam）等。如何提供统一简洁的接口，并同时实现不同场景下tensor高效传递是关键，Rendezvous功能上主要涉及以下两点：</p>
<ol>
<li>Send操作不会被block，而Recv操作可能会block，一直等到有tensor，才会返回或者调用异步的callback。</li>
<li>由于send 和recv node可能在同一个worker的不同device上，也有可能在不同worker的不同device上，所以Rendezvous又分为LocalRendezvous, IntraProcessRendezvous, RemoteRendezvous 以对应不同的场景。</li>
</ol>
<h3><a class="header" href="#rendezvous" id="rendezvous">Rendezvous</a></h3>
<h4><a class="header" href="#继承关系" id="继承关系">继承关系</a></h4>
<p>Rendezvous中各个层级实现功能如下：</p>
<ul>
<li>LocalRendezvor实现了核心Send和Recv操作，每个key对应了一个queue, send根据key放到相应的队列里，recv根据key去对应的队列取。</li>
<li>IntraProcessRendezvou使用CopyTensor::ViaDMA处理了不同device间的copy问题，其send, recv还是会交由LocalRendezvous去做。 </li>
<li>RpcProcessRendezvous实现了将woker的本地tensor(tensor如果在GPU上的话，需要先从GPu上copy到内存中）通过grpc buffer传递给调用者。</li>
</ul>
<p><img src="tensorflow/./images/rendezvous_inherit.jpeg" alt="rendezvous inherit" /></p>
<h4><a class="header" href="#localrendezvous-send-and-recv" id="localrendezvous-send-and-recv">LocalRendezvous: Send and Recv</a></h4>
<p>LocalRendezvous 实现了send和recv最基本的操作，按照send请求和recv请求顺序做了不同的处理：</p>
<ol>
<li>
<p>如果recv先到，就新创建一个item，把recv请求放到queue里面，等待send tensor抵达的时候，调用item.waiter回调函数通知recv， tensor已经到了。</p>
</li>
<li>
<p>如果send先到，就新创建一个item, 把item放到queue里面，等recv请求到达的时候，从队列中取出最开头的一个，调用recv.waiter回调函数，通知tensor已经到了。这里send请求就是简单的把tensor放入key对应的队列中，并不会block住。</p>
</li>
</ol>
<p><img src="tensorflow/./images/rendezvous_send_recv.jpeg" alt="local rendezvous send recv" /></p>
<h4><a class="header" href="#intraprocessrendezvous" id="intraprocessrendezvous">IntraProcessRendezvous</a></h4>
<p>IntraProcessRendezvous 用于处理进程内的通信, 他的send和recv是委托给LocalRendezvous, 在Local的RecvAsync的回调函数中，它会调用SameWokerRecvDone, 使用CopyTensor::ViaDMA处理跨device通信问题。</p>
<pre><code class="language-cpp">void IntraProcessRendezvous::SameWorkerRecvDone(...)
  //other code ...
  //case 1：都在内存中，直接用使用tensor的operator=
  if (src_host &amp;&amp; dst_host) {
    *out = in;
    done(Status::OK());
    return;
  }
  //other code ...
  //case 2: 使用ViaDMA处理不同device之间的tensor通信
  CopyTensor::ViaDMA(parsed.edge_name, send_args.device_context,
</code></pre>
<h4><a class="header" href="#copytensorviadma" id="copytensorviadma">CopyTensor::ViaDMA</a></h4>
<p>CopyTensor::ViaDMA处理了device之间的copy tensor。 Tensor的copy有3个方向：</p>
<ol>
<li>HOST_TO_DEVICE</li>
<li>DEVICE_TO_HOST</li>
<li>DEVICE_TO_DEVICE</li>
</ol>
<p>从下图可以看出这些操作最终调用的还是stream_executor的ThenMemcpy所封装的函数。 </p>
<p><img src="tensorflow/./images/copy_tensor_via_dma.jpeg" alt="copy tensor via dma" /></p>
<p>VarientDeviceCopy这个处理数据是DT_VARIENT结构的Tensor的，最后调用的是TensorListeDeviceCopy函数，这个函数所对应的deviceCopyFn就是stream_executor所封装的Memcpy, 这里的VarientDeviceCopy和copyfn都采用了static registor的模式（这种模式在tensorflow中用的非常多）。</p>
<pre><code class="language-cpp">static Status TensorListDeviceCopy(
    const TensorList&amp; from, TensorList* to,
    const UnaryVariantOpRegistry::AsyncTensorDeviceCopyFn&amp; copy) {
  to-&gt;element_shape = from.element_shape;
  to-&gt;element_dtype = from.element_dtype;
  to-&gt;tensors.reserve(from.tensors.size());
  for (const Tensor&amp; t : from.tensors) {
    Tensor tmp(t.dtype());
    TF_RETURN_IF_ERROR(copy(t, &amp;tmp));
    to-&gt;tensors.push_back(tmp);
  }
  return Status::OK();
}
</code></pre>
<h4><a class="header" href="#baseremoterendezvous" id="baseremoterendezvous">BaseRemoteRendezvous</a></h4>
<p>BaseRemoteRendezvous 的RecvAsync中会检查是否是同一个recv 和sender是否在同一个worker上。</p>
<pre><code class="language-cpp">// 检查是否是同一个worker
bool BaseRemoteRendezvous::IsSameWorker(DeviceNameUtils::ParsedName src,
                                        DeviceNameUtils::ParsedName dst) {
  return DeviceNameUtils::IsSameAddressSpace(src, dst);
}
</code></pre>
<p>如果是同一个worker的话就采用类似IntraProcessRendezvous方式来处理，否则需要通过远程调RecvFromRemoteAsync。</p>
<pre><code class="language-cpp">void BaseRemoteRendezvous::RecvAsync(const ParsedKey&amp; parsed,
  //other code ..
  //case1: 是同一个worker, 说明在本地上
  if (IsSameWorker(parsed.src, parsed.dst)) {
    local_-&gt;RecvAsync(
        parsed, recv_args,
        [this, parsed, done](
        //other code ... 
        //in recv done callback
        SameWorkerRecvDone(parsed, send_args, recv_args, in, out,
  } else {
  //case2: 不是同一个worker需要用RPC 去取。
    RecvFromRemoteAsync(parsed, recv_args, std::move(done));
  }
</code></pre>
<p>RemoteRendezvous中加了个一个Initialize的接口, 这样绑定了一个WorkerSession, 然后在SameWorkerRecvDone的时候，通过这个workerSession去找到对应的device。</p>
<pre><code class="language-cpp">Status BaseRemoteRendezvous::Initialize(WorkerSession* session) {
//other codes...
}
</code></pre>
<p>在SameWorkerRecvDone中通过workerSession找到src_device和dst_device</p>
<pre><code class="language-cpp">void BaseRemoteRendezvous::SameWorkerRecvDone(
  //other code ...
  Status s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.src_device, &amp;src_device);
  //other code ...
  s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.dst_device, &amp;dst_device);
  //other code ..
  //通过ViaDMA实现各个device之间的copy
  CopyTensor::ViaDMA(parsed.edge_name, send_args.device_context,
</code></pre>
<h4><a class="header" href="#rpcremoterendezvous" id="rpcremoterendezvous">RpcRemoteRendezvous</a></h4>
<p>RpcRemoteRendezvous在BaseRemoteRendezvous的基础上，实现了RecvFromeRemoteAsync的功能, 首先找到send所在的src_worker, 
然后通过rpc调用去取的远程src_worker上的tensor。</p>
<pre><code class="language-cpp">void RpcRemoteRendezvous::RecvFromRemoteAsync(
  //other code..
  RpcRecvTensorCall* call = get_call_freelist()-&gt;New();

  //1. 找到远程的src_worker
  WorkerSession* sess = session();
  WorkerInterface* rwi = sess-&gt;worker_cache-&gt;CreateWorker(call-&gt;src_worker_);

  //2. 找到要copy到的device
  s = sess-&gt;device_mgr-&gt;LookupDevice(parsed.dst_device, &amp;dst_device);

 //other code ..
  //3. Grpc call
  call-&gt;Init(rwi, step_id_, parsed.FullKey(), recv_args.alloc_attrs, dst_device,
             recv_args, std::move(done));
  call-&gt;Start([this, call]() {
 //other code ..
</code></pre>
<p>在RpcRecvTensorCall中会call worker的RecvTensorAsync。</p>
<pre><code class="language-cpp">  void StartRTCall(std::function&lt;void()&gt; recv_done) {
   //other code
    wi_-&gt;RecvTensorAsync(&amp;opts_, &amp;req_, &amp;resp_, std::move(cb));
  }
</code></pre>
<p>中间经过worker service，最终会去call GrpcWorker::GrpcRecvTensorAsync.</p>
<pre><code class="language-cpp">void GrpcWorker::GrpcRecvTensorAsync(CallOptions* opts,
    // Case 1: 如果目标tensor在GPU上的话，需要先cp到host上
    if (src_dev-&gt;tensorflow_gpu_device_info() &amp;&amp; (!on_host)) {
        StatusCallback copy_ready = [response, done, copy, is_dead](const Status&amp; s) {  
            //other code ..
            // copy到response buffer中
            grpc::EncodeTensorToByteBuffer(is_dead, *copy, response);
            done(s);
        }
        GPUUtil::CopyGPUTensorToCPU(src_dev, send_dev_context, &amp;val, copy, copy_ready);
        } else {
        //Case 2: 在Host上直接cp到response的buffer中。
            grpc::EncodeTensorToByteBuffer(is_dead, val, response);
            done(Status::OK());
        }
    }
</code></pre>
<h3><a class="header" href="#rendezvousmgr" id="rendezvousmgr">RendezvousMgr</a></h3>
<p>RendezvousMgr的作用是维护一个从step_id到Rendezvous的映射。</p>
<blockquote>
<p>RendezvousMgr keeps track of a set of local rendezvous instances.
All tensors sent by this worker are buffered in a RendezvousMgr
until the tensor is received.  Each global unique &quot;step_id&quot;
corresponds to one local rendezvous instance managed by a
RendezvousMgr.</p>
</blockquote>
<p>RendezvousMgr的继承关系如下
<img src="tensorflow/./images/rendezvous_mgr.jpeg" alt="rendezvous mgr" /></p>
<p>映射的table在BaseRendezvousMgr中。</p>
<pre><code class="language-cpp">  //BaseRendezvousMgr的数据成员
  typedef gtl::FlatMap&lt;int64, BaseRemoteRendezvous*&gt; Table;
  mutex mu_;
  Table table_ GUARDED_BY(mu_);
</code></pre>
<p>它的派生类比如RpcRendezvousMgr通过override它的Create函数来创建自己版本的rendezvous。</p>
<pre><code class="language-cpp">  //BaseRendezvousMgr 的CreateRendezvous的纯虚函数
 protected:
  virtual BaseRemoteRendezvous* Create(int64 step_id,
                                       const WorkerEnv* worker_env) = 0;
</code></pre>
<h2><a class="header" href="#tensorflow-device" id="tensorflow-device">Tensorflow Device</a></h2>
<h3><a class="header" href="#摘要-3" id="摘要-3">摘要</a></h3>
<p>Device包含了自己的memory的计算单元，它是对GPU， TPU， CPU等计算device统一抽象，主要的接口有以下几个：</p>
<ol>
<li>GetAllocator: 这个返回一个allocator，负责在device上分配memory</li>
<li>Compute，ComputeAsync: 负责执行OpKernel中的运算。</li>
<li>ResourceMgr: 负责管理分配在Device上的Variable</li>
<li>tensorflow device thread pool: 调度执行device compute的线程池。</li>
</ol>
<p>其中1，2最重要，分别负责allocate memory和执行opkernel的compute。</p>
<h3><a class="header" href="#device" id="device">Device</a></h3>
<h4><a class="header" href="#device的继承关系" id="device的继承关系">Device的继承关系</a></h4>
<p><img src="tensorflow/./images/device_inherit.jpeg" alt="Device继承关系" /></p>
<h4><a class="header" href="#device-thread-pool" id="device-thread-pool">Device thread pool</a></h4>
<p>Gpu对应的线程池创建有三种模式:global, gpu_private, gpu_shared，由环境变量TF_GPU_THREAD_MODE控制, 默认是global的。</p>
<ol>
<li>global: GPU uses threads shared with CPU in the main compute, thread-pool. This is currently the default.</li>
<li>gpu_private: GPU uses threads dedicated to this device.</li>
<li>gpu_shared: All GPUs share a dedicated thread pool.</li>
</ol>
<p>在DirectSession::Ruinternal调用executor的时候，会把device_thread_pool 传给Executor</p>
<pre><code class="language-cpp">//  DirectSession::RunInternal

    thread::ThreadPool* device_thread_pool =
        item.device-&gt;tensorflow_device_thread_pool();
    if (!device_thread_pool) {
      args.runner = default_runner;
    } else {
      args.runner = [this, device_thread_pool](Executor::Args::Closure c) {
        SchedClosure(device_thread_pool, std::move(c));
      };
    }
    item.executor-&gt;RunAsync(args, barrier-&gt;Get());
  }
</code></pre>
<p>在分布式tensorflow中，GraphMgr::StartParallelExecutors, 通过类似的方法吧device_thread_pool 传给executor。</p>
<pre><code class="language-cpp">//GraphMgr::StartParallelExecutors
    thread::ThreadPool* device_thread_pool =
        item.device-&gt;tensorflow_device_thread_pool();
    if (!device_thread_pool) {
      args.runner = default_runner;
    } else {
      args.runner = [this, device_thread_pool](Executor::Args::Closure c) {
        SchedClosure(device_thread_pool, std::move(c));
      };
    }
    item.executor-&gt;RunAsync(args, barrier-&gt;Get());
  }
</code></pre>
<p>在Executor::schedulReady中，会使用这个runner去执行node的process。</p>
<pre><code class="language-cpp">// Executor::ScheduleReady
//Case 1

//other code and 
// Schedule to run all the ready ops in thread pool.
runner_([=]() { Process(tagged_node, scheduled_usec); });

//other code and if...
// Dispatch to another thread since there is plenty of work to
// do for this thread.
runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec));

//other code under some if ...
 // There are inline nodes to run already. We dispatch this expensive
 // node to other thread.
runner_(std::bind(&amp;ExecutorState::Process, this, *curr_expensive_node, scheduled_usec));
</code></pre>
<h3><a class="header" href="#device-context" id="device-context">Device Context</a></h3>
<p>GpuDeviceContext有点复杂，有不少的代码逻辑是用来处理一个GPU 启动了多个streams的，graph中的每个node会分配一个stream_id。</p>
<h4><a class="header" href="#device-context-map" id="device-context-map">device context map</a></h4>
<p>每个node对应OpKernel的device_context会使用这个stream_i来CopyCpuTensorToDevice， CopyDeviceTensorToCpu, 在Compute的时候，opkernel的计算也会这个stream_id对应的stream上执行。</p>
<p><img src="tensorflow/./images/device_context.jpeg" alt="device context" /></p>
<p>不过现在好玩的是现在BaseGPuDevice的构造函数中max_stream传的值为1，使用多个stream的特性没开，大家用的是同一个stream，在stackflow上搜到了一个为啥这么做的回答：</p>
<blockquote>
<p>Yeah, you are looking at code that is a bit stale; we've tried experimenting with multiple compute streams and have found that, so far, it hasn't helped that much in important use cases. We technically support multiple streams, but we never turn it on.</p>
<p>At some point in the future, we want to start playing around with using multiple compute streams again though, so it's nice to have the code there.</p>
<p>Devices can use as many DeviceContexts as they want; on GPU we only use a few and we use FillContextMap to do the mapping, but it really depends on the hardware how they should be used</p>
</blockquote>
<p>目前这个特性是实验性的，在重要的use cases中没起到重要的作用，所以这个特性没开, 后续可能会开，所以这部分代码保留了。</p>
<p>除此之外，还在stream_id的基础上做了一个EigenDevice，估计是给Eigen计算提供的吧。无论怎样，DeviceContext给每个Opkernel包了stream_id，然后在执行的时候，会找到这个stream_id对应的cuda_stream。</p>
<h4><a class="header" href="#eigengpudevice" id="eigengpudevice">Eigen::GpuDevice</a></h4>
<p>给Eigen::GpuDevice封装了一个EigenCudaStreamDevice, 用来给Eigen::GpuDevice allocate和deallocate memroy, 具体的怎么用的估计要去挖Eigen的代码了, 还有scratch buffer的作用也不是很明白。</p>
<pre><code class="language-cpp">class EigenCudaStreamDevice : public ::Eigen::StreamInterface 
  // allocate
  void* allocate(size_t num_bytes) const override{
    //使用device的allocate进行内存分配
  }

  //deallocate
  void deallocate(void* buffer) const override {
   //异步的AsyncFreeData，最终调用的是Device的allocate去free内存
  }
</code></pre>
<h4><a class="header" href="#compute" id="compute">Compute</a></h4>
<p>Gpu的Compute部分主要有BaseGpuDevice::ComputeHelper来处理，主要是如果gpu使用了多个stream特性的话，需要等待input的stream都完成之后，再执行op对应的stream。</p>
<pre><code class="language-cpp">void BaseGPUDevice::ComputeHelper(OpKernel* op_kernel,
  //如果是多个stream,需要等待所有input的stream执行完毕。
  if (num_streams &gt; 1) {
    // If this op's device context is different from the other contexts,
    // we must wait on the stream.
    for (int i = 0; i &lt; context-&gt;num_inputs(); ++i) {
      const GPUDeviceContext* idc =
          static_cast&lt;GPUDeviceContext*&gt;(context-&gt;input_device_context(i));
      //other code: 主要是log
      if (idc-&gt;stream() != stream) stream-&gt;ThenWaitFor(idc-&gt;stream());
    }
  gpu::cuda::ScopedActivateExecutorContext scoped_activation{stream-&gt;parent()};
  op_kernel-&gt;Compute(context);
  //other code: 主要是cuda执行状态检查
</code></pre>
<h3><a class="header" href="#device-factory" id="device-factory">Device Factory</a></h3>
<p>DeviceFactory的继承关系如下：</p>
<p><img src="tensorflow/./images/device_factory.jpeg" alt="image" /></p>
<p>DeviceFactory包含了一些静态函数： AddDevices, NewDevices, Register, GetFactory, 和一个virutal CreateDevices。
NewDevices用于自动化测试，对外主要接口是AddDevices, Register负责device factory的注册, 这两者的调用关系如下：</p>
<p><img src="tensorflow/./images/device_factory_add_devices.jpeg" alt="image" /></p>
<p>DeviceFactory也采用了static registor的方法，自动注册了DeviceFactory,</p>
<pre><code class="language-cpp">//device_type, DeviceFactoryClass, Prority
REGISTER_LOCAL_DEVICE_FACTORY(&quot;CPU&quot;, ThreadPoolDeviceFactory, 60);
REGISTER_LOCAL_DEVICE_FACTORY(&quot;CPU&quot;, GPUCompatibleCPUDeviceFactory, 70);
REGISTER_LOCAL_DEVICE_FACTORY(&quot;GPU&quot;, GPUDeviceFactory, 210);
</code></pre>
<p>这个宏展开后是声明了一个Registrar的 static var, 在它的构造函数中会去调用DeviceFactory的Register注册Factory，
而Register函数最后会把Factory 加入到static device_factories中。</p>
<pre><code class="language-cpp">template &lt;class Factory&gt;
class Registrar {
 public:
  explicit Registrar(const string&amp; device_type, int priority = 50) {
    DeviceFactory::Register(device_type, new Factory(), priority);
  }
}
</code></pre>
<p>在创建一个DirectSesion, 或者GrpServer::Init(每个worker都会起一个GrpcServer)的时候，会调用AddDevices获取worker上的devices.</p>
<h1><a class="header" href="#tensorflow-model-optimize" id="tensorflow-model-optimize">tensorflow model optimize</a></h1>
<h2><a class="header" href="#将keras模型导出为tf-frozen-graph" id="将keras模型导出为tf-frozen-graph">将keras模型导出为tf frozen graph</a></h2>
<h4><a class="header" href="#frozen-keras-model" id="frozen-keras-model">frozen keras model</a></h4>
<p>将keras的h5文件转换为tensorflow的pb文件,  这里面使用了 <code>convert_variables_to_constants</code>将模型中的变量都convert成了常量（方便后续采用quantilize或者tensorrt， 对模型推断部分做进一步的优化）</p>
<pre><code class="language-python">import keras
from keras.layers.core import K
import tensorflow as tf

def frozen_keras_model(keras_model_path, output_node_names, export_path):
    output_node_namess = output_nodes.split(&quot;,&quot;)
    model = keras.models.load_model(keras_model_path)
    print(&quot;the model output nodes is {}&quot;.format(model.outputs))
    with K.get_session() as sess:
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess,
            tf.get_default_graph().as_graph_def(),
            output_nodes_names,
            variable_names_blacklist=['global_step']
        )
        with tf.gfile.Gfile(export_path, &quot;wb&quot;) as f:
            f.write(output_graph_def.SerializeToString())
    
</code></pre>
<p>将<code>global_step</code>放到<code>variable_names_blacklist</code>是因为2中的bug.</p>
<pre><code class="language-python">    variable_names_blacklist=['global_step']
</code></pre>
<p>可以通过print <code>model.outputs</code>来查看keras的输出节点，可以通过tensorboard来看keras模型，然后找到最后的输出节点。一般keras模型的输出节点有好多个（比如训练用的之类的)，预测输出节点为其中的一个。</p>
<h4><a class="header" href="#使用tensorboard展示keras-model对应的graph" id="使用tensorboard展示keras-model对应的graph">使用tensorboard展示keras model对应的graph</a></h4>
<p>首先使用tf summary创建相应的log</p>
<pre><code class="language-python">def keras_model_graph(keras_model_path, log_dir):
    model = keras.model.load_model(keras_model_path)
    with K.get_session() as sess:
        train_writer = tf.summary.FileWriter(log_dir)
        train_writer.add_graph(sess.graph)
</code></pre>
<p>启动tensorboard</p>
<pre><code>$tensorboard --log_dir logdir
</code></pre>
<h3><a class="header" href="#参考文献-1" id="参考文献-1">参考文献</a></h3>
<ol>
<li>
<p><a href="https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">Stackoverflow: How to export Keras .h5 to tensorflow .pb</a></p>
</li>
<li>
<p><a href="https://github.com/tensorflow/tensorflow/issues/14452">BUG: freeze_graph producing invalid graph_def in tensorflow </a></p>
</li>
</ol>
<h2><a class="header" href="#使用dataset-iterator-优化keras-model预测的吞吐量" id="使用dataset-iterator-优化keras-model预测的吞吐量">使用dataset iterator 优化keras model预测的吞吐量</a></h2>
<h3><a class="header" href="#predict_on_generator" id="predict_on_generator">predict_on_generator</a></h3>
<p>现在做的项目，需要在短时间内一次性预测一组大量的图片，刚开始的时候，采用了keras的predict_on_generator和Sequnce，速度比一个个feed dict的形式快了不少, 但是吞吐量还是没达到要求，感觉还有优化的地方。</p>
<pre><code class="language-python">class BatchSequnce(Sequence):
    def __len__(self):
        # 返回batch总个数
        return self.batch_count

    def __getitem__(self, idx):
        #返回一个batch的数据
        #这里可能会做一些数据预处理的工作，比如将图片从文件中加载到内存中然后做特征预处理
        pass
</code></pre>
<pre><code class="language-python"> model = keras.load_model(model_path)
 generator = BatchSequnce(....)
 ret = model.predict_generator(
         generator=generator,
         steps=None,
         workers=10,
         verbose=True,
 )
</code></pre>
<h3><a class="header" href="#dataset" id="dataset">Dataset</a></h3>
<p>经分析, GPU每次都要等 BatchSequnce的<code>__getitem___</code>处理完之后，才能fetch到数据，如果<code>__getitem__</code>做了比较耗时间的操作的话，会让GPU一直在等待, 而且GPU在处理每个Batch数据的时候，都要等一次, tensorflow的Prefech感觉可以缓解这个问题，后来尝试了下，所消耗的时间优化到了以前的70%左右。</p>
<h4><a class="header" href="#使用iterator-改造keras模型" id="使用iterator-改造keras模型">使用iterator 改造keras模型</a></h4>
<ol>
<li>
<p>首先采用<a href="tensorflow/./export-keras-model-as-tf-frozen-graph.html">将keras模型导出为tf frozen graph</a>中的方式，将Keras的h5模型转换成tensorflow的pb文件。</p>
</li>
<li>
<p>使用<code>tf.data.Iterator.from_structure</code>(可重新初始化迭代器可以通过多个不同的 Dataset 对象进行初始)的形式, 声明iterator的输出dtype和TensorShape,</p>
</li>
<li>
<p>调用<code>tf.import_graph_def</code> 导入模型, 导入的时候，使用input_map将placeholde,比如&quot;input&quot;替换成Dataset的itereator next_element</p>
</li>
</ol>
<p>这部分代码如下</p>
<pre><code class="language-python">    def load_model(self, sess, frozen_model_file):
        with tf.name_scope(&quot;dataset&quot;):
            iterator = tf.data.Iterator.from_structure(
                    tf.float32,
                    tf.TensorShape([self.batch_size, 450, 450, 3]))
            next_element = iterator.get_next()
            next_element = tf.convert_to_tensor(next_element, tf.float32)

        with tf.gfile.GFile(frozen_model_file, &quot;rb&quot;) as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())

        tf.import_graph_def(
                graph_def,
                name=&quot;&quot;,
                input_map={&quot;input_1:0&quot;: next_element})
        output_op_name = &quot;y&quot;
        output_op = sess.graph.get_operation_by_name(output_op_name).outputs[0]
        return iterator, output_op
</code></pre>
<h3><a class="header" href="#设计dataset" id="设计dataset">设计DataSet</a></h3>
<p>这里面需要注意的时候, 真正的map函数需要采用py_func包一层, 同事指定py_func的输出tensor shape, 这里的num_map_parall一般取cpu的个数.</p>
<pre><code class="language-python">class DataSetFactory(object):
    def make_dataset(self):
        def generator():
            #返回要处理的文件路径, 或者坐标等
            yield [x, y, w, h]

        output_types = (tf.float32)
        output_shapes = (tf.TensorShape([4]))
        ds = tf.data.Dataset.from_generator(
                generator,
                output_types,
                output_shapes=output_shapes)

        ds = ds.map(lambda region: self.map_func(region), num_map_parall=80)

        ds = ds.prefetch(buffer_size=self.batch_size * 256)
        ds = ds.batch(self.batch_size)
        ds = ds.prefetch(buffer_size=self.batch_size * 10)

        return ds

    def map_func(self, region):
        def do_map(region):
            # 加载图片和预处理
            return img_data
        # 这里采用了py_func，可以执行任意的Python函数，同时需要后面通过reshape的方式设置
        # image_data的shape。
        img_data = tf.py_func(do_map, [region], [tf.float64])
        img_data = tf.reshape(img_data, [450, 450, 3])
        img_data = tf.cast(img_data, tf.float32)
        return image_data
</code></pre>
<h4><a class="header" href="#prefetch_to_device" id="prefetch_to_device">prefetch_to_device</a></h4>
<p>tensorflow 后来加了prefetch_to_device, 经测试可以提高5%左右的效率吧,但是和structure iterator初始化的时候有冲突，因此这个地方把它去掉了。</p>
<pre><code class="language-python"># 由于prefech_to_device必须是dataset的最后一个处理单元，
# structure iterator用这个ds初始化的时候会有问题，
# 因此这个地方将prefetch_to_gpu注释掉了
# gpu_prefetch = tf.contrib.data.prefetch_to_device(
#         &quot;/device:GPU:0&quot;,
#         buffer_size=self.batch_size * 10)
# ds = ds.apply(gpu_prefetch)
</code></pre>
<h4><a class="header" href="#使用dataset初始化iterator" id="使用dataset初始化iterator">使用dataset初始化iterator</a></h4>
<pre><code class="language-python">    def init_iterator(self, dataset):
        # 这里的output_op就是load_model时返回的iterator
        init_iterator_op = self.iterator.make_initializer(dataset)
        self.sess.run(init_iterator_op)

    def predict(self):
        # 这里的output_op就是load_model时返回的output_op
        while True:
            outputs = self.sess.run(self.output_op)
</code></pre>
<h2><a class="header" href="#统计gpucpu利用率脚本" id="统计gpucpu利用率脚本">统计gpu,cpu利用率脚本</a></h2>
<pre><code class="language-bash">#!/bin/bash
start=$(date +%s)
while [ 1 ]
do
    cpu=$(awk -v a=&quot;$(awk '/cpu /{print $2+$4,$2+$4+$5}' /proc/stat; sleep 1)&quot; '/cpu /{split(a,b,&quot; &quot;); print 100*($2+$4-b[1])/($2+$4+$5-b[2])}'  /proc/stat)
    seconds=$(expr $(date +%s) - $start)
    gpu_util=$(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader,nounits)
    echo &quot;$seconds, $cpu, $gpu_util&quot;
    #sleep 1
done

</code></pre>
<h1><a class="header" href="#pthread" id="pthread">pthread</a></h1>
<h1><a class="header" href="#pthread-primer-笔记" id="pthread-primer-笔记">Pthread primer 笔记</a></h1>
<h2><a class="header" href="#进程和线程" id="进程和线程">进程和线程</a></h2>
<h4><a class="header" href="#在kernel中process的context" id="在kernel中process的context">在kernel中process的context</a></h4>
<ul>
<li>cpu相关：program counter pointer, stack top pointer, cpu general registers, sates.</li>
<li>内存：memory map</li>
<li>user: uid, gid, euid, egid, cwd.</li>
<li>信号: signal dispatch table</li>
<li>File: file descriptors</li>
</ul>
<img alt="process-struct" src="pthread/./images/process-struct.jpeg" width=500px/>
<h4><a class="header" href="#thread的context-data" id="thread的context-data">thread的context data</a></h4>
<ul>
<li>cpu相关：program counter pointer, stack top pointer, cpu general registers, sates.</li>
<li>内存相关: stack</li>
</ul>
<img alt="process-struct" src="pthread/./images/thread-struct.jpeg" width=500px/>
<p><b>线程的stack是分配在process的heap上的</b></p>
<pre><code class="language-cpp">//设置和获取线程的stack address
include &lt;pthread.h&gt;
int pthread_attr_setstack(pthread_attr_t *attr, void* stackaddr, size_t stacksize);
int pthread_attr_getstack(const pthread_attr_t* attr, void** stackaddr, size_t* stacksize);
</code></pre>
<p><b>整个进程只有一份signal dispatch table</b></p>
<p>所以signal 中断的时候，说不准会中断到那个thread里面，需要加signal mask来处理。</p>
<h4><a class="header" href="#使用thread的好处" id="使用thread的好处">使用thread的好处</a></h4>
<ol>
<li><code>context switch</code>: process的上下文切换比thread的context switch 耗时间.</li>
<li><code>memory share</code>: thread之间的通信，共享process的内存，file等资源比process之间的通信，share内存方便.</li>
</ol>
<h2><a class="header" href="#线程调度和生命周期" id="线程调度和生命周期">线程调度和生命周期</a></h2>
<h4><a class="header" href="#线程调度" id="线程调度">线程调度</a></h4>
<p>线程有两种调度方式，一种是完全在user space, 由thread库做调度，优点是省了system call 从而省下了从user space 到kernel space的切换, 比较快，缺点是，有一个线程挂在IO上后，整个process都会被挂起.(可以把block的system call 改成nonblock的，使用asyc io来解决这个问题).</p>
<p>另外一种是kernel 实现的light weight process(lwp), lwp避免了整个线程被挂起的缺点，但是需要从user space 到kernel space的切换, 比完全user space实现的线程慢一点。</p>
<p>现实中这两种的实现的方式可以混合起来， 混合方式如下：</p>
<ul>
<li>多个线程对应一个lwp</li>
<li>一个线程对应一个lwp</li>
<li>多个线程对应多个lwp</li>
</ul>
<p>在pthread 中可以这么设置调度的属性:</p>
<pre><code class="language-cpp">//pthread中设置调度scope
//PTHREAD_SCOPE_SYSTEM 表示system 全局的， PTHREAD_SCOPE_PROCESS 表示process scope的。
pthread_attr_t attr;
pthread_attr_init(&amp;attr);
pthread_setscope(&amp;atttr, PTHREAD_SCOPE_SYSTEM);
pthread_create(&amp;tid, &amp;attr, foo, NULL);
</code></pre>
<p><b>影响线程调度的一些属性</b></p>
<ul>
<li>scope: PTHREAD_SCOPE_PROCESS, PTHREAD_SCOPE_GLOBAL</li>
<li>policy: SCHED_RR, SCHED_FIFO, SCHED_OTHER</li>
<li>priority</li>
<li>inheritance</li>
</ul>
<p><b>线程状态以及状态之间的迁移关系如下图：</b></p>
<img alt="threads states" src="pthread/./images/thread-states.jpeg" width=500px/>
<p><b>四种running中的线程被切出去的状况</b></p>
<ul>
<li>synchronization 线程require lock的失败被挂在lock的sleep queue上。</li>
<li>preemption 被抢占了，T1在运行的时候，一个更高优先级的线程T2到了runnable的状态, T1会被T2抢占了。</li>
<li>yielding. 线程T1主动调用sched_yield, 如果有和T1优先权一样的T2线程，就切换到T2线程，如果没有，T1就接着运行。</li>
<li>time-slicing. T1的时间片用完了，和T1有同样优先权的T2接着运行。</li>
</ul>
<h4><a class="header" href="#创建和退出线程" id="创建和退出线程">创建和退出线程</a></h4>
<pre><code class="language-cpp">//create
int pthread_create(pthread_t* thread, const pthread_attr_t* attr, void*(* start_routine)(void*), void* arg);
//exit
void pthread_exit(status);
</code></pre>
<p>线程的返回值，一种是函数执行结束后，直接return的值，另外一种是pthread_exit(status)这个的返回值。</p>
<h4><a class="header" href="#join-等待线程执行结束" id="join-等待线程执行结束">join: 等待线程执行结束</a></h4>
<p>join之后线程会处于阻塞状态直到等待的线程T1执行完毕，join之后t1线程的相关内存会被清理掉，所以说一个子线程只能被join一次.</p>
<p>设置线程的属性为joinable</p>
<pre><code class="language-cpp">pthread_t thread_id;
pthread_attr_t attr;
pthread_attr_init(&amp;attr);
pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_JOINABLE);
pthread_create(&amp;thread_id, &amp;attr, work, (void*)arg);
</code></pre>
<p>阻塞等待线程的执行结果，获取线程的返回结果</p>
<pre><code class="language-cpp">//等待t1线程执行结束, exit_status 是子线程的返回值.
pthread_join(t1, &amp;exit_status)
</code></pre>
<p>joinable线程和detehced线程的区别是线程结束的时候，资源(线程对应的标识符pthread_t, 线程返回信息)该怎么释放.</p>
<p>对于joinable线程t1, 只有当其他线程对t1调用了pthread_join之后, 线程t1才会释放所占用的资源, 否则 会进入类似于进程的zombile状态，这些资源不会被会回收掉.</p>
<h4><a class="header" href="#使用信号量-等待线程执行结束" id="使用信号量-等待线程执行结束">使用信号量 等待线程执行结束</a></h4>
<p>使用信号量等待一堆子线程执行结束, 在主线程里面调用thread_signle_barrier, 然后子线程结束的时候调用<code>SEM_POST(barrier)</code></p>
<pre><code class="language-cpp">void thread_signle_barrier(sem_t* barrier, int count){
    while( count &gt; 0) {
        SEM_WAIT(barrier);
        count--;
    }
}
</code></pre>
<h4><a class="header" href="#detach" id="detach">detach</a></h4>
<p>如果想要t1线程执行结束收系统自动回收t1的资源, 而不是通过调用pthread_join回收资源(会阻塞线程), 我们可以将线程设置为deteched, 有三种方式可以设置线程为deteched.</p>
<ul>
<li>创建线程时指定线程的 detach 属性: pthread_attr_setdetachstate(&amp;attr, PTHREAD_CREATE_DETACHED);</li>
<li>通过在子线程中调用 pthread_detach(pthread_self());</li>
<li>在主线程中调用 pthread_detach(thread_id);(非阻塞, 执行完会立即会返回)</li>
</ul>
<h4><a class="header" href="#取消线程的执行" id="取消线程的执行">取消线程的执行</a></h4>
<p>在pthread中可以通过pthread_cancel(t1)来取消线程t1的执行, 这个会设置线程t1的cancel state, 由线程t1在自己在cancel point 检查是否退出线程, 在退出线程的时候会执行cleanup stack中的函数(比如释放自己hold的锁). 一般会block的函数调用，比如sem_wait, pthread_cond_wait或者会block的系统调用前后检查check point.</p>
<p>如下代码段：</p>
<pre><code class="language-cpp">void cleanup_lock2(void* arg){
    pthread_mutex_unlock((pthread_mutex_t*)arg)
}

void thread1_run(){
    pthread_mutex_lock(&amp;answer_lock);
    pthread_cleanup_push(cleanup_lock2, (void*)&amp;answer_lock);
    while(!firest_thread_to_find_answer) {
        pthread_cond_wait(&amp;cvn, &amp;answer_lock);
    }
    pthread_cleanup_pop(0)
}
</code></pre>
<p>也可以通过<code>pthread_setcanceltype</code>设置为异步取消PTHREAD_CANCEL_ASYNCHRONOUS，这样会给t1线程发送<code>SIGCANCEL</code>信号，t1线程在信号处理函数中结束自己的执行。</p>
<h4><a class="header" href="#signal-信号处理" id="signal-信号处理">Signal 信号处理</a></h4>
<p>Linux 多线程应用中，每个线程可以通过调用 pthread_sigmask() 设置本线程的信号掩码, pthread_kill像某个线程发送signal.</p>
<h5><a class="header" href="#signal-handler-异步的方式处理信号" id="signal-handler-异步的方式处理信号">signal handler 异步的方式处理信号</a></h5>
<p>多线程处理signal时候需要注意事项</p>
<ul>
<li>信号处理函数尽量只执行简单的操作，譬如只是设置一个外部变量，其它复杂的操作留在信号处理函数之外执行；</li>
<li>errno 是线程安全，即每个线程有自己的 errno，但不是异步信号安全。如果信号处理函数比较复杂，且调用了可能会改变 errno 值的库函数，必须考虑在信号处理函数开始时保存、结束的时候恢复被中断线程的 errno 值；</li>
<li>信号处理函数只能调用可以重入的 C 库函数(只能调用async safe 的函数)；譬如不能调用 malloc（），free（）以及标准 I/O 库函数等；</li>
<li>信号处理函数如果需要访问全局变量，在定义此全局变量时须将其声明为 volatile，以避免编译器不恰当的优化</li>
</ul>
<h5><a class="header" href="#sigwait-同步串行方式" id="sigwait-同步串行方式">sigwait, 同步串行方式</a></h5>
<p>等待信号的到来，以串行的方式从信号队列中取出信号进行处理.</p>
<pre><code class="language-cpp">void signal_hanlder_thread() {
    sigemptyset(&amp;waitset);  
    sigaddset(&amp;waitset, SIGRTMIN);  
    sigaddset(&amp;waitset, SIGUSR1);  
    while (1)  {  
        //串行的方式处理信号
        rc = sigwaitinfo(&amp;waitset, &amp;info);  
        if (rc != -1) {  
        sig_handler(info.si_signo);  
    }
}
</code></pre>
<h3><a class="header" href="#thread-local-storage" id="thread-local-storage">Thread local storage</a></h3>
<p>TLS是只在线程自己可见的全局数据, 而不必担心别的线程会改变这个全局数据, 比如要实现每个线程对db的connection单例模式的话，可以把线程的全局connection单例变量存在TLS中。 在使用中有两种方式，一个是pthread_key的方式，另外一个是使用gcc提供的<code>__thread</code>.</p>
<h4><a class="header" href="#thread-specific-data" id="thread-specific-data">Thread Specific Data</a></h4>
<pre><code class="language-code">pthread_keycreate
pthread_setspecific
pthread_getspecific
</code></pre>
<h4><a class="header" href="#__thread" id="__thread"><code>__thread</code></a></h4>
<p><code>__thread</code>是gcc提内置的attr, 它只能用于修饰POD类型，不能修饰class类型，因为它无法自动调用构造函数和析构函数。 <code>__thread</code>每个线程都有一份独立的实体，线程之间相互不影响.</p>
<pre><code class="language-cpp">int g_var; // 全局变量
__thread int t_var; //thread变量
</code></pre>
<h2><a class="header" href="#线程的同步" id="线程的同步">线程的同步</a></h2>
<h3><a class="header" href="#atomic-指令" id="atomic-指令">atomic 指令</a></h3>
<p>线程执行的时候，在两个指令之间，随时都可能会被抢占掉, 所以需要一个atomic的指令来避免这种状况.</p>
<h4><a class="header" href="#atomic-test-and-set-style-ldstub" id="atomic-test-and-set-style-ldstub">atomic test and set style: ldstub</a></h4>
<p>ldstub (load and store unsigned byte) 就是一个atomic test and set的指令, 从内存中载入一个unsigned字节，并且把内存中那个字节设置为1.</p>
<p><b>一个mutex lock的实现</b></p>
<pre><code>try_agin: ldstub address -&gt; register
          compare register, 0
          branch_equal got_it
          call go_to_sleep
          jump try_again
got_it:  return
</code></pre>
<p>从这儿可以看到，线程从go_to_sleep返回之后，需要去重新获取lock, 如果获取失败，就接着go_to_sleep.</p>
<h3><a class="header" href="#basic-primitive" id="basic-primitive">basic primitive</a></h3>
<p>所有线程之前shared的数据需要被用lock保护起来，比如全局数据，传入到另外一个线程的Data struct， 还有static数据。</p>
<h4><a class="header" href="#mutex-lock互斥锁" id="mutex-lock互斥锁">mutex lock(互斥锁)</a></h4>
<p>线程获取mutex lock失败以后，会被放到mutex对应的sleep队列中。</p>
<pre><code class="language-cpp">pthread_mutex_lock
//critical section
pthread_mutex_unlock
</code></pre>
<img alt="mutex lock sleep queue" src="pthread/./images/mutex-lock.jpeg" width=500px/>
<p>另外一种非阻塞的获取锁的方法<code>pthread_mutex_trylock</code> 如果获取锁成功返回0，否则返回<code>EBUSY</code>.</p>
<h4><a class="header" href="#semaphores信号量" id="semaphores信号量">semaphores(信号量)</a></h4>
<p>信号量机制用于协调多个资源的使用(比如一个队列或者缓冲区)，semaphores的值表示可用资源的数量(队列中可用资源的个数)。常用于解决生产者和消费者问题.</p>
<pre><code>// 初始化
int sem_init(sem_t *sem, int pshared, unsigned int val);
// 没有可用的信号量就等待，否则
int sem_wait(sem_t *sem);
// 释放一个信号量，信号量的值加1
int sem_post(sem_t *sem);
</code></pre>
<p>信号量处理流程</p>
<img src="pthread/./images/semaphore-flow.jpeg" width=400px/>
<p>生产者消费者问题, 假设队列的长度是20:</p>
<img src="pthread/./images/producer-consumer.jpeg" width=340px/>
<pre><code class="language-cpp">#include &lt;semaphore.h&gt;

//shared global vars
sem_t sem_producer;
sem_t sem_consumer;
//list

void producer(){
    while(1){
        sem_wait(sem_consumer);
        pthread_mutex_lock(list_lock);
        add(list);
        pthread_mutex_unlock(list_lock);
        sem_post(sem_producer);
    }
}

void consumer(){
    while(1) {
        sem_wait(sem_producer);
        pthread_mutex_lock(list_lock);
        consume(list);
        pthread_mutex_unlock(list_lock);
        sem_post(sem_consumer);
    }
}
void main(){
    sem_init(&amp;sem_producer, 0);
    sem_init(&amp;sem_consumer, 20);

    pthread_t producer_tid;
    pthread_t consumer_tid;

    pthread_create(&amp;producer_tid, nullptr, producer, nullptr);
    pthread_create(&amp;consumer_tid, nullptr, consumer, nullptr);
}
</code></pre>
<h4><a class="header" href="#condition-var-条件变量" id="condition-var-条件变量">condition var (条件变量)</a></h4>
<p>condition var 的流程, condition var 访问需要用个mutex lock保护起来, condition判断失败之后，会unlock 保护condition var 的lock, 然后进入sleep, 之后被唤醒的时候，会再次去获取condition var的lock。</p>
<img src="pthread/./images/condition-flow.jpeg" width=400px/>
<pre><code class="language-cpp">&lt;code&gt;
// 初始化
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
// 动态初始化
int pthread_cond_init(pthread_cond_t* restrict cond, const pthread_condattr_t* restrict attr);

//销毁
int pthread_cond_destroy(pthread_cond_t* cond);

//等待
int pthread_cond_wait( pthread_cond_t*   restrict cond, pthread_mutex_t*  restrict mutex );
int pthread_cond_timedwait( pthread_cond_t*         restrict cond, pthread_mutex_t*        restrict mutex, const struct timespec*  restrict abstime );


// 通知
// singal 函数一次只能唤醒一个线程, 而 broadcast 会唤醒所有在当前条件变量下等待的线程.
int pthread_cond_broadcast(pthread_cond_t* cond);
int pthread_cond_signal(pthread_cond_t* cond);

</code></pre>
<p>wait for condition</p>
<pre><code class="language-cpp">// safely examine the condition, prevent other threads from
// altering it
pthread_mutex_lock (&amp;lock);
while ( SOME-CONDITION is false)
    pthread_cond_wait (&amp;cond, &amp;lock);

// Do whatever you need to do when condition becomes true
do_stuff();
pthread_mutex_unlock (&amp;lock);
</code></pre>
<p>signal condition</p>
<pre><code class="language-cpp">// ensure we have exclusive access to whathever comprises the condition
pthread_mutex_lock (&amp;lock);

ALTER-CONDITION

// Wakeup at least one of the threads that are waiting on the condition (if any)
pthread_cond_signal (&amp;cond);

// allow others to proceed
pthread_mutex_unlock (&amp;lock)
</code></pre>
<h3><a class="header" href="#read-write-lock-读写锁" id="read-write-lock-读写锁">read write lock (读写锁)</a></h3>
<p>在某个时间内，多个线程可以同时获得读锁, 如果已经有线程获得了读锁，那么尝试获取写锁的将被block, 如果已经有线程获取了读锁，那么其他线程的尝试获取读锁或者写锁将会被block.</p>
<img src="pthread/./images/rw-lock.jpeg" width=400px/>
<pre><code class="language-cpp">pthread_rwlock_t rwlock;
int pthread_rwlock_init(pthread_rwlock_t* restrict rwlock, const pthread_rwlockattr_t * restrict attr);
int pthread_rwlock_destroy(pthread_rwlock_t* rwlock);

// 获取读锁
int pthread_rwlock_rdlock(pthread_rwlock_t* rwlock);
// 获取写锁
int pthread_rwlock_wrlock(pthread_rwlock_t* rwlock);
// 释放锁  
int pthread_rwlock_unlock(pthread_rwlock_t* rwlock);
</code></pre>
<h3><a class="header" href="#spin-lock-自旋锁" id="spin-lock-自旋锁">Spin lock (自旋锁)</a></h3>
<p>多次trylock, 如果失败了再block, 它的出发点是trylock这个指令的时间很短（比如2us)然后mutex block一次可能需要42us,所以它先尝试几次, 如果在这几us内，lock被释放了，那么能够成功的获取锁了。</p>
<pre><code class="language-cpp">spin_lock(mutex_t* m) {
    for(int i = 0; i &lt; SPIN_COUNT; i++) {
        if (pthread_mutex_trylock(m) != EBUSY) {
            return;
        }
    }
    pthread_mutex_lock(m);
    return;
}
</code></pre>
<p><b>Adaptive Spin lock</b></p>
<p>在很多kernel里面使用的，kernel先看拥有锁的线程在不在running(如果在跑的话，那么线程可能短时间内会释放这个锁，所以值得spin几次去尝试下), 如果不在running 状态的话，就直接去require lock了,然后线程会被block.</p>
<p>使用spin lock的时候，需要好好的评估下到底值不值得，就是critical section hold住lock的时间会不会很长。。如果一般很短的话，值得用spin lock，否则的话用spin lock反而浪费时间。</p>
<h3><a class="header" href="#barriers" id="barriers">Barriers</a></h3>
<pre><code class="language-cpp">pthread_barrier_t mybarrier;
//初始化
pthread_barrier_init(&amp;mybarrier, NULL, THREAD_COUNT + 1);
pthread_barrier_destroy(&amp;mybarrier);
pthread_barrier_wait(&amp;mybarrier);
</code></pre>
<p>等待最后一个线程达到barrier点。
<img src="pthread/./images/barrier-wait.jpeg" width=500px/></p>
<h2><a class="header" href="#附录" id="附录">附录</a></h2>
<ol>
<li>linux中的process的virutal memory layout 参见<a href="http://www.enseignement.polytechnique.fr/informatique/INF583/INF583_5.pdf">Processes and Memory Management</a></li>
</ol>
<img src="pthread/./images/process-memeroy-address-layout.png" width=500px/>
<h2><a class="header" href="#参考-1" id="参考-1">参考</a></h2>
<ol>
<li><a href="http://www8.cs.umu.se/kurser/TDBC64/VT03/pthreads/pthread-primer.pdf">pthread primer</a></li>
<li><a href="http://www.enseignement.polytechnique.fr/informatique/INF583/INF583_5.pdf">Processes and Memory Management</a></li>
<li><a href="http://blog.zhangjikai.com/2016/04/25/%E3%80%90Pthreads%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E3%80%91%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/">pthread学习笔记, 基本使用</a></li>
</ol>
<h2><a class="header" href="#glibc的pthread实现代码研读-1-线程的生命周期" id="glibc的pthread实现代码研读-1-线程的生命周期">Glibc的pthread实现代码研读 1: 线程的生命周期</a></h2>
<p>本文主要包含pthread线程在linux上的创建，执行，exit, detach, join, cancel, thread local storage。</p>
<h3><a class="header" href="#pthread_t" id="pthread_t">pthread_t</a></h3>
<p>struct pthread定义在<code>nptl/descr.h</code>中, 这边抽几组主要的field来说明下(这里为了方便描述，对field在struct的顺序做了重新的编排)。</p>
<p>首先是创建完线程之后，系统给的id和各种flag attribute.</p>
<pre><code class="language-cpp">/* Flags.  Including those copied from the thread attribute.  */
 int flags;
 pid_t tid;
 /* Lock to synchronize access to the descriptor.  */
 int lock;
</code></pre>
<p>然后最显而易见的是, 线程要执行的函数指针，函数参数以及函数执行的结果, 这几个字段会在线程的入口start_thread中用到。对于result字段: pthread_join(t1, &amp;status), 这个会等待线程t1执行结束，然后把结果放到status中。</p>
<pre><code class="language-cpp"> //保存线程返回结果
  void *result;
 // 线程执行的函数和参数
  void *(*start_routine) (void *);
  void *arg;
</code></pre>
<p>然后一些field用于处理下面这几种异常情况: 线程如果抛异常了，线程调用pthread_exit提前exit了，线程被其它线程pthread_cancel了。</p>
<pre><code class="language-cpp">// 线程cancel的状态
int cancelhandling;
// 线程被cancel的时候，处理cleanup callback和cleanup jmp
struct _pthread_cleanup_buffer* cleanup;
struct pthread_unwind_buf* cleanup_jmp_buf;
/* Machine-specific unwind info.  */
struct _Unwind_Exception exc;
</code></pre>
<p>标明线程是被join的还是已经deteched字段, 这个字段涉及到线程的pthread struct该什么时候释放。</p>
<pre><code class="language-cpp"> struct pthread* joinid;
 #define IS_DETACHED(pd) ((pd)-&gt;joinid == (pd))
</code></pre>
<p>stack相关的field, 在ALLOCATE_STACK和回收statck的时候会用到，由于pthread的这个struct也是放在stack上的，因此需要一些参数记录pthread的offset, user_statck表示是否是由用户提供的stack。</p>
<pre><code class="language-cpp">/* True if the user provided the stack.  */
 bool user_stack;
 void *stackblock;
 size_t stackblock_size;
 /* Size of the included guard area.  */
 size_t guardsize;
 /* This is what the user specified and what we will report.  */
 size_t reported_guardsize;
</code></pre>
<p>thread specific data相关的字段</p>
<pre><code class="language-cpp">
// 用于thread specific data, thread local storage
struct pthread_key_data
{
  uintptr_t seq;
  void* data;
} specific_1stblock[PTHREAD_KEY_2NDLEVEL_SIZE];

struct pthread_key_data* specific[PTHREAD_KEY_1STLEVEL_SIZE];
</code></pre>
<p>最后调度策略和调度参数相关的字段，在线程create的时候，会调用sched_setaffinity， sched_setscheduler让系统设置这些参数。</p>
<pre><code class="language-cpp"> // 调度策略和调度参数
 struct sched_param schedparam;
 int schedpolicy;
</code></pre>
<h4><a class="header" href="#pthread-struct-的alloc和free" id="pthread-struct-的alloc和free">pthread struct 的alloc和free</a></h4>
<p><code>nptl/allocatestatck.c</code> 中的<code>allocate_stack</code>和<code>__deallocate_stack</code>负责alloc和free pd struct。如果用的是系统分配的stack话， pthread有个stack list，当alloc的时候，从这个stack list中取出一个，然后在free的时候，把这个stack放回到stack list中。</p>
<p>这就导致了一个问题, pthread_t 并不适合作为线程的标识符，比如下面两个线程的pthread_t的地址是一样的(参考自Linux 多线程服务端编程: 4.3节):</p>
<pre><code class="language-cpp">int main() {
    pthread_t t1, t2;
    pthread_create(&amp;t1, NULL, threadFunc, NULL);
    printf(&quot;%lx\n&quot;, t1);
    pthread_join(t1, NULL);

    pthread_create(&amp;t2, NULL, threadFunc, NULL);
    printf(&quot;%lx\n&quot;, t2);
    pthread_join(t2, NULL);

}
</code></pre>
<h3><a class="header" href="#pthread_create" id="pthread_create">pthread_create</a></h3>
<p>pthread create 首先分配线程的栈，并在这个栈上划出一片内存给pthread struct, 然后调syscall clone(2) 创建一个线程，创建的新的线程会从<code>START_THREAD_DEFF</code> 这个入口开始执行起来，最后线程的执行结果保存在pd-&gt;result里面， 用户可以通过pthread_attr_setstack来指定线程stack的内存，也可以直接使用系统的内存。</p>
<img src="pthread/./images/pthread-create.jpeg" />
分配stack, 使用用户提供的stack或者系统分配一个stack(pd 这个struct也存放在stack里面了)
<pre><code class="language-cpp">ALLOCATE_STACK(iattr, &amp;pd)
</code></pre>
<p><code>create_thread</code> 调用linux系统接口clone创建线程, 如果线程要指定在某个CPU上跑的话，调用sched_setaffinity设置好cpuset, 最后何止好调度策略和调度参数。</p>
<pre><code class="language-cpp">ARCH_CLONE(&amp;start_thread, STACK_VARIABLES_ARGS, clone_flags, pd, &amp;pd-&gt;tid, tp, &amp;pd-&gt;tid)

INTERNAL_SYSCALL(sched_setaffinity, err, 3, pd-&gt;tid, attr-&gt;cpusetsize, attr-&gt;cpuset)

INTERNAL_SYSCALL(sched_setscheduler, err, 3, pd-&gt;tid, pd-&gt;schedpolicy, &amp;pd-&gt;schedparam)
</code></pre>
<p>其中clone 的flags如下：</p>
<pre><code class="language-cpp">const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM
              | CLONE_SIGHAND | CLONE_THREAD
              | CLONE_SETTLS | CLONE_PARENT_SETTID
              | CLONE_CHILD_CLEARTID
              | 0);
</code></pre>
<p><code>CLONE_THREAD</code>, 标明是创建一个线程，和创建者同一个group,  同一个parent。</p>
<p><code>STACK_VARIABLES_ARGS</code>对应着上一步ALLOCATE_STACK分配好的内存地址, 这块内存会作为新的线程的stack来用。</p>
<p>clone中的的start_thread就是线程的entry_point, 这个函数定义在nptl/pthread_create.c里面 <code>START_THREAD_DEFF</code>, 这个函数就是新创建的线程的入口。</p>
<h3><a class="header" href="#start-thread" id="start-thread">start thread</a></h3>
<p>start thread是线程的入口， 在跑用户函数之前，会设置一个jmp point, 之后等线程执行结束的时候(调用pthread_exit, 或者线程被cancel掉的时候)，会longjump 回到这个函数, 接着做线程执行完的清理工作。</p>
<p>如果线程是Deteched， 那么线程的pd结构就会被释放掉(因为pthread返回的status指针是保存在pd-&gt;result这个里面的)，否则就要等pthread_join完之后释放掉。</p>
<p>最后线程exit_thread之后，会把pd中的tid设置为0，这样就可以唤醒等待join该线程结束的线程。</p>
<img src="pthread/./images/start-thread.jpeg" />
<ol>
<li>设置好unwind buffer, do cancel的时候可以跳回来</li>
</ol>
<pre><code class="language-cpp">int not_first_call;
 not_first_call = setjmp ((struct __jmp_buf_tag* ) unwind_buf.cancel_jmp_buf);
 if (__glibc_likely (! not_first_call))
   {
     THREAD_SETMEM (pd, cleanup_jmp_buf, &amp;unwind_buf);
</code></pre>
<p>setjmp和longjmp是非局部跳转函数, 它可以在在栈上跳过若干调用帧，返回到当前函数调用路径上的某一个函数中, 若直接调用则返回0，若从longjmp调用返回则返回非0值的longjmp中的val值。之后的do_cancel可能会longjmp到这个地方。</p>
<ol start="2">
<li>调用用户提供的函数, 结果存在<code>pd-&gt;result</code>中</li>
</ol>
<pre><code class="language-cpp">#ifdef CALL_THREAD_FCT
      THREAD_SETMEM (pd, result, CALL_THREAD_FCT (pd));
#else
      THREAD_SETMEM (pd, result, pd-&gt;start_routine (pd-&gt;arg));
#endif
</code></pre>
<ol start="3">
<li>做一些清理工作，清理TLS, 标记stack为可复用状态，如果线程是detached, 则释放pd struct的内存, 否则要在pthread_join里面释放这个pb struct, 如果一个线程既不是deteched，也没有线程在pthread_join等待他，这个pb struct就不会被释放，进入一个类似于zombile的状态。</li>
</ol>
<pre><code class="language-cpp">__call_tls_dtors ();
/* Run the destructor for the thread-local data.  */
__nptl_deallocate_tsd ();
/* Clean up any state libc stored in thread-local variables.  */
__libc_thread_freeres ();
if (IS_DETACHED (pd))
    __free_tcb (pd);

// mark stack resuable
char *sp = CURRENT_STACK_FRAME;
size_t freesize = (sp - (char *) pd-&gt;stackblock) &amp; ~pagesize_m1;
assert (freesize &lt; pd-&gt;stackblock_size);
if (freesize &gt; PTHREAD_STACK_MIN)
  __madvise (pd-&gt;stackblock, freesize - PTHREAD_STACK_MIN, MADV_DONTNEED);

// other code
__exit_thread ();
</code></pre>
<h3><a class="header" href="#pthread_exit" id="pthread_exit">pthread_exit</a></h3>
<p>猜测pthread_exit 的do_cancel的unwind会调用pthread_cleanup_push中注册的cleaup函数，最后会longjmp回到start_thread里面的setjmp那块，继续执行线程结束后的清理工作。</p>
<pre><code class="language-cpp">__pthread_exit (void* value)
{
  THREAD_SETMEM (THREAD_SELF, result, value);

  __do_cancel ();
}
</code></pre>
<p>do_cancel定义如下:</p>
<pre><code class="language-cpp">__do_cancel (void)
{
  struct pthread* self = THREAD_SELF;

  THREAD_ATOMIC_BIT_SET (self, cancelhandling, EXITING_BIT);
  __pthread_unwind ((__ pthread_unwind_buf_t*)
		    THREAD_GETMEM (self, cleanup_jmp_buf));
}
</code></pre>
<h3><a class="header" href="#pthread_join" id="pthread_join">pthread_join</a></h3>
<p><code>pthread_join(t1, &amp;result)</code> 线程会调用lll_wait_tid等到t1执行结束，然后从t1的pd-&gt;result获取线程返回的结果, 返回给status，最后释放线程t1对应的pd sturct.</p>
<ol>
<li>检查是否有死锁, 避免等待自己，以及正在被cancel的线程，</li>
</ol>
<pre><code class="language-cpp">if ((pd == self
       || (self-&gt;joinid == pd
	   &amp;&amp; (pd-&gt;cancelhandling
	       &amp; (CANCELING_BITMASK | CANCELED_BITMASK | EXITING_BITMASK
		  | TERMINATED_BITMASK)) == 0))
      &amp;&amp; !CANCEL_ENABLED_AND_CANCELED (self-&gt;cancelhandling))
result = EDEADLK;
</code></pre>
<ol start="2">
<li>设置<code>t1-&gt;joinid = self;</code></li>
</ol>
<pre><code class="language-cpp">/* Wait for the thread to finish.  If it is already locked something
     is wrong.  There can only be one waiter.  */
  else if (__builtin_expect (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;joinid,
								   self,
								   NULL), 0))
    /* There is already somebody waiting for the thread.  */
    result = EINVAL;
</code></pre>
<ol start="3">
<li>等待t1线程执行结束, 这里的lll_wait_tid 最后会去调用linux提供的futex, 会被挂起来，一直等到t1的tid变为0。</li>
</ol>
<pre><code class="language-cpp">    /* Wait for the child.  */
    lll_wait_tid (pd-&gt;tid);
</code></pre>
<ol start="4">
<li>free t1线程的pd struct</li>
</ol>
<pre><code>pd-&gt;tid = -1;

     /* Store the return value if the caller is interested.  */
     if (thread_return != NULL)
   *thread_return = pd-&gt;result;


     /* Free the TCB.  */
     __free_tcb (pd);
</code></pre>
<h3><a class="header" href="#pthread_detach" id="pthread_detach">pthread_detach</a></h3>
<p>标记线程为detached, 把pd的jionid改为自己。</p>
<pre><code class="language-cpp">  int result = 0;
  /* Mark the thread as detached.  */
  if (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;joinid, pd, NULL))
    {
      if (IS_DETACHED (pd))
	      result = EINVAL;
    }
  else if ((pd-&gt;cancelhandling &amp; EXITING_BITMASK) != 0)
      __free_tcb (pd);
  return result;

</code></pre>
<h3><a class="header" href="#pthread_cancel" id="pthread_cancel">pthread_cancel</a></h3>
<p>pthread_cancel 只是把<code>pd-&gt;cancelhandling</code>的状态记为<code>CANCLEING_BITMASK|CANCELED_BITMASK</code>。</p>
<pre><code class="language-cpp">do{
    oldval = pd-&gt;cancelhandling;
    newval = oldval | CANCELING_BITMASK | CANCELED_BITMASK;
    //other code

} while (atomic_compare_and_exchange_bool_acq (&amp;pd-&gt;cancelhandling, newval,
                          oldval);
</code></pre>
<p>然后在pthread_testcancel的时候，才真正的调用do_cancel去cancel thread.</p>
<pre><code class="language-cpp">//pthread_testcancel --&gt; CANCELLATION_P

if (CANCEL_ENABLED_AND_CANCELED (cancelhandling))			      \
     {									      \
   THREAD_SETMEM (self, result, PTHREAD_CANCELED);			      \
   __do_cancel ();							      \
     }			
</code></pre>
<p>或者一些会check cancel point的调用比如pthread_cond_wait里面，会去检查这个标记，</p>
<pre><code class="language-cpp">pthread_cond_wait --&gt;futex_wait_cancelable --&gt; pthread_enable_asynccancel --&gt;  __do_cancel
futex_reltimed_wait_cancelable --&gt; pthread_enable_asynccancel --&gt; __do_cancel
sem_wait_common -&gt; futex_abstimed_wait_cancelable --&gt; pthread_enable_asynccancel --&gt; __do_cancel
</code></pre>
<h3><a class="header" href="#singal-handle" id="singal-handle">singal handle</a></h3>
<h2><a class="header" href="#glibc的pthread实现代码研读-2-线程同步" id="glibc的pthread实现代码研读-2-线程同步">Glibc的pthread实现代码研读 2: 线程同步</a></h2>
<p>第二部分主要讲述pthread中的线程的同步方法包括mutex, sem, condition var, rwlock, barrier的实现，pthread使用了linux的futex来实现这些同步方法。</p>
<h3><a class="header" href="#futex" id="futex">futex</a></h3>
<p>pthread中的locks通过linux的futex(faster user space locking)实现, lock放在process之间的共享内存中, pthread通过atomic的指令来对这个lock进行dec, inc, load and test 等操作, 如果有竞态冲突的时候获取锁失败的时候，才会去sys call 调用linux底层的do_futex, 底层把线程放到futex对应的wait队列里面, 然后挂起线程等待被唤醒。</p>
<p>由于只有竞态冲突的时候才需要syscall, 其他情况都不需要，因此节省了很多sys call，这样比较快。</p>
<img src="pthread/./glibc-pthread-images/pthread-lock-overview.jpeg" width=300px/>
<h4><a class="header" href="#mutex" id="mutex">Mutex</a></h4>
<p>xchgl 这个是atomic操作吧，失败了回去调用do_futex, flag 是FUTEX_WAIT</p>
<pre><code>phtread_mutex_lock --&gt; LL_MUTEX_LOCK --&gt; ll_lock --&gt; lll_lock_wait|lll_lock_wait_private --&gt; xchgl

</code></pre>
<h4><a class="header" href="#sem" id="sem">Sem</a></h4>
<h4><a class="header" href="#condition-var" id="condition-var">Condition var</a></h4>
<h4><a class="header" href="#read-write-lock" id="read-write-lock">Read write lock</a></h4>
<h4><a class="header" href="#barrier" id="barrier">Barrier</a></h4>
<h1><a class="header" href="#react" id="react">react</a></h1>
<h2><a class="header" href="#react中state-render到html-dom的流程分析" id="react中state-render到html-dom的流程分析">React中state render到html dom的流程分析</a></h2>
<h3><a class="header" href="#questions" id="questions">Questions</a></h3>
<ol>
<li>React的component的lifecycle 在react中是怎么被调到的.</li>
<li>分析jsx =&gt; element tree =&gt; fiber tree =&gt; html dom在react中的流程.</li>
<li>react中的fiber tree的建立和执行, 以及异步的schedule.</li>
</ol>
<p><img src="react/./images/react-questions.jpeg" alt="react-questions" /></p>
<h3><a class="header" href="#研究工具和方法" id="研究工具和方法">研究工具和方法</a></h3>
<ul>
<li>chrome debug 打断点</li>
<li><a href="https://github.com/ggreer/the_silver_searcher">ag the silver searcher</a>, 源代码全局搜索.</li>
<li>猜测它的实现原理，打log, call trace验证, console.log, console.trace;</li>
</ul>
<h3><a class="header" href="#准备工作" id="准备工作">准备工作</a></h3>
<p>代码下载,编译</p>
<pre><code class="language-bash">$ git clone git@github.com:facebook/react.git
$ cd react
$ yarn install
$ gulp react:extract-errors
$ yarn build
</code></pre>
<h3><a class="header" href="#component-lifecycle-callback" id="component-lifecycle-callback">Component lifeCycle callback</a></h3>
<p>准备最简单的组件HelloWorld</p>
<pre><code class="language-jsx">import React from &quot;react&quot;
import ReactDom from &quot;react-dom&quot;

class HelloWorld extends React.Component{
    constructor(props){
        super(props);
        this.state = {
            message: &quot;hello, world&quot;
        }
    }

    componentWillMount(){
        console.log(&quot;component will mount&quot;);
    }

    componentWillUpdate(){
        console.log(&quot;component will update&quot;);
    }

    componentDidUpdate(){
        console.log(&quot;component did update&quot;);
    }

    componentDidMount(){
        console.log(&quot;componentDidMount&quot;);
    }

    render(){
        return &lt;span className={this.state.message}&gt;
            {this.state.message}
        &lt;/span&gt;;
    }
}
ReactDom.render(&lt;HelloWorld/&gt;, document.getElementById(&quot;app&quot;));
</code></pre>
<p>在<code>componentWillMount</code>, <code>componentDidMount</code>, <code>componentWillUpdate</code>, <code>componentDidUpdate</code>中打个断点</p>
<p><img src="react/./images/react-component-life-cycle-callback.jpeg" alt="" /></p>
<h3><a class="header" href="#创建html-dom的callstack" id="创建html-dom的callstack">创建html dom的callstack</a></h3>
<p>react中最后一定会去调用<code>document.createElement</code>去创建html的dom节点，所以把document.createElement这个方法覆盖了，加了一层log.</p>
<pre><code class="language-javascript">var originCreateElement = document.createElement;
document.createElement = function() {
    if (arguments[0] === 'span'){
        console.log('create span');
    }
   return originCreateElement.apply(document, arguments);
}
</code></pre>
<p>然后打断点，得到的callstack如下:</p>
<p><img src="react/./images/react-create-dom.jpeg" alt="" /></p>
<h3><a class="header" href="#call-flow-整理" id="call-flow-整理">call flow 整理</a></h3>
<p>函数间的callflow 整理如下
<img src="react/./images/react-lifecycle-call-flow.jpeg" alt="" /></p>
<p>函数所属模块之间的call flow 整理如下</p>
<p><img src="react/./images/react-module-call-flow.jpeg" alt="" /></p>
<h2><a class="header" href="#fiber" id="fiber">Fiber</a></h2>
<h4><a class="header" href="#fiber的设计思想" id="fiber的设计思想">fiber的设计思想</a></h4>
<p>在<a href="https://github.com/acdlite/react-fiber-architecture">react-fiber-artchitecture</a> 中作者描述了fiber的设计思想，简单来说，每个fiber就是一个执行单元，可以任意的修改它的优先级，可以pause 它，之后再继续执行（感觉很像进程线程的概念）。</p>
<p>实际中执行一个fiber可以生成下一步要执行的fiber，然后fiber执行之前可以检查时候js跑的时间时候用完了，如果用完了，就挂起来，等待下次requestIdleCallback/requestAnimationFrame的callback, schedule 开始接着上次结束的地方继续执行js code.</p>
<p>相当于把以前的js function 的call stack 改成fiber chain了。</p>
<p><img src="react/./images/fiber-flow.jpeg" alt="" /></p>
<p><code>workLoop</code> 函数主要逻辑如下（注，删除了错误处理和其他不相干的<code>if else</code> 分支)
performWork</p>
<pre><code class="language-javascript">// ReactScheduler.js workLoop
if (deadline !== null &amp;&amp; priorityLevel &gt; TaskPriority) {
      // The deferred work loop will run until there's no time left in
      // the current frame.
      while (nextUnitOfWork !== null &amp;&amp; !deadlineHasExpired) {
        if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
          nextUnitOfWork = performUnitOfWork(nextUnitOfWork);
          if (nextUnitOfWork === null &amp;&amp; pendingCommit !== null) {
           // If we have time, we should commit the work now.
           if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
             commitAllWork(pendingCommit);
             nextUnitOfWork = findNextUnitOfWork();
             // Clear any errors that were scheduled during the commit phase.
           }
         }
       }
   }
  }
</code></pre>
<h4><a class="header" href="#schedule" id="schedule">schedule</a></h4>
<p>schedule 有同步和异步的，同步的会一直执行，直到fiber tree被执行结束，不会去检查time限制和priorityLevel的问题，异步的有两类权限，一个是animation的，一类是HighPriority, OffScreen Priority这个会有个deadline.</p>
<p><img src="react/./images/schedule-update.jpeg" alt="schedule-update" /></p>
<p>在preformwork的末尾会去检查nextLevelPriority的优先权，然后根据优先权异步的schedule.</p>
<pre><code class="language-javascript">switch (nextPriorityLevel) {
      case SynchronousPriority:
      case TaskPriority:
        // Perform work immediately by switching the priority level
        // and continuing the loop.
        priorityLevel = nextPriorityLevel;
        break;
      case AnimationPriority:
        scheduleAnimationCallback(performAnimationWork);
        // Even though the next unit of work has animation priority, there
        // may still be deferred work left over as well. I think this is
        // only important for unit tests. In a real app, a deferred callback
        // would be scheduled during the next animation frame.
        scheduleDeferredCallback(performDeferredWork);
        break;
      case HighPriority:
      case LowPriority:
      case OffscreenPriority:
        scheduleDeferredCallback(performDeferredWork);
        break;
    }
</code></pre>
<h4><a class="header" href="#fiber类型" id="fiber类型">fiber类型</a></h4>
<p>FunctionalComponent, ClassComponent 对应着用户创建的Component, HostRoot, HostComponent, HostPortal, HostText这些是和平台相关的组件。对于web来说就是 div, span这些dom元素了。</p>
<pre><code class="language-javascript">// ReactTypeOfWork.js
module.exports = {
  IndeterminateComponent: 0, // Before we know whether it is functional or class
  FunctionalComponent: 1,
  ClassComponent: 2,
  HostRoot: 3, // Root of a host tree. Could be nested inside another node.
  HostPortal: 4, // A subtree. Could be an entry point to a different renderer.
  HostComponent: 5,
  HostText: 6,
  CoroutineComponent: 7,
  CoroutineHandlerPhase: 8,
  YieldComponent: 9,
  Fragment: 10,
};
</code></pre>
<h3><a class="header" href="#fiber执行的三个阶段" id="fiber执行的三个阶段">fiber执行的三个阶段</a></h3>
<p><code>react</code>中的<code>fiber</code>执行的执行主要分为三个阶段</p>
<ol>
<li>
<p><code>beginWork</code>: fiber展开（把ClassComponent render开来，最后展开到fiber tree的叶子节点都是hostComponent)</p>
</li>
<li>
<p><code>completeWork</code>: 计算fiber之间的diff, 底层的dom元素的创建，以及dom tree的建立，还有事件绑定。</p>
</li>
<li>
<p><code>commitWork</code>: 调用host接口，把fiber的diff更新到host上去</p>
</li>
</ol>
<h4><a class="header" href="#begin-work-fiber-tree-的展开" id="begin-work-fiber-tree-的展开">begin work: fiber tree 的展开</a></h4>
<p>每次的beginWork(fiber), 会把fiber的所有直接子节点展开（这里只展开一层, 不会递归的去展开子节点的子节点）</p>
<pre><code class="language-javascript">function performUnitOfWork(workInProgress: Fiber): Fiber | null {
   const current = workInProgress.alternate;
   let next = beginWork(current, workInProgress, nextPriorityLevel);

   if (next === null) {
     next = completeUnitOfWork(workInProgress);
   }
   return next;
 }
</code></pre>
<p>在workloop里面会把beginWork创建的子节点接着传给beginWork，继续展开fiber tree</p>
<pre><code class="language-javascript">//workLoop
while (nextUnitOfWork !== null &amp;&amp; !deadlineHasExpired) {
       if (deadline.timeRemaining() &gt; timeHeuristicForUnitOfWork) {
         nextUnitOfWork = performUnitOfWork(nextUnitOfWork);
</code></pre>
<p><img src="react/./images/begin-work-create-fiber.jpeg" alt="" /></p>
<h4><a class="header" href="#completework-创建dom元素计算diff" id="completework-创建dom元素计算diff">completeWork 创建dom元素，计算diff</a></h4>
<p>创建的<code>instance</code>(对于html来说，就是dom节点), 存储在<code>workInProgress.stateNode</code> 里面, 计算好的props diff存放在了<code>workInProgress.updateQueue</code>，在下一个阶段commitWork 会把这个updateQueue里面的patch提交到host。</p>
<p><img src="react/./images/completework-flow.jpeg" alt="" /></p>
<h4><a class="header" href="#commitwork-提交diff" id="commitwork-提交diff">commitWork 提交diff</a></h4>
<p>在commitUpdate中取WorkInprogress.updateQueue,然后调用Dom操作把diff apply上去</p>
<p><img src="react/./images/commit-work.jpeg" alt="" /></p>
<h1><a class="header" href="#hotspot" id="hotspot">hotspot</a></h1>
<h2><a class="header" href="#在osx下编译调试hotspot" id="在osx下编译调试hotspot">在osx下编译调试hotspot</a></h2>
<h3><a class="header" href="#摘要-4" id="摘要-4">摘要</a></h3>
<p>本文主要描述了在osx下编译hotspot debug版本以方便后续的hotspot代码研读，并尝试了使用gdb和lldb对hotspot进行debug。解决了Debug的时候会遇到的SIGSEGV问题，最后确定用lldb脚本来debug hotspot。</p>
<h3><a class="header" href="#准备工作-1" id="准备工作-1">准备工作</a></h3>
<ol>
<li>安装freetype</li>
</ol>
<pre><code class="language-bash">$brew install freetype
</code></pre>
<ol start="2">
<li>获取openjdk repo代码</li>
</ol>
<pre><code class="language-bash">$git clone https://github.com/dmlloyd/openjdk.git
</code></pre>
<ol start="3">
<li>configure然后make slowdebug 版本, 开启<code>--with-native-debug-symbols=internal</code>选项以保留debug-symols</li>
</ol>
<pre><code class="language-bash">$bash ./configure  --with-target-bits=64 --with-freetype-include=/usr/X11/include/freetype2 --with-freetype-lib=/usr/X11/lib --disable-warnings-as-errors --with-debug-level=slowdebug  --with-native-debug-symbols=internal

$make
</code></pre>
<h3><a class="header" href="#gdb-调试" id="gdb-调试">GDB 调试</a></h3>
<p>准备好HelloWorld.java, 然后用javac编译</p>
<pre><code class="language-java">public class HelloWorld {
    public static void main(String[] args) {
        System.out.println(&quot;hello,world&quot;);
    }
}
</code></pre>
<p>准备gdb 调试脚本, 这里面的file指向第一步编译好的java</p>
<pre><code>$sudo gdb -x hello.gdb
</code></pre>
<p>里面的hello.gdb内容如下：</p>
<pre><code>//hello.gdb
file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
handle SIGSEGV nostop noprint pass

# break points
break java.c:JavaMain
break InitializeJVM
break LoadJavaVM
break ContinueInNewThread

#in javaMain, after InitializeJVM
break java.c:477
commands
print &quot;vm is&quot;
print **vm
print &quot;env is&quot;
print **env
end

run HelloWorld
</code></pre>
<p>gdb脚本中的 break java.c:477 commands ... end 在到达断点的时候，会去执行commands中的命令，这样感觉非常方便~~. 在这边可以看到运行完InitializeJVM之后，vm和env这两个都初始化好了。</p>
<p>vm初始化之后是这样的, 绑定了几个函数指针, env中绑定的函数指针太多了，在此就不列举了。</p>
<pre><code>{reserved0 = 0x0, reserved1 = 0x0, reserved2 = 0x0,
    DestroyJavaVM = 0x104939bb0,
    AttachCurrentThread = 0x104939e20,
    DetachCurrentThread = 0x10493a2d0,
    GetEnv = 0x10493a470,
    AttachCurrentThreadAsDaemon = 0x10493a770
</code></pre>
<p>gdb 调试在mac下会有些问题，libjvm这个so中的符号看不到，无法打断点，网上研究了不少时间，最后发现是osx sierra和gdb兼容性问题，最后搞了半天，感觉太麻烦了。只好放弃，改用lldb.</p>
<h3><a class="header" href="#lldb-调试" id="lldb-调试">lldb 调试</a></h3>
<p><a href="https://developer.apple.com/library/content/documentation/IDEs/Conceptual/gdb_to_lldb_transition_guide/document/lldb-basics.html">lldb</a>调试和gdb很类似. lldb类似的脚本如下, 感觉比gdb清晰些，但是也啰嗦了些~~。</p>
<p>由于lldb只有在进程跑起来的时候，才能加<code>process handle xxx</code>, 所以在main上加一个breakpoint，在那个时候把hanlde SIGSEGV这个加上，忽略SIGSEGV信号。 lldb中通过breakpoing command add 这个加断点的时候要执行的命令，以DONE作为结束。</p>
<pre><code class="language-gdb">file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;

#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

run HelloWorld
process handle SIGSEGV --notify false --pass true --stop false
</code></pre>
<p>通过下面命令执行lldb debug的脚本</p>
<pre><code class="language-sh">$lldb -s helloworld.lldb
</code></pre>
<h2><a class="header" href="#hotspot代码研读-jvm-初始化时创建的线程" id="hotspot代码研读-jvm-初始化时创建的线程">Hotspot代码研读: jvm 初始化时创建的线程</a></h2>
<h3><a class="header" href="#摘要-5" id="摘要-5">摘要</a></h3>
<p>本文通过在pthread_create方法上打断点的方式，得到了jvm初始化的时候创建的线程。然后对里面主要线程JavaThread, VMThread， CompilerThread, GCthread 做了简要的分析。</p>
<h3><a class="header" href="#创建线程的callstack" id="创建线程的callstack">创建线程的callstack</a></h3>
<p>由于创建线程最终肯定会调用pthread_create方法，所以为了研究jvm启动的时候，创立了哪些线程，准备了下面的lldb调试脚本。在pthread_create方法上打断点，然后用bt命令打印callstack， 然后continue接着执行, 去打印下一个pthread_create的callstack, 这样最后就可以得到所有的pthread_create的callstack了。</p>
<p>HelloWorld.java</p>
<pre><code class="language-java">public class HelloWorld {
    public static void main(String[] args) {
        System.out.println(&quot;hello,world&quot;);
    }
}
</code></pre>
<p>HelloWorld.lldb</p>
<pre><code class="language-sh">//hello.lldb
file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;
#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

breakpoint set --name pthread_create
breakpoint command add
bt
continue
DONE

run HelloWorld
</code></pre>
<p>执行lldb 脚本</p>
<pre><code class="language-bash">#编译HelloWorld.java
$javac HelloWorld.java
#执行lldb脚本
$lldb -s HelloWorld.lldb
</code></pre>
<p>最后得到的<a href="java/./src/pthread_create_bt.log">pthread_create_bt.log</a>, pthread call stack关系整理如下图:</p>
<img src="java/./images/jvm-threads-pthread-callstack.jpeg" />
<h3><a class="header" href="#线程创建的过程" id="线程创建的过程">线程创建的过程</a></h3>
<p>main是java的laucher入口，在<code>main-&gt; JLI_LAUCH -&gt; LoadJavaVM </code>中会调用dlopen加载libjvm的so, 设置好JNI_CreateJavaVm的函数指针.</p>
<pre><code class="language-cpp">// main -&gt; JLI_LAUCH -&gt; LoadJavaVM:
// load libjvm so
#ifndef STATIC_BUILD
    libjvm = dlopen(jvmpath, RTLD_NOW + RTLD_GLOBAL);
#else
    libjvm = dlopen(NULL, RTLD_FIRST);
#endif
//other codes
ifn-&gt;CreateJavaVM = (CreateJavaVM_t)
       dlsym(libjvm, &quot;JNI_CreateJavaVM&quot;);
</code></pre>
<p>然再<code>main-&gt;JLI_LAUCH -&gt; JVMinit -&gt; ContinueInNewThread</code>创建一个新的线程。新的线程开始执行JavaMain函数.</p>
<p>在JavaMain中最终调用Threads::create_vm 创建java vm中的其他线程。</p>
<pre><code class="language-cpp">//JNI_CreateJavaVM jni.cpp:4028
frame #13: thread.cpp:3623: Threads::create_vm(JavaVMInitArgs*, bool*)
frame #14: jni.cpp:3938: JNI_CreateJavaVM_inner(JavaVM_**, void**, void*)
frame #15: jni.cpp:4033: ::JNI_CreateJavaVM(JavaVM **, void **, void *)
frame #16: java.c:1450: InitializeJVM
frame #17: java.c:402: JavaMain
</code></pre>
<h4><a class="header" href="#thread-之间的继承关系" id="thread-之间的继承关系">Thread 之间的继承关系</a></h4>
<p>线程class之间的继承关系如下:</p>
<img src="java/./images/thread-inherit.jpeg"/>
<h3><a class="header" href="#javathread" id="javathread">JavaThread</a></h3>
<p>// TODO</p>
<h3><a class="header" href="#vmthread" id="vmthread">VMThread</a></h3>
<p>// TODO</p>
<h3><a class="header" href="#compilebroker" id="compilebroker">CompileBroker</a></h3>
<p>// TODO</p>
<h2><a class="header" href="#hotspot代码研读-class文件的加载和执行" id="hotspot代码研读-class文件的加载和执行">Hotspot代码研读: class文件的加载和执行</a></h2>
<h4><a class="header" href="#摘要-6" id="摘要-6">摘要</a></h4>
<p>本文首先描述了Helloworld.class文件的结构，然后分析了HelloWorld这个在Java中的类在hotpos jvm对应的instanceKlass实例。然后具体分析了HelloWorld中的<code>static main</code>函数字节码， 以及它被加载以后在JVM中存放的位置。之后描述了字节码解释器TemplateInterpreter初始化过程, 最后分析了static main这个java 代码入口函数被调用的过程, 以及<code>new</code>这个字节码执行的时候具体做了哪些工作。</p>
<h4><a class="header" href="#helloworldclass-字节码分析" id="helloworldclass-字节码分析">HelloWorld.class 字节码分析</a></h4>
<p>这里先准备一个HelloWord.java，main函数里面new了一个HelloWorld对象，然后调用了该对象的一个成员函数hello方法。</p>
<pre><code class="language-java">public class HelloWorld {
    String m_name;
    int m_age = 0;

    public static void main(String[] args) {
        HelloWorld obj = new HelloWorld();
        obj.hello();
    }

    private void hello(){
        m_age ++;
        System.out.println(&quot;hello, world&quot;);
    }
}
</code></pre>
<p>编译完之后，可以用如下命令查看生成的class二进制文件, 包括常量池和方法的字节码。</p>
<pre><code class="language-bash">$javac HelloWorld.java
$javap -v HelloWorld &gt;HelloWorld-javap
</code></pre>
<p>class文件包含两部分，一部分是常量池，另外一部分是方法对应的字节码。常量池包含了这个class中涉及到的字符串，字面常量，methodref, classRef等各种引用。</p>
<pre><code>Constant pool:
   #1 = Methodref          #9.#23         // java/lang/Object.&quot;&lt;init&gt;&quot;:()V
   #2 = Fieldref           #3.#24         // HelloWorld.m_age:I
   #3 = Class              #25            // HelloWorld
   #4 = Methodref          #3.#23         // HelloWorld.&quot;&lt;init&gt;&quot;:()V
   ....
   #10 = Utf8               m_name
   #11 = Utf8               Ljava/lang/String;
</code></pre>
<p>下面HelloWorld.class的方法有三个，<code>&lt;init&gt;&quot;:()V</code>对应着HelloWorld的构造函数，还有main, hello这两个函数，下面主要看下HelloWorld main方法生成的字节码。new之后，做了一个dup(dup的原因是因为后面调用构造函数需要消耗一个，赋值操作也需要消耗一个)，调用了helloWorld的构造函数，对obj做了赋值, 最后调用了hello方法之后就返回了。</p>
<pre><code>  public static void main(java.lang.String[]);
    descriptor: ([Ljava/lang/String;)V
    flags: ACC_PUBLIC, ACC_STATIC
    Code:
      stack=2, locals=2, args_size=1
         0: new           #3                  // class HelloWorld
         3: dup
         4: invokespecial #4                  // Method &quot;&lt;init&gt;&quot;:()V
         7: astore_1
         8: aload_1
         9: invokespecial #5                  // Method hello:()V
        12: return
      LineNumberTable:
        line 6: 0
        line 7: 8
        line 8: 12
}
</code></pre>
<p>在hotspot的<code>vm/interpreter/bytecodes.hpp</code>中定义了一个字节码table，可查到上面各个指令对应的字节码值如下：</p>
<pre><code class="language-cpp">_new                  = 187, // 0xbb
_dup                  =  89, // 0x59
_invokespecial        = 183, // 0xb7
_astore_1             =  76, // 0x4c
_aload_1              =  43, // 0x2b
_invokespecial        = 183, // 0xb7
_return               = 177, // 0xb1
</code></pre>
<p>使用vim编辑<code>HelloWorld.class</code> 这块对应的二进制文件(从bb开头)如下:
<img src="java/./images/helloworld-main-byte-codes.jpeg"  width=400px/></p>
<h4><a class="header" href="#helloworldclass的加载" id="helloworldclass的加载">HelloWorld.class的加载</a></h4>
<p>在HotSpot中由ClassFileParser负责解析class文件，并创建Class对应的instanceKlass实例，首先在<code>vm/classfile/classFileParser.cpp</code>中加入一段代码判断时候是HelloWorld.class的方法, 以方便打断点~~</p>
<pre><code class="language-cpp">InstanceKlass* ClassFileParser::create_instance_klass(bool changed_by_loadhook, TRAPS) {
  if ( _klass != NULL) {
    return _klass;
  }

  InstanceKlass* const ik =
    InstanceKlass::allocate_instance_klass(*this, CHECK_NULL);

  fill_instance_klass(ik, changed_by_loadhook, CHECK_NULL);
  //新加的代码，以在加载HelloWorld.class的时候才打断点
  if (ik-&gt;_name-&gt;index_of_at(0, &quot;HelloWorld&quot;, strlen(&quot;HelloWorld&quot;)) != -1){
      assert(_klass == ik, &quot;invariant&quot;);
  }
  //other code
 }
 __
</code></pre>
<p>然后准备的lldb调试脚本如下：</p>
<pre><code class="language-sh">file /codes/openjdk/build/macosx-x86_64-normal-server-slowdebug/jdk/bin/java
settings set frame-format &quot;frame #${frame.index}: ${line.file.basename}:${line.number}: ${function.name}\n&quot;
#breakpoints
breakpoint set --name main
breakpoint command add
process handle SIGSEGV --notify false --pass true --stop false
continue
DONE

breakpoint set --file classFileParser.cpp --line 5229
breakpoint command add
print *ik
print ik-&gt;_methods-&gt;_data[0]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[0]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[0]-&gt;_constMethod-&gt;code_size()`

print ik-&gt;_methods-&gt;_data[1]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[1]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[1]-&gt;_constMethod-&gt;code_size()`

print ik-&gt;_methods-&gt;_data[2]-&gt;name_and_sig_as_C_string()
memory read  ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_size()`
DONE

run  -Xint HelloWorld
</code></pre>
<p>最后HelloWorld.main对应的的调试lldb输出如下:</p>
<pre><code class="language-log">(lldb)  print ik-&gt;_methods-&gt;_data[2]-&gt;name_and_sig_as_C_string()
(char *) $7 = 0x0000000101002d30 &quot;HelloWorld.main([Ljava/lang/String;)V&quot;
(lldb)  memory read  ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_base() -c `ik-&gt;_methods-&gt;_data[2]-&gt;_constMethod-&gt;code_size()`
0x121faab00: bb 00 03 59 b7 00 04 4c 2b b6 00 05 b1           �..Y�..L+�..�
</code></pre>
<p>通过上面的lldb调试可以看到，对于java中的<code>HelloWorld</code>这个类，hotspot创建了一个对应的<code>InstanceKlass</code>实例（假定为ik), <code>ik-&gt;_methods</code>中包含了helloworld中的方法。 HelloWorld中的方法对应的字节码, 保存在<code>ik-&gt;_methods-&gt;_data[i]-&gt;_constMethod-&gt;code_base()</code>指向保存它的字节码的内存，然后``ik-&gt;_mehods-&gt;_data[i]-&gt;_constMethod-&gt;_consts, 都指向了这个HelloWorld.class中对应的常量池。ConstPool-&gt;_tags这个数组标明了每个常量的类型（比如methodref对应这JVM_CONSTANT_Methodref,等等)</p>
<img src="java/./images/helloworld-main-method.jpeg" />
<h3><a class="header" href="#callstack-分析" id="callstack-分析">callstack 分析</a></h3>
<p>通过<a href="java/./test/helloworld-main-load-stack">callstack</a>可以看到, 一个class的加载要通过下面的流程。SystemDictionary负责保存已经loadedclass的一个map, 如果mapl里面有了，就直接返回，如果没有，就调用classLoader去加载class文件，最后用klassFactory从class文件中创建出InstanceKlass来。</p>
<pre><code>Class.c --&gt; SystemDictionary --&gt; ClassLoader --&gt; klassFactory --&gt; classFileParser--&gt;Inputstream --&gt; HelloWorld.class文件
</code></pre>
<pre><code>frame #0: classFileParser.cpp:5229: ClassFileParser::create_instance_klass
frame #1: klassFactory.cpp:203: KlassFactory::create_from_stream
frame #2: systemDictionary.cpp:1142: SystemDictionary::resolve_from_stream
...
frame #5: ClassLoader.c:150: Java_java_lang_ClassLoader_defineClass1
...
frame #21: systemDictionary.cpp:1586: SystemDictionary::load_instance_class
...
frame #24: systemDictionary.cpp:185: SystemDictionary::resolve_or_fail
...
frame #27: Class.c:135: Java_java_lang_Class_forName0
....
 frame #38: java.c:1543: LoadMainClass
 frame #39: java.c:477: JavaMain
</code></pre>
<h3><a class="header" href="#instanceklass的link" id="instanceklass的link">InstanceKlass的link</a></h3>
<p>常量池中符号的resolve, method的link.
//TODO</p>
<h2><a class="header" href="#interpreter" id="interpreter">Interpreter</a></h2>
<p>在上面加载class的<a href="java/./test/helloworld-main-load-stack">callstack</a>中可以看到有几段的callstack是机器码， 那些是TemplateInterpreter初始化的时候，生成的StubCode(机器代码), java的字节码就是在StubCode中按字节码解释执行（或者直接编译好机器码，直接跑的)~~。</p>
<pre><code class="language-cpp">frame #27: Class.c:135: Java_java_lang_Class_forName0
frame #28: 0x000000010602c838 0x000000010602c838
frame #29: 0x000000010600b220 0x000000010600b220
frame #30: 0x000000010600b220 0x000000010600b220
frame #31: 0x000000010600b220 0x000000010600b220
frame #32: 0x00000001060009f1 0x00000001060009f1
frame #33: javaCalls.cpp:410: JavaCalls::call_helper
frame #34: os_bsd.cpp:3682: os::os_exception_wrapper
frame #35: javaCalls.cpp:306: JavaCalls::call
</code></pre>
<p>JavaCalls::call 这个是从jvm中调java方法的入口。可以在跑代码的时候加个<code>-XX:+PrintInterpreter</code>选项打印这些生成的studecode的代码。</p>
<h4><a class="header" href="#stubqueue的创建" id="stubqueue的创建">StubQueue的创建</a></h4>
<p>在hotspot中有三种解释器：TemplateInterpreter，CppInterprete 还有遗留的bytecodeInterpreter。默认用的是TemplateInterpreter，TemplateInterpreter在初始化的时候会把字节码对应的执行代码通过MASM直接转对应平台(比如X86， X86-64)对应的机器代码, 这部分的机器码作为一个个Stub保存在StubQueue中，除了字节码, method_entry也会生成一个个的stub。 生成Stub的callstack如下：</p>
<pre><code>* frame #0: templateInterpreterGenerator.cpp:57: TemplateInterpreterGenerator::generate_all()
  frame #1: templateInterpreterGenerator.cpp:40: TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue*)
  frame #2: templateInterpreterGenerator.cpp:37: TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue*)
  frame #3: templateInterpreter.cpp:56: TemplateInterpreter::initialize()
  frame #4: interpreter.cpp:116: interpreter_init()
  frame #5: init.cpp:115: init_globals()
  frame #6: thread.cpp:3623: Threads::create_vm(JavaVMInitArgs*, bool*)
  frame #7: jni.cpp:3938: JNI_CreateJavaVM_inner(JavaVM_**, void**, void*)
  frame #8: jni.cpp:4033: ::JNI_CreateJavaVM(JavaVM **, void **, void *)
  frame #9: java.c:1450: InitializeJVM
  frame #10: java.c:402: JavaMain
</code></pre>
<h4><a class="header" href="#分配stubqueue内存" id="分配stubqueue内存">分配StubQueue内存</a></h4>
<p>在<code>TemplateInterpreter::initialize</code>中，首先会去申请一块内存，存放stubcode, 然后下面TemplateInterpreterGenerator的代码都会保存到这块内存里面。</p>
<pre><code class="language-cpp">//TemplateInterpreter::initialize
// generate interpreter
 { ResourceMark rm;
   TraceTime timer(&quot;Interpreter generation&quot;, TRACETIME_LOG(Info, startuptime));
   int code_size = InterpreterCodeSize;
   NOT_PRODUCT(code_size*=4;)  // debug uses extra interpreter code space
   _code = new StubQueue(new InterpreterCodeletInterface, code_size, NULL,
                         &quot;Interpreter&quot;);
   TemplateInterpreterGenerator g(_code);
 }
 __
</code></pre>
<p>code_size 和各个平台是相关的，比如x86平台, 大小为224K。</p>
<pre><code>hotspot/src/cpu/x86/vm/templateInterpreterGenerator_x86.cpp
58:int TemplateInterpreter::InterpreterCodeSize = JVMCI_ONLY(268) NOT_JVMCI(256) * 1024;
60:int TemplateInterpreter::InterpreterCodeSize = 224 * 1024;
</code></pre>
<h4><a class="header" href="#生成字节码对应的-stub" id="生成字节码对应的-stub">生成字节码对应的 stub</a></h4>
<p>在set_entry_points_for_all_bytes里面，会遍历所有的bytecode，根据预先创建好的_template_table（这个表是在TemplateTable::initialize初始化的时候创建的）去生成字节码对应的code。比如字节码_new生成stubcode时候的callstack如下:</p>
<pre><code>   frame #0: templateTable_x86.cpp:3830: TemplateTable::_new()
   frame #1: templateTable.cpp:63: Template::generate(InterpreterMacroAssembler*)
   frame #2: templateInterpreterGenerator.cpp:396: TemplateInterpreterGenerator::generate_and_dispatch(Template*, TosState)
   frame #3: templateInterpreterGenerator_x86.cpp:1814: TemplateInterpreterGenerator::set_vtos_entry_points(Template*, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;)
   frame #4: templateInterpreterGenerator.cpp:364: TemplateInterpreterGenerator::set_short_entry_points(Template*, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;, unsigned char*&amp;)
   frame #5: templateInterpreterGenerator.cpp:329: TemplateInterpreterGenerator::set_entry_points(Bytecodes::Code)
   frame #6: templateInterpreterGenerator.cpp:285: TemplateInterpreterGenerator::set_entry_points_for_all_bytes()
   frame #7: templateInterpreterGenerator.cpp:263: TemplateInterpreterGenerator::generate_all()
</code></pre>
<p>生成字节码对应的stub的函数如下，这里的_gen这个generate就是<code>TemplateInterpreter::_new</code>了。 masm最后的flush会把机器码都flush到StubQueue那边分配的buffer中。</p>
<pre><code class="language-cpp">void Template::generate(InterpreterMacroAssembler* masm) {
  // parameter passing
  TemplateTable::_desc = this;
  TemplateTable::_masm = masm;
  // code generation
  _gen(_arg);
  masm-&gt;flush();
}
</code></pre>
<p>字节码的stub生成完之后，interpreter会有个_normal_table 保存对这些bytecode对应的stubcode的引用, 这儿entry的一堆参数代表了寄存器， 在dispatch_next中会用到这个表。</p>
<pre><code class="language-cpp">EntryPoint entry(bep, zep, cep, sep, aep, iep, lep, fep, dep, vep);
Interpreter::_normal_table.set_entry(code, entry);
</code></pre>
<p>dispatch_next中会去取bytecode对应stubcode的地址，然后在dispatch_base中jmp到字节码对应的code去执行。</p>
<pre><code class="language-cpp">void InterpreterMacroAssembler::dispatch_next(TosState state, int step) {
  // load next bytecode (load before advancing _bcp_register to prevent AGI)
  load_unsigned_byte(rbx, Address(_bcp_register, step));
  // advance _bcp_register
  increment(_bcp_register, step);
  dispatch_base(state, Interpreter::dispatch_table(state));
}
__
</code></pre>
<h4><a class="header" href="#method-entry" id="method-entry">method entry</a></h4>
<p>hotspot中给java的method分了好几类，这样对不同种类的method可以做专门的优化, 比如针对<code>java_lang_math_sin</code>这些常用的数学函数，对应的method entry就直接是sin的汇编代码了。</p>
<p>Method entry的种类定义在了AbstractInterpreter::MethodKind中，通常用的都是zerolocals, 同步的method入口就是<code>zerolocals_synchronized</code>这个了，相应的还有native, native_synchronized， native的方法。部分的methodKind如下:</p>
<pre><code class="language-cpp">    zerolocals,                                                 // method needs locals initialization
   zerolocals_synchronized,                                    // method needs locals initialization &amp; is synchronized
   native,                                                     // native method
   native_synchronized,                                        // native method &amp; is synchronized
   empty,                                                      // empty method (code: _return)
   accessor,                                                   // accessor method (code: _aload_0, _getfield, _(a|i)return)
   abstract,                                                   // abstract method (throws an AbstractMethodException)
   method_handle_invoke_FIRST,                                 // java.lang.invoke.MethodHandles::invokeExact, etc.
   java_lang_math_sin,                                         // implementation of java.lang.Math.sin   (x)

</code></pre>
<p>method entry定义如下：</p>
<pre><code class="language-cpp">#define method_entry(kind)                                              \
  { CodeletMark cm(_masm, &quot;method entry point (kind = &quot; #kind &quot;)&quot;); \
    Interpreter::_entry_table[Interpreter::kind] = generate_method_entry(Interpreter::kind); \
    Interpreter::update_cds_entry_table(Interpreter::kind); \
  }
__
</code></pre>
<p>对于zerolocals_synchronized和zerolocals对应的入口是: <code>generate_normal_entry </code>生成的stubcode, 可以看到, zerolocals_synchronized多了调了一个<code>lock_method</code>, 而且调用了dispatch_next jmp到bytecode对应的stubcode.</p>
<pre><code class="language-cpp">// address TemplateInterpreterGenerator::generate_normal_entry(bool synchronized) {
 const Address constMethod(rbx, Method::const_offset());
 const Address access_flags(rbx, Method::access_flags_offset());
 const Address size_of_parametersrdx,
 const Address size_of_locals(rdx, ConstMethod::size_of_locals_offset());


 // get parameter size (always needed)
 __ movptr(rdx, constMethod);


  //other code
 if (synchronized) {
    // Allocate monitor and lock method
    lock_method();
  }
  //other code
__ notify_method_entry();
  //other code
 __ dispatch_next(vtos);
  //other code
</code></pre>
<p>native方法入口generate_native_entry 生成部分如下, 最终会去call native的方法。</p>
<pre><code class="language-cpp">//address TemplateInterpreterGenerator::generate_native_entry(bool synchronized)
// allocate space for parameters
 __ get_method(method);
 __ movptr(t, Address(method, Method::const_offset()));
 __ load_unsigned_short(t, Address(t, ConstMethod::size_of_parameters_offset()));
//other code
__ call(t);
__ get_method(method);        // slow path can do a GC, reload RBX
//other code
</code></pre>
<h4><a class="header" href="#methodlink" id="methodlink">method::link</a></h4>
<p>这个entry_table中的入口最后会在instancKlass中method::link的时候和method关联起来。</p>
<pre><code>//method::link代码片段
address entry = Interpreter::entry_for_method(h_method);
set_interpreter_entry(entry);

//native functions
if (is_native() &amp;&amp; !has_native_function()) {
  set_native_function(
    SharedRuntime::native_method_throw_unsatisfied_link_error_entry(),
    !native_bind_event_is_interesting);
}

//设置method的入口
void set_interpreter_entry(address entry) {
    assert(!is_shared(), &quot;shared method's interpreter entry should not be changed at run time&quot;);
    if (_i2i_entry != entry) {
      _i2i_entry = entry;
    }
    if (_from_interpreted_entry != entry) {
      _from_interpreted_entry = entry;
    }
  }
</code></pre>
<p>TemplateInterpreter初始画之后，各个table之前的关系图如下:</p>
<img src="java/./images/StubQueue.jpeg"/>
<h3><a class="header" href="#helloworld的static-main的执行" id="helloworld的static-main的执行">HelloWorld的static main的执行</a></h3>
<p>经过上面的分析，再来看java代码中main被执行过程，首先在JavaMain中加载main class，然后获得class的static main method, 最后调用了这个static method，开始执行HelloWorld.class的Main method。</p>
<pre><code class="language-cpp">//JavaMain 代码片段
mainClass = LoadMainClass(env, mode, what);
//...some other code
mainID = (*env)-&gt;GetStaticMethodID(env, mainClass, &quot;main&quot;,
                                   &quot;([Ljava/lang/String;)V&quot;);

//...some other code
(*env)-&gt;CallStaticVoidMethod(env, mainClass, mainID, mainArgs);
</code></pre>
<p>这里先不管LoadMainClass的过程， 主要分析CallStaticVoidMethod这个方法。首先准备下面的lldb调试脚本, 在JavaMain 执行CallStaticVoidMethod之前打个断点，然后再在JavaCalls::call_helper中打个断点。</p>
<pre><code>#just a line before   (*env)-&gt;CallStaticVoidMethod(env, mainClass, mainID, mainArgs);
breakpoint set --file java.c --line 517
breakpoint command add
breakpoint set --method JavaCalls::call_helper
continue
DONE
</code></pre>
<p>可以看到callstack如下</p>
<pre><code>* frame #0: javaCalls.cpp:360: JavaCalls::call_helper(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #1: os_bsd.cpp:3682: os::os_exception_wrapper(void (*)(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*), JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #2: javaCalls.cpp:306: JavaCalls::call(JavaValue*, methodHandle const&amp;, JavaCallArguments*, Thread*)
    frame #3: jni.cpp:1120: jni_invoke_static(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)
    frame #4: jni.cpp:1990: ::jni_CallStaticVoidMethod(JNIEnv *, jclass, jmethodID, ...)
    frame #5: java.c:518: JavaMain
</code></pre>
<p><b>JavaCalls::call_helper</b></p>
<p>在JavaCalls::call_helper中关键代码片段如下， 首先设置好method的entry point， 这个entry point就是上文中所说的interpreter初始化的时候建立的method entry point（两者之间连接是在method::link的时候建立的）。</p>
<pre><code class="language-cpp">//JavaCalls::call_helper代码片段

//设置entry point
address entry_point = method-&gt;from_interpreted_entry();
if (JvmtiExport::can_post_interpreter_events() &amp;&amp; thread-&gt;is_interp_only_mode()) {
  entry_point = method-&gt;interpreter_entry();
}
//other code

// do call
  { JavaCallWrapper link(method, receiver, result, CHECK);
    { HandleMark hm(thread);  // HandleMark used by HandleMarkCleaner

      StubRoutines::call_stub()(
        (address)&amp;link,
        // (intptr_t*)&amp;(result-&gt;_value), // see NOTE above (compiler problem)
        result_val_address,          // see NOTE above (compiler problem)
        result_type,
        method(),
        entry_point,
        args-&gt;parameters(),
        args-&gt;size_of_parameters(),
        CHECK
      );

      result = link.result();  // circumvent MS C++ 5.0 compiler bug (result is clobbered across call)
      // Preserve oop return value across possible gc points
      if (oop_result_flag) {
        thread-&gt;set_vm_result((oop) result-&gt;get_jobject());
      }
    }

    //保存执行结果：
    if (oop_result_flag) {
       result-&gt;set_jobject((jobject)thread-&gt;vm_result());
       thread-&gt;set_vm_result(NULL);
     }
</code></pre>
<p><b>call_stub</b></p>
<p>这个call_stub也是一段汇编代码（定义在StubGenerator_x86_X64.cpp:203由generate_call_stub生成）它在保存好一堆寄存器和栈之后，就把用到的参数都压到寄存器里面，然后调method的entry point去执行，执行完了再把寄存器和栈恢复了。</p>
<p>其中的call Java function部分的汇编代码如下：</p>
<pre><code class="language-c">// call Java function
    __ BIND(parameters_done);
    __ movptr(rbx, method);             // get Method*
    __ movptr(c_rarg1, entry_point);    // get entry_point
    __ mov(r13, rsp);                   // set sender sp
    BLOCK_COMMENT(&quot;call Java function&quot;);
    __ call(c_rarg1);

    BLOCK_COMMENT(&quot;call_stub_return_address:&quot;);
    return_address = __ pc();
</code></pre>
<p><b>JavaCallWrapper</b></p>
<p>在JavaCallWrapper的构造函数中，会申请一个新的JNIHandleBlock，并把它设置为thread的active_handles，在析构函数中会恢复线程之前的JNIHandleBlock，并释放之前申请的JNIHandleBlock。</p>
<p>线程自身维护一个free_handle_block的list， 申请JNIHandleBlock的时候，就从这里面去取，如果freelist用完了，才回去加个mutex lock new一个JNIHanleBlock。释放的时候，就放回到这个list里面。</p>
<p>很多的java code都会去调用vm/prims/jvm.cpp中的JVM_ENTRY，而JVM_ENTRY会用<code>JNIHanleBlock-&gt;make_local</code>保存返回给java代码的的结果。在java code跑完以后,由JavaCallWrapper的destructor负责释放这些内存。</p>
<pre><code class="language-cpp">jobject JNIHandles::make_local(Thread* thread, oop obj) {
  if (obj == NULL) {
    return NULL;                // ignore null handles
  } else {
    assert(Universe::heap()-&gt;is_in_reserved(obj), &quot;sanity check&quot;);
    return thread-&gt;active_handles()-&gt;allocate_handle(obj);
  }
}
</code></pre>
<h3><a class="header" href="#bytecode对应的代码" id="bytecode对应的代码">bytecode对应的代码</a></h3>
<p>hotspot默认用的是TemplateInterpreter把字节码对应的代码直接生成汇编代码，读起来比较难。hotspot中还有一个bytecodeInterpreter，这个完全是用cpp写的，两者从逻辑上看起来没区别。所以看每个bytecode对应的代码可以从bytecodeInterpreter入手。</p>
<h4><a class="header" href="#new" id="new">new</a></h4>
<p>java中的new 一个class 有两个path： 一个是fastpath: class对应的instanceKlass已经解析，初始化好了，这种比较快，另外一种是
slowpath需要去掉interpreter的runtime 去link, init这个klass, 然后把它放到constpoll的cache里面， 然后分配内存。</p>
<p>在heap堆或者thread的localstorage上分配内存， 设置好oop的 mark head,还有iklass指针，iklass指向在jvm中代表该java类的instanceKlass。</p>
<p>首先判断instanceklass是否已经加载了并且初始化了</p>
<pre><code class="language-cpp">ConstantPool* constants = istate-&gt;method()-&gt;constants();
        if (!constants-&gt;tag_at(index).is_unresolved_klass()) {
          // Make sure klass is initialized and doesn't have a finalizer
          Klass* entry = constants-&gt;slot_at(index).get_klass();
          InstanceKlass* ik = InstanceKlass::cast(entry);
          if (ik-&gt;is_initialized() &amp;&amp; ik-&gt;can_be_fastpath_allocated() ) {
</code></pre>
<p>如果要求useTLAB的话，就分配在thread localstorage上</p>
<pre><code>size_t obj_size = ik-&gt;size_helper();
if (UseTLAB) {
    result = (oop) THREAD-&gt;tlab().allocate(obj_size);
}

</code></pre>
<p>否则从heap上分配</p>
<pre><code class="language-cpp">HeapWord* compare_to = *Universe::heap()-&gt;top_addr();
HeapWord* new_top = compare_to + obj_size;
if (new_top &lt;= *Universe::heap()-&gt;end_addr()) {
  if (Atomic::cmpxchg_ptr(new_top, Universe::heap()-&gt;top_addr(), compare_to) != compare_to) {
    goto retry;
  }
  result = (oop) compare_to;
}
</code></pre>
<p>然后就是初始化这块内存了，设置好对象的markhead和kclass指针, 最后把它放到栈上。</p>
<pre><code class="language-cpp">if (need_zero ) {
    HeapWord* to_zero = (HeapWord*) result + sizeof(oopDesc) / oopSize;
    obj_size -= sizeof(oopDesc) / oopSize;
    if (obj_size &gt; 0 ) {
      memset(to_zero, 0, obj_size * HeapWordSize);
    }
  }
  if (UseBiasedLocking) {
    result-&gt;set_mark(ik-&gt;prototype_header());
  } else {
    result-&gt;set_mark(markOopDesc::prototype());
  }
  result-&gt;set_klass_gap(0);
  result-&gt;set_klass(ik);
  // Must prevent reordering of stores for object initialization
  // with stores that publish the new object.
  OrderAccess::storestore();
  SET_STACK_OBJECT(result, 0);
  UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
</code></pre>
<p>对于slowcase回去调用<code>interpreterRuntime::_new</code>去创建对象</p>
<pre><code class="language-cpp">CALL_VM(InterpreterRuntime::_new(THREAD, METHOD-&gt;constants(), index),
             handle_exception);
     // Must prevent reordering of stores for object initialization
     // with stores that publish the new object.
     OrderAccess::storestore();
     SET_STACK_OBJECT(THREAD-&gt;vm_result(), 0);
     THREAD-&gt;set_vm_result(NULL);
     UPDATE_PC_AND_TOS_AND_CONTINUE(3, 1);
___
</code></pre>
<h1><a class="header" href="#go" id="go">go</a></h1>
<h1><a class="header" href="#runtime-pgm-schedule" id="runtime-pgm-schedule">Runtime PGM Schedule</a></h1>
<h2><a class="header" href="#pgm-concept" id="pgm-concept">PGM concept:</a></h2>
<pre><code class="language-go">// 摘自src/runtime/proc.go
// G - goroutine.
// M - worker thread, or machine.
// P - processor, a resource that is required to execute Go code.
//     M must have an associated P to execute Go code, however it can be
//     blocked or in a syscall w/o an associated P.
</code></pre>
<p>三者struct之间的引用关系如下：</p>
<p><img src="golang/./pgm-struct.svg" alt="pgm-struct" /></p>
<h2><a class="header" href="#work-stealing-scheduler" id="work-stealing-scheduler">Work stealing scheduler</a></h2>
<p>Golang中的PGM采用类似于tokio的thread pool executor.  采用了worksteal的形式, 一方面降低了对global队列的锁的竞争。
另一方面每个G(go routine) 生成的go routine优先放到proc的local 队列里面，优先由同一个线程执行，比较好的增加了局部性。</p>
<p><img src="golang/./pgm-work-stealing.svg" alt="pgm-work-stealing" /></p>
<h2><a class="header" href="#processor创建" id="processor创建">processor创建</a></h2>
<p><img src="golang/./processor.svg" alt="processor" /></p>
<h2><a class="header" href="#machine-worker-thread线程创建" id="machine-worker-thread线程创建">machine worker thread线程创建</a></h2>
<p><img src="golang/./m-os-thread.svg" alt="m-os-thread" /></p>
<h2><a class="header" href="#status" id="status">Status</a></h2>
<h3><a class="header" href="#goroutine" id="goroutine">Goroutine</a></h3>
<p><img src="golang/./goroutine-status.svg" alt="goroutine-status" /></p>
<h3><a class="header" href="#proc" id="proc">Proc</a></h3>
<p><img src="golang/./proc-status.svg" alt="proc-status" /></p>
<h2><a class="header" href="#sysmon" id="sysmon">sysmon</a></h2>
<p><img src="golang/./sysmon.svg" alt="sysmon" /></p>
<h1><a class="header" href="#goroutine-stack" id="goroutine-stack">Goroutine Stack</a></h1>
<h2><a class="header" href="#goroutine-switch" id="goroutine-switch">goroutine switch</a></h2>
<p><img src="golang/./goroutine-stack-switch.svg" alt="goroutine-stack-switch" /></p>
<h3><a class="header" href="#mcall" id="mcall">mcall</a></h3>
<p>mcall 保存被切换gorutine信息，并在当前线程g0 goroutine上执行新的func</p>
<p><img src="golang/./mcall.svg" alt="mcall" /></p>
<pre><code>// func mcall(fn func(*g))
// Switch to m-&gt;g0's stack, call fn(g).
// Fn must never return. It should gogo(&amp;g-&gt;sched)
// to keep running g.
TEXT runtime·mcall(SB), NOSPLIT, $0-8
	MOVQ	fn+0(FP), DI

	get_tls(CX)
	MOVQ	g(CX), AX	// save state in g-&gt;sched
	MOVQ	0(SP), BX	// caller's PC
	MOVQ	BX, (g_sched+gobuf_pc)(AX)
	LEAQ	fn+0(FP), BX	// caller's SP
	MOVQ	BX, (g_sched+gobuf_sp)(AX)
	MOVQ	AX, (g_sched+gobuf_g)(AX)
	MOVQ	BP, (g_sched+gobuf_bp)(AX)

	// switch to m-&gt;g0 &amp; its stack, call fn
	MOVQ	g(CX), BX
	MOVQ	g_m(BX), BX
	MOVQ	m_g0(BX), SI
	CMPQ	SI, AX	// if g == m-&gt;g0 call badmcall
	JNE	3(PC)
	MOVQ	$runtime·badmcall(SB), AX
	JMP	AX
	MOVQ	SI, g(CX)	// g = m-&gt;g0
	MOVQ	(g_sched+gobuf_sp)(SI), SP	// sp = m-&gt;g0-&gt;sched.sp
	PUSHQ	AX
	MOVQ	DI, DX
	MOVQ	0(DI), DI
	CALL	DI
	POPQ	AX
	MOVQ	$runtime·badmcall2(SB), AX
	JMP	AX
	RET
</code></pre>
<h3><a class="header" href="#gogo" id="gogo">gogo</a></h3>
<p>gogo 用来从gobuf中恢复协程执行状态，并跳转到上一次指令处继续执行</p>
<pre><code class="language-go">// func gogo(buf *gobuf)
// restore state from Gobuf; longjmp
TEXT runtime·gogo(SB), NOSPLIT, $16-8
	MOVQ	buf+0(FP), BX		// gobuf
	MOVQ	gobuf_g(BX), DX
	MOVQ	0(DX), CX		// make sure g != nil
	get_tls(CX)
	MOVQ	DX, g(CX)
	MOVQ	gobuf_sp(BX), SP	// restore SP
	MOVQ	gobuf_ret(BX), AX
	MOVQ	gobuf_ctxt(BX), DX
	MOVQ	gobuf_bp(BX), BP
	MOVQ	$0, gobuf_sp(BX)	// clear to help garbage collector
	MOVQ	$0, gobuf_ret(BX)
	MOVQ	$0, gobuf_ctxt(BX)
	MOVQ	$0, gobuf_bp(BX)
	MOVQ	gobuf_pc(BX), BX
	JMP	BX
</code></pre>
<h3><a class="header" href="#gosave" id="gosave">gosave</a></h3>
<p>gosave感觉和cgo相关，这个代码还没怎么搞明白</p>
<p><img src="golang/./gosave.svg" alt="gosave" /></p>
<pre><code>// func gosave(buf *gobuf)
// save state in Gobuf; setjmp
TEXT runtime·gosave(SB), NOSPLIT, $0-8
	MOVQ	buf+0(FP), AX		// gobuf
	LEAQ	buf+0(FP), BX		// caller's SP
	MOVQ	BX, gobuf_sp(AX)
	MOVQ	0(SP), BX		// caller's PC
	MOVQ	BX, gobuf_pc(AX)
	MOVQ	$0, gobuf_ret(AX)
	MOVQ	BP, gobuf_bp(AX)
	// Assert ctxt is zero. See func save.
	MOVQ	gobuf_ctxt(AX), BX
	TESTQ	BX, BX
	JZ	2(PC)
	CALL	runtime·badctxt(SB)
	get_tls(CX)
	MOVQ	g(CX), BX
	MOVQ	BX, gobuf_g(AX)
	RET
</code></pre>
<h2><a class="header" href="#stack增长" id="stack增长">Stack增长</a></h2>
<p>编译器在每个函数调用中都会插入对morestack的调用。</p>
<p>morestack会检查当前栈空间是否够用，不够用的话，会调用newstack增长空间.
newstack 会分配2倍大小的stack, copy过去, 并将指向该stack的引用指针也修改过去。</p>
<p><img src="golang/./morestack.svg" alt="morestack" /></p>
<h1><a class="header" href="#memory分配" id="memory分配">Memory分配</a></h1>
<h2><a class="header" href="#struct之间引用关系" id="struct之间引用关系">struct之间引用关系</a></h2>
<p><img src="golang/./mem-struct.svg" alt="mem-struct" /></p>
<pre><code class="language-go">//src/runtime/malloc.go
//	fixalloc: a free-list allocator for fixed-size off-heap objects,
//		used to manage storage used by the allocator.
//	mheap: the malloc heap, managed at page (8192-byte) granularity.
//	mspan: a run of in-use pages managed by the mheap.
//	mcentral: collects all spans of a given size class.
//	mcache: a per-P cache of mspans with free space.
//	mstats: allocation statistics.
</code></pre>
<ol>
<li>fixalloc 用于分配mspan等固定大小的object</li>
<li>mheap 用于8KB page粒度内存管理</li>
<li>mspan: 一段连续的pages,用于分配制定specClass的object.</li>
<li>mcentral: 所有span的list</li>
<li>mcache: 线程的span cache, 优先从cache中分配, 避免每次访问heap需要lock.</li>
</ol>
<p>下图摘自<a href="golang/blog.learngoprogramming.com/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed">1</a>
比较清楚的画出了这几者之间的层级关系</p>
<p><img src="golang/./golang-mem-overview.png" alt="golang-mem-overview" /></p>
<h2><a class="header" href="#mspan" id="mspan">mspan</a></h2>
<p>mspan的创建路径如下</p>
<p><img src="golang/./mspan-create.svg" alt="mspan-create" /></p>
<h2><a class="header" href="#ref" id="ref">Ref</a></h2>
<ol>
<li><a href="https://blog.learngoprogramming.com/a-visual-guide-to-golang-memory-allocator-from-ground-up-e132258453ed">A visual guide to Go Memory Allocator from scratch</a></li>
</ol>
<h1><a class="header" href="#gc" id="gc">GC</a></h1>
<h2><a class="header" href="#gcphase" id="gcphase">GcPhase</a></h2>
<ol>
<li><code>_GCoff</code>:  GC not running; sweeping in background, write barrier disabled</li>
<li><code>_GCmark</code>: GC marking roots and workbufs: allocate black, write barrier ENABLED</li>
<li><code>_GCmarktermination</code>: GC mark termination: allocate black, P's help GC, write barrier ENABLED</li>
</ol>
<p>如下图所示，GC过程中开启了两次STW(stop the world),　第一次主要为parepare阶段，
第二次为Marktermination阶段:</p>
<p><img src="golang/./gcphase.svg" alt="gcphase" /></p>
<pre><code class="language-go">//go:nosplit
func setGCPhase(x uint32) {
	atomic.Store(&amp;gcphase, x)
	writeBarrier.needed = gcphase == _GCmark || gcphase == _GCmarktermination
	writeBarrier.enabled = writeBarrier.needed || writeBarrier.cgo
}
</code></pre>
<h2><a class="header" href="#mark-phase" id="mark-phase">Mark Phase</a></h2>
<p>Golang中是如何根据指针找到对象，以及该对象所引用的对象的？答案是根据heap Arena中bitmap存储的元信息。
对于Arena中每个word, bitmap使用了两个bit，来标识该word是否是指针，以及该word是否已被扫描过。</p>
<pre><code class="language-go">type heapArena struct {
	// bitmap stores the pointer/scalar bitmap for the words in
  // this arena
	bitmap [heapArenaBitmapBytes]byte
	spans [pagesPerArena]*mspan
	pageInUse [pagesPerArena / 8]uint8
	pageMarks [pagesPerArena / 8]uint8
	zeroedBase uintptr
}
</code></pre>
<p><img src="golang/./heapbits.svg" alt="heapbits" /></p>
<p>另外每个span中有allocBits和gcmarkbits用来标记span中每个slot是否被分配。在mallocgc中会使用该信息，找到可分配的slot,
另外在gc sweep阶段根据coutAlloc()==0 来判断mspan是否是空闲的，可以被回收.</p>
<pre><code class="language-go">//go:nosplit
// 返回一个指针在heapArena中的bits位
func heapBitsForAddr(addr uintptr) (h heapBits) {
	// 2 bits per word, 4 pairs per byte, and a mask is hard coded.
	arena := arenaIndex(addr)
	ha := mheap_.arenas[arena.l1()][arena.l2()]
	// The compiler uses a load for nil checking ha, but in this
	// case we'll almost never hit that cache line again, so it
	// makes more sense to do a value check.
	if ha == nil {
		// addr is not in the heap. Return nil heapBits, which
		// we expect to crash in the caller.
		return
	}
	h.bitp = &amp;ha.bitmap[(addr/(sys.PtrSize*4))%heapArenaBitmapBytes]
	h.shift = uint32((addr / sys.PtrSize) &amp; 3)
	h.arena = uint32(arena)
	h.last = &amp;ha.bitmap[len(ha.bitmap)-1]
	return
}
</code></pre>
<pre><code class="language-go">func (s *mspan) markBitsForIndex(objIndex uintptr) markBits {
	bytep, mask := s.gcmarkBits.bitp(objIndex)
	return markBits{bytep, mask, objIndex}
}

// bitp returns a pointer to the byte containing bit n and a mask for
// selecting that bit from *bytep.
func (b *gcBits) bitp(n uintptr) (bytep *uint8, mask uint8) {
	return b.bytep(n / 8), 1 &lt;&lt; (n % 8)
}
</code></pre>
<h3><a class="header" href="#并发标记" id="并发标记">并发标记</a></h3>
<p><img src="golang/./gcmark.svg" alt="gcmark" /></p>
<h4><a class="header" href="#writebarrier" id="writebarrier">WriteBarrier</a></h4>
<pre><code class="language-go">	a := new(A)
	a.c = new(C)
</code></pre>
<p>混合写屏障<a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">1</a>
这里的shade就是将白色对象放入待扫描队列中(wbBuf)</p>
<pre><code>writePointer(slot, ptr):
    shade(*slot)
    if current stack is grey:
        shade(ptr)
    *slot = ptr
</code></pre>
<p>编译器注入的writeBarrier</p>
<pre><code class="language-go">	0x0059 00089 (test.go:14)	CMPL	runtime.writeBarrier(SB), $0
	0x0060 00096 (test.go:14)	JEQ	100
	0x0062 00098 (test.go:14)	JMP	115
	0x0064 00100 (test.go:14)	MOVQ	AX, (DI)
	0x0067 00103 (test.go:14)	JMP	105
	0x0069 00105 (test.go:15)	PCDATA	$0, $0
	0x0069 00105 (test.go:15)	PCDATA	$1, $0
	0x0069 00105 (test.go:15)	MOVQ	56(SP), BP
	0x006e 00110 (test.go:15)	ADDQ	$64, SP
	0x0072 00114 (test.go:15)	RET
	0x0073 00115 (test.go:14)	PCDATA	$0, $-2
	0x0073 00115 (test.go:14)	PCDATA	$1, $-2
	0x0073 00115 (test.go:14)	CALL	runtime.gcWriteBarrier(SB)
	0x0078 00120 (test.go:14)	JMP	105
</code></pre>
<h4><a class="header" href="#scanobject" id="scanobject">scanobject</a></h4>
<p>scanobject:根据bitmap信息，判断是否是指针，是否已扫描过。
如果是指针的话，查找指针对应的object, 并加到队列里面（标记为灰色）
这样下次gcDrain会从队列中去取，接着循环的扫描。。</p>
<pre><code class="language-go">// scanobject scans the object starting at b, adding pointers to gcw.
// b must point to the beginning of a heap object or an oblet.
// scanobject consults the GC bitmap for the pointer mask and the
// spans for the size of the object.
//
//go:nowritebarrier
func scanobject(b uintptr, gcw *gcWork) {
	// Find the bits for b and the size of the object at b.
	//
	// b is either the beginning of an object, in which case this
	// is the size of the object to scan, or it points to an
	// oblet, in which case we compute the size to scan below.
	hbits := heapBitsForAddr(b)
	s := spanOfUnchecked(b)
  //...
			if s.spanclass.noscan() {
				// Bypass the whole scan.
				gcw.bytesMarked += uint64(n)
				return
			}

	var i uintptr
	for i = 0; i &lt; n; i += sys.PtrSize {
		// Find bits for this word.
		if i != 0 {
			// Avoid needless hbits.next() on last iteration.
			hbits = hbits.next()
		}
		// Load bits once. See CL 22712 and issue 16973 for discussion.
		bits := hbits.bits()
		// During checkmarking, 1-word objects store the checkmark
		// in the type bit for the one word. The only one-word objects
		// are pointers, or else they'd be merged with other non-pointer
		// data into larger allocations.
		if i != 1*sys.PtrSize &amp;&amp; bits&amp;bitScan == 0 {
			break // no more pointers in this object
		}
		if bits&amp;bitPointer == 0 {
			continue // not a pointer
		}

		// Work here is duplicated in scanblock and above.
		// If you make changes here, make changes there too.
		obj := *(*uintptr)(unsafe.Pointer(b + i))

		// At this point we have extracted the next potential pointer.
		// Quickly filter out nil and pointers back to the current object.
		if obj != 0 &amp;&amp; obj-b &gt;= n {
			// Test if obj points into the Go heap and, if so,
			// mark the object.
			//
			// Note that it's possible for findObject to
			// fail if obj points to a just-allocated heap
			// object because of a race with growing the
			// heap. In this case, we know the object was
			// just allocated and hence will be marked by
			// allocation itself.
			if obj, span, objIndex := findObject(obj, b, i); obj != 0 {
				greyobject(obj, b, i, span, gcw, objIndex)
			}
		}
	}
  //...
}
</code></pre>
<h2><a class="header" href="#sweep-phase" id="sweep-phase">Sweep Phase</a></h2>
<p><img src="golang/./go-sweep.svg" alt="gcsweep" /></p>
<h3><a class="header" href="#scavenging" id="scavenging">scavenging</a></h3>
<p>go1.13之后改为更智能的内存归还给os<a href="https://github.com/golang/go/issues/30333">2</a></p>
<p><img src="golang/./scavenge.svg" alt="scavege" /></p>
<h2><a class="header" href="#ref-1" id="ref-1">Ref</a></h2>
<ol>
<li><a href="https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md">Proposal: Eliminate STW stack re-scanning</a></li>
<li><a href="https://github.com/golang/go/issues/30333">Proposal: Smarter Scavenging</a></li>
</ol>
<h1><a class="header" href="#context" id="context">Context</a></h1>
<h3><a class="header" href="#context-struct之间关系" id="context-struct之间关系">Context struct之间关系</a></h3>
<p><img src="golang/./context-struct.svg" alt="context-struct" /></p>
<h3><a class="header" href="#context-example" id="context-example">Context example</a></h3>
<pre><code class="language-go">// This example demonstrates the use of a cancelable context to prevent a
// goroutine leak. By the end of the example function, the goroutine started
// by gen will return without leaking.
func ExampleWithCancel() {
	// gen generates integers in a separate goroutine and
	// sends them to the returned channel.
	// The callers of gen need to cancel the context once
	// they are done consuming generated integers not to leak
	// the internal goroutine started by gen.
	gen := func(ctx context.Context) &lt;-chan int {
		dst := make(chan int)
		n := 1
		go func() {
			for {
				select {
				case &lt;-ctx.Done():
					return // returning not to leak the goroutine
				case dst &lt;- n:
					n++
				}
			}
		}()
		return dst
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel() // cancel when we are finished consuming integers

	for n := range gen(ctx) {
		fmt.Println(n)
		if n == 5 {
			break
		}
	}
	// Output:
	// 1
	// 2
	// 3
	// 4
	// 5
}
</code></pre>
<h1><a class="header" href="#defer-recover-panic" id="defer-recover-panic">defer, recover, panic</a></h1>
<ol>
<li>每个defer语句生成的defer结构会插到队首，defer执行时从defer link list头开始执行.所以defer以LIFO顺序执行。</li>
<li>每个return语句会编译器会插入deferreturn。</li>
<li>在panic中会调用defer 链表中的函数，然后在defer中可以recover, 也可以接着panic.</li>
</ol>
<h2><a class="header" href="#defer" id="defer">defer</a></h2>
<p><img src="golang/./defer.svg" alt="defer" /></p>
<h3><a class="header" href="#defer-语句" id="defer-语句">defer 语句</a></h3>
<p>每个defer语句会转换成对deferproc的调用.</p>
<pre><code class="language-go">// Calls the function n using the specified call type.
// Returns the address of the return value (or nil if none).
func (s *state) call(n *Node, k callKind) *ssa.Value {
//...
		switch {
		case k == callDefer:
			call = s.newValue1A(ssa.OpStaticCall, types.TypeMem, deferproc, s.mem())
      ...
}
</code></pre>
<p>deferproc 会新建一个<code>_defer</code>结构的struct, 并插到当前goroutine的<code>_defer</code>列表队头</p>
<pre><code class="language-go">// Create a new deferred function fn with siz bytes of arguments.
// The compiler turns a defer statement into a call to this.
//go:nosplit
func deferproc(siz int32, fn *funcval) { // arguments of fn follow fn
	if getg().m.curg != getg() {
		// go code on the system stack can't defer
		throw(&quot;defer on system stack&quot;)
	}

	// the arguments of fn are in a perilous state. The stack map
	// for deferproc does not describe them. So we can't let garbage
	// collection or stack copying trigger until we've copied them out
	// to somewhere safe. The memmove below does that.
	// Until the copy completes, we can only call nosplit routines.
	sp := getcallersp()
	argp := uintptr(unsafe.Pointer(&amp;fn)) + unsafe.Sizeof(fn)
	callerpc := getcallerpc()

	d := newdefer(siz)
	if d._panic != nil {
		throw(&quot;deferproc: d.panic != nil after newdefer&quot;)
	}
	d.fn = fn
	d.pc = callerpc
	d.sp = sp
	switch siz {
	case 0:
		// Do nothing.
	case sys.PtrSize:
		*(*uintptr)(deferArgs(d)) = *(*uintptr)(unsafe.Pointer(argp))
	default:
		memmove(deferArgs(d), unsafe.Pointer(argp), uintptr(siz))
	}

	// deferproc returns 0 normally.
	// a deferred func that stops a panic
	// makes the deferproc return 1.
	// the code the compiler generates always
	// checks the return value and jumps to the
	// end of the function if deferproc returns != 0.
	return0()
	// No code can go here - the C return register has
	// been set and must not be clobbered.
}
</code></pre>
<p>其中return0的定义如下</p>
<pre><code>TEXT runtime·return0(SB), NOSPLIT, $0
	MOVL	$0, AX
	RET
</code></pre>
<p>compiler生成的代码会检查ax寄存器的值。</p>
<h3><a class="header" href="#defer函数的调用" id="defer函数的调用">defer函数的调用</a></h3>
<p>编译器在函数的return RET指令后面加入deferreturn的调用.</p>
<pre><code>func fa() {
		defer fmt.Printf(&quot;hello&quot;)
}
</code></pre>
<pre><code class="language-go">&quot;&quot;.fa STEXT size=106 args=0x0 locals=0x48
	0x0000 00000 (test.go:7)	TEXT	&quot;&quot;.fa(SB), ABIInternal, $72-0
	0x0000 00000 (test.go:7)	MOVQ	(TLS), CX
	0x0009 00009 (test.go:7)	CMPQ	SP, 16(CX)
	0x000d 00013 (test.go:7)	JLS	99
	0x000f 00015 (test.go:7)	SUBQ	$72, SP
	0x0013 00019 (test.go:7)	MOVQ	BP, 64(SP)
	0x0018 00024 (test.go:7)	LEAQ	64(SP), BP
	0x001d 00029 (test.go:7)	FUNCDATA	$0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
	0x001d 00029 (test.go:7)	FUNCDATA	$1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
	0x001d 00029 (test.go:7)	FUNCDATA	$2, gclocals·9fb7f0986f647f17cb53dda1484e0f7a(SB)
	0x001d 00029 (test.go:8)	PCDATA	$0, $0
	0x001d 00029 (test.go:8)	PCDATA	$1, $0
	0x001d 00029 (test.go:8)	MOVL	$0, &quot;&quot;..autotmp_1+8(SP)
	0x0025 00037 (test.go:8)	PCDATA	$0, $1
	0x0025 00037 (test.go:8)	LEAQ	&quot;&quot;.fa.func1·f(SB), AX
	0x002c 00044 (test.go:8)	PCDATA	$0, $0
	0x002c 00044 (test.go:8)	MOVQ	AX, &quot;&quot;..autotmp_1+32(SP)
	0x0031 00049 (test.go:8)	PCDATA	$0, $1
	0x0031 00049 (test.go:8)	LEAQ	&quot;&quot;..autotmp_1+8(SP), AX
	0x0036 00054 (test.go:8)	PCDATA	$0, $0
	0x0036 00054 (test.go:8)	MOVQ	AX, (SP)
	0x003a 00058 (test.go:8)	CALL	runtime.deferprocStack(SB)
  // 如果deferprocStack返回值不为０,则调到末尾执行deferreturn
	0x003f 00063 (test.go:8)	TESTL	AX, AX
	0x0041 00065 (test.go:8)	JNE	83
	0x0043 00067 (test.go:11)	XCHGL	AX, AX
	0x0044 00068 (test.go:11)	CALL	runtime.deferreturn(SB)
	0x0049 00073 (test.go:11)	MOVQ	64(SP), BP
	0x004e 00078 (test.go:11)	ADDQ	$72, SP
	0x0052 00082 (test.go:11)	RET
	0x0053 00083 (test.go:8)	XCHGL	AX, AX
	0x0054 00084 (test.go:8)	CALL	runtime.deferreturn(SB)
	0x0059 00089 (test.go:8)	MOVQ	64(SP), BP
	0x005e 00094 (test.go:8)	ADDQ	$72, SP
	0x0062 00098 (test.go:8)	RET
	0x0063 00099 (test.go:8)	NOP
	0x0063 00099 (test.go:7)	PCDATA	$1, $-1
	0x0063 00099 (test.go:7)	PCDATA	$0, $-1
	0x0063 00099 (test.go:7)	CALL	runtime.morestack_noctxt(SB)
	0x0068 00104 (test.go:7)	JMP	0
</code></pre>
<p>deferreturn 会调用jmpdefer，不断的执行defer link中的fn</p>
<pre><code class="language-go">// func jmpdefer(fv *funcval, argp uintptr)
// argp is a caller SP.
// called from deferreturn.
// 1. pop the caller
// 2. sub 5 bytes from the callers return
// 3. jmp to the argument
TEXT runtime·jmpdefer(SB), NOSPLIT, $0-16
	MOVQ	fv+0(FP), DX	// fn
	MOVQ	argp+8(FP), BX	// caller sp
	LEAQ	-8(BX), SP	// caller sp after CALL
	MOVQ	-8(SP), BP	// restore BP as if deferreturn returned (harmless if framepointers not in use)
	SUBQ	$5, (SP)	// return to CALL again
	MOVQ	0(DX), BX
	JMP	BX	// but first run the deferred function
</code></pre>
<h2><a class="header" href="#panic" id="panic">panic</a></h2>
<p><img src="golang/./panic.svg" alt="panic" /></p>
<p>在panic中会调用当前goroutine的defer 函数，在这些defer函数中也可能会有panic，所有每个goroutine也有个panic的link list。</p>
<p>如果在defer中调用了recover, 那么goroutine会从derfer的sp,pc处接着执行，否则就进入fatalpanic，打印堆栈，最后<code>exit(2)</code></p>
<pre><code class="language-go">func gopanic(e interface{}) {
  //other code
	var p _panic
	p.arg = e
	p.link = gp._panic
	gp._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p)))

	atomic.Xadd(&amp;runningPanicDefers, 1)

	for {
		d := gp._defer
		if d == nil {
			break
		}

		// If defer was started by earlier panic or Goexit (and, since we're back here, that triggered a new panic),
		// take defer off list. The earlier panic or Goexit will not continue running.
		if d.started {
			if d._panic != nil {
				d._panic.aborted = true
			}
			d._panic = nil
			d.fn = nil
			gp._defer = d.link
			freedefer(d)
			continue
		}

		// Mark defer as started, but keep on list, so that traceback
		// can find and update the defer's argument frame if stack growth
		// or a garbage collection happens before reflectcall starts executing d.fn.
		d.started = true

		// Record the panic that is running the defer.
		// If there is a new panic during the deferred call, that panic
		// will find d in the list and will mark d._panic (this panic) aborted.
		d._panic = (*_panic)(noescape(unsafe.Pointer(&amp;p)))

		p.argp = unsafe.Pointer(getargp(0))
		reflectcall(nil, unsafe.Pointer(d.fn), deferArgs(d), uint32(d.siz), uint32(d.siz))
		p.argp = nil

		// reflectcall did not panic. Remove d.
		if gp._defer != d {
			throw(&quot;bad defer entry in panic&quot;)
		}
		d._panic = nil
		d.fn = nil
		gp._defer = d.link

		// trigger shrinkage to test stack copy. See stack_test.go:TestStackPanic
		//GC()

		pc := d.pc
		sp := unsafe.Pointer(d.sp) // must be pointer so it gets adjusted during stack copy
		freedefer(d)
		if p.recovered {
			atomic.Xadd(&amp;runningPanicDefers, -1)

			gp._panic = p.link
			// Aborted panics are marked but remain on the g.panic list.
			// Remove them from the list.
			for gp._panic != nil &amp;&amp; gp._panic.aborted {
				gp._panic = gp._panic.link
			}
			if gp._panic == nil { // must be done with signal
				gp.sig = 0
			}
			// Pass information about recovering frame to recovery.
			gp.sigcode0 = uintptr(sp)
			gp.sigcode1 = pc
			mcall(recovery)
			throw(&quot;recovery failed&quot;) // mcall should not return
		}
	}

	// ran out of deferred calls - old-school panic now
	// Because it is unsafe to call arbitrary user code after freezing
	// the world, we call preprintpanics to invoke all necessary Error
	// and String methods to prepare the panic strings before startpanic.
	preprintpanics(gp._panic)

	fatalpanic(gp._panic) // should not return
	*(*int)(nil) = 0      // not reached
}
</code></pre>
<p>recovery, 这个地方将ret值改为了1</p>
<pre><code class="language-go">func recovery(gp *g) {
	// Info about defer passed in G struct.
	sp := gp.sigcode0
	pc := gp.sigcode1

	// d's arguments need to be in the stack.
	if sp != 0 &amp;&amp; (sp &lt; gp.stack.lo || gp.stack.hi &lt; sp) {
		print(&quot;recover: &quot;, hex(sp), &quot; not in [&quot;, hex(gp.stack.lo), &quot;, &quot;, hex(gp.stack.hi), &quot;]\n&quot;)
		throw(&quot;bad recovery&quot;)
	}

	// Make the deferproc for this d return again,
	// this time returning 1.  The calling function will
	// jump to the standard return epilogue.
	gp.sched.sp = sp
	gp.sched.pc = pc
	gp.sched.lr = 0
	gp.sched.ret = 1
	gogo(&amp;gp.sched)
}
</code></pre>
<h2><a class="header" href="#ref-2" id="ref-2">Ref:</a></h2>
<ol>
<li>https://tiancaiamao.gitbooks.io/go-internals/content/zh/03.4.html</li>
<li>https://blog.learngoprogramming.com/gotchas-of-defer-in-go-1-8d070894cb01</li>
</ol>
<h1><a class="header" href="#kafka" id="kafka">Kafka</a></h1>
<h1><a class="header" href="#kafka-client-producer" id="kafka-client-producer">Kafka client: producer</a></h1>
<p>producer client端发送消息过程</p>
<p><img src="kafka/./client-producer.svg" alt="client-producer" /></p>
<p>更新元数据过程: updateMetadata</p>
<p><img src="kafka/./client-producer-udatemetadata.svg" alt="client-update-metadata" /></p>
<h1><a class="header" href="#kafka-groupcoordinator" id="kafka-groupcoordinator">Kafka GroupCoordinator</a></h1>
<p>GroupCoordinator handles general group membership and offset management.</p>
<h2><a class="header" href="#consumergroup" id="consumergroup">ConsumerGroup</a></h2>
<p>consumer group是kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费</p>
<ol>
<li>consumer group下可以有一个或多个consumer instance，consumer instance可以是一个进程，也可以是一个线程</li>
<li>group.id是一个字符串，唯一标识一个consumer group</li>
<li>consumer group下订阅的topic下的每个分区只能分配给某个group下的一个consumer(当然该分区还可以被分配给其他group)</li>
</ol>
<p><code>__consumer_offsets</code> 中的消息保存了每个consumer group某一时刻提交的offset信息。 这个key是consumer-group-id-topic-partition- 这样？
谁来提交offsets?</p>
<p>group与coordinator共同使用它来完成group的rebalance。目前kafka提供了5个协议来处理与consumer group coordination相关的问题：</p>
<ol>
<li>Heartbeat请求：consumer需要定期给coordinator发送心跳来表明自己还活着</li>
<li>JoinGroup请求：成员请求加入组</li>
<li>LeaveGroup请求：主动告诉coordinator我要离开consumer group</li>
<li>SyncGroup请求：group leader把分配方案告诉组内所有成员</li>
<li>DescribeGroup请求：显示组的所有信息，包括成员信息，协议名称，分配方案，订阅信息等。通常该请求是给管理员使用</li>
</ol>
<h2><a class="header" href="#joinleave-group" id="joinleave-group">join/leave group</a></h2>
<p>reblance的时候发生了啥？parition 和consumer之间的分配？？谁负责把partition给各个consumer?</p>
<p>staticMember 是client指定的groupInsanceID</p>
<p>staticMember 和PendingMember是啥？作用是啥？</p>
<p>GroupInstanceId用户指定的consumerid,每个group中这些ID必须是唯一的。</p>
<p>和member.id不同的是，每次成员重启回来后，其静态成员ID值是不变的，因此之前分配给该成员的所有分区也是不变的，而且在没有超时前静态成员重启回来是不会触发Rebalance的。</p>
<pre><code>Static Membership: the membership protocol where the consumer group will not trigger rebalance unless 
  * A new member joins
  * A leader rejoins (possibly due to topic assignment change)
  * An existing member offline time is over session timeout
  * Broker receives a leave group request containing alistof `group.instance.id`s (details later)

Group instance id: the unique identifier defined by user to distinguish each client instance.
</code></pre>
<p><img src="kafka/./coordinator-join-group.svg" alt="join-leave-group-coordinator" /></p>
<h2><a class="header" href="#sync-group" id="sync-group">Sync group</a></h2>
<p>总体而言，rebalance分为2步：Join和Sync</p>
<ol>
<li>Join， 顾名思义就是加入组。这一步中，所有成员都向coordinator发送JoinGroup请求，请求入组。一旦所有成员都发送了JoinGroup请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader——注意leader和coordinator不是一个概念。leader负责消费分配方案的制定。</li>
<li>Sync，这一步leader开始分配消费方案，即哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案封装进SyncGroup请求中发给coordinator，非leader也会发SyncGroup请求，只是内容为空。coordinator接收到分配方案之后会把方案塞进SyncGroup的response中发给各个consumer。这样组内的所有成员就都知道自己应该消费哪些分区了。</li>
</ol>
<p><img src="kafka/./sync-group.svg" alt="sync-group" /></p>
<h2><a class="header" href="#fetchcommit-offset" id="fetchcommit-offset">Fetch/Commit Offset</a></h2>
<p><img src="kafka/./commit-offset.svg" alt="commit-offset" /></p>
<h2><a class="header" href="#heartbeat" id="heartbeat">heartbeat</a></h2>
<p><img src="kafka/./group-heartbeat.svg" alt="group-heartbeat" /></p>
<h2><a class="header" href="#group状态" id="group状态">Group状态</a></h2>
<p>group状态，以及group各个状态下对join/leave/sync/offset_commit等行为的反应</p>
<p><img src="kafka/./group-state.svg" alt="group-state" /></p>
<h2><a class="header" href="#ref-3" id="ref-3">Ref</a></h2>
<ol>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances">static memeber</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Client-side+Assignment+Proposal">Kafka Client-side Assignment Proposal</a></li>
<li><a href="https://www.cnblogs.com/huxi2b/p/11386847.html">Kafka消费者组静态成员</a></li>
<li><a href="https://www.cnblogs.com/huxi2b/p/6223228.html">Kafka消费组(consumer group)</a></li>
</ol>
<h1><a class="header" href="#kafka-读写消息" id="kafka-读写消息">Kafka 读写消息</a></h1>
<h2><a class="header" href="#消息的produce-and-consume" id="消息的produce-and-consume">消息的produce and consume</a></h2>
<p><img src="kafka/./kafkaServer.svg" alt="kafka-produce-fetch" /></p>
<h2><a class="header" href="#partition-对应log对象创建" id="partition-对应log对象创建">partition 对应log对象创建</a></h2>
<p>log对象是什么时候创建的？parition创建时候就创建吗？</p>
<p><img src="kafka/./partition-log-create.svg" alt="kafka-log-create" /></p>
<h2><a class="header" href="#replicamanager-partion信息维护" id="replicamanager-partion信息维护">ReplicaManager Partion信息维护</a></h2>
<p>ReplicaManager的allPartions是存放在zk中的吗？不同broker server之间这个信息是怎么同步的?</p>
<pre><code class="language-java">public final class TopicPartition implements Serializable {
//other code
    private final int partition;
    private final String topic;
}
</code></pre>
<pre><code class="language-scala">class ReplicaManager{
/* other code */
  private val allPartitions = new Pool[TopicPartition, HostedPartition](
    valueFactory = Some(tp =&gt; HostedPartition.Online(Partition(tp, time, this)))
  )
/* other code */
}
</code></pre>
<p>当zk中broker,topic, partion, controller等发生变动时候，由kafka controller通过ControllerChannelManager
向每个kafka broker发送<code>LEADER_AND_ISR</code>消息, broker收到消息以后，会更新ReplicaManager中的allPartitions信息。</p>
<p><img src="kafka/./allpartion-overview.svg" alt="allpartionsoverview" /></p>
<p>具体细节如下
<img src="kafka/./getpartition.svg" alt="getPartition" /></p>
<h1><a class="header" href="#kafka-logmanager" id="kafka-logmanager">Kafka LogManager</a></h1>
<h2><a class="header" href="#kafka日志层级" id="kafka日志层级">Kafka日志层级</a></h2>
<p>在kafka中每个topic可以有多个partition,　每个partition存储时候分为多个segment。</p>
<p>每个parition有多个副本，副本分布在不同的broker上，其中一个broker被选为该partition的leader,
消息是写到kafka partition leader副本中的，而follower通过fetchmessage，同步该partition的消息。</p>
<p><img src="kafka/./log_struct.svg" alt="logstruct" /></p>
<h2><a class="header" href="#日志文件加载和创建" id="日志文件加载和创建">日志文件加载和创建</a></h2>
<p>启动时候，会打开log所有segment log file, Lazy的加载他们对应的index.</p>
<p><img src="kafka/./logmanager_loadlog.svg" alt="loadlog" /></p>
<h2><a class="header" href="#日志读写" id="日志读写">日志读写</a></h2>
<p>写的message个数超过了配置也会触发flush，将cache中msg刷新到磁盘中。</p>
<p><img src="kafka/./log_read_write.svg" alt="load-read-write" /></p>
<h2><a class="header" href="#日志后台清理和压缩" id="日志后台清理和压缩">日志后台清理和压缩</a></h2>
<h3><a class="header" href="#清理过期日志" id="清理过期日志">清理过期日志</a></h3>
<p>后台线程根据配置定期清理过期或者超过大小的日志segment</p>
<p><img src="kafka/./log_clean.svg" alt="log-clean" /></p>
<h3><a class="header" href="#日志缓存flush" id="日志缓存flush">日志缓存flush</a></h3>
<p>后台线程定期将cache刷新到磁盘.</p>
<p><img src="kafka/./log_flush.svg" alt="log-flush" /></p>
<h3><a class="header" href="#日志compact" id="日志compact">日志compact</a></h3>
<p>有相同key的msg按照时间顺序只用保留最后一条。</p>
<p><img src="kafka/./kafka-log-compaction-process.png" alt="kafka-log-compact-process" /></p>
<p>首先会创建key -&gt; offset的映射，然后在遍历records的时候，只保留offset最大的那个。</p>
<pre><code class="language-scala">  private def buildOffsetMapForSegment(topicPartition: TopicPartition,
                                       segment: LogSegment,
                                       map: OffsetMap,
                                       startOffset: Long,
                                       maxLogMessageSize: Int,
                                       transactionMetadata: CleanedTransactionMetadata,
                                       stats: CleanerStats): Boolean = {
      //other code
      val records = MemoryRecords.readableRecords(readBuffer)
      throttler.maybeThrottle(records.sizeInBytes)
      for (batch &lt;- records.batches.asScala) {
        //other code...
        map.put(record.key, record.offset)
      }
}
</code></pre>
<p>在memory records的filter中根据这个OffsetMap 过滤掉相同key下offset小的record</p>
<pre><code class="language-scala">  private def shouldRetainRecord(map: kafka.log.OffsetMap,
                                 retainDeletes: Boolean,
                                 batch: RecordBatch,
                                 record: Record,
                                 stats: CleanerStats): Boolean = {
    val pastLatestOffset = record.offset &gt; map.latestOffset
    if (pastLatestOffset)
      return true

    if (record.hasKey) {
      val key = record.key
      val foundOffset = map.get(key)
      /* First,the message must have the latest offset for the key
       * then there are two cases in which we can retain a message:
       *   1) The message has value
       *   2) The message doesn't has value but it can't be deleted now.
       */
      val latestOffsetForKey = record.offset() &gt;= foundOffset
      val isRetainedValue = record.hasValue || retainDeletes
      latestOffsetForKey &amp;&amp; isRetainedValue
    } else {
      stats.invalidMessage()
      false
    }
  }
</code></pre>
<p><img src="kafka/./log_compact.svg" alt="log-compact" /></p>
<h2><a class="header" href="#ref-4" id="ref-4">Ref</a></h2>
<ol>
<li><a href="http://cloudurable.com/blog/kafka-architecture-log-compaction/index.html">Kafka Architecture: Log Compaction</a></li>
</ol>
<h1><a class="header" href="#kafka-partition" id="kafka-partition">Kafka Partition</a></h1>
<h2><a class="header" href="#partionstate" id="partionstate">PartionState</a></h2>
<p>PartionState中重要信息为当前partion的leader和ISR(in sync replica)的replicaId， PartitionState最终是存储在zk中的。
isr信息有<code>maybeShrinkIsr</code>和<code>maybeExpandIsr</code>这两个函数维护.</p>
<p>每个parition的replica follower都有一个replicaFetcher 线程，该线程负责从partition的leader中
获取消息，在parition leader中处理fetchMessage请求时，判断该follower是否达到in sync标准，将该replicaId加入到该partiton中的ISR中。</p>
<p>另外ReplicaManager后台会周期性的调用<code>maybeShrinkIsr</code>将outOfSync的replica从ISR中踢掉。</p>
<p><img src="kafka/./partition-isr.svg" alt="isr" /></p>
<h3><a class="header" href="#replica-inout-sync-state" id="replica-inout-sync-state">replica in/out sync state</a></h3>
<p>判断replica是否处于in/out sync状态</p>
<pre><code class="language-scala">  private def isFollowerOutOfSync(replicaId: Int,
                                  leaderEndOffset: Long,
                                  currentTimeMs: Long,
                                  maxLagMs: Long): Boolean = {
    val followerReplica = getReplicaOrException(replicaId)
    followerReplica.logEndOffset != leaderEndOffset &amp;&amp;
      (currentTimeMs - followerReplica.lastCaughtUpTimeMs) &gt; maxLagMs
  }

  private def isFollowerInSync(followerReplica: Replica, highWatermark: Long): Boolean = {
    val followerEndOffset = followerReplica.logEndOffset
    followerEndOffset &gt;= highWatermark &amp;&amp; leaderEpochStartOffsetOpt.exists(followerEndOffset &gt;= _)
  }
</code></pre>
<h3><a class="header" href="#partition-对应log对象创建-1" id="partition-对应log对象创建-1">partition 对应log对象创建</a></h3>
<p>在成为leader或者follower时会创建相应的log对象</p>
<p>log对象是什么时候创建的？parition创建时候就创建吗？
<img src="kafka/./partition-log-create.svg" alt="kafka-log-create" /></p>
<h3><a class="header" href="#partition-sate-在zk中的存储" id="partition-sate-在zk中的存储">Partition sate 在zk中的存储</a></h3>
<h4><a class="header" href="#存储路径" id="存储路径">存储路径</a></h4>
<p>Partition 的ISR信息存储在zk下</p>
<pre><code>/broker/topics/{topic}/partitions/{partition}/state，
</code></pre>
<p>具体对应代码在<code>zkData.scala</code>中</p>
<pre><code class="language-scala">// tp partition状态在zk中存储路径
object TopicPartitionStateZNode {
  def path(partition: TopicPartition) = s&quot;${TopicPartitionZNode.path(partition)}/state&quot;
  //other code
}

//tp路径
object TopicPartitionsZNode {
  def path(topic: String) = s&quot;${TopicZNode.path(topic)}/partitions&quot;
}

object TopicZNode {
  def path(topic: String) = s&quot;${TopicsZNode.path}/$topic&quot;
  //othercode
}

//topics路径
object TopicsZNode {
  def path = s&quot;${BrokersZNode.path}/topics&quot;
}

</code></pre>
<h4><a class="header" href="#存储信息" id="存储信息">存储信息</a></h4>
<p>paritionstate中存储信息如下</p>
<pre><code class="language-scala">  def decode(bytes: Array[Byte], stat: Stat): Option[LeaderIsrAndControllerEpoch] = {
    Json.parseBytes(bytes).map { js =&gt;
      val leaderIsrAndEpochInfo = js.asJsonObject
      val leader = leaderIsrAndEpochInfo(&quot;leader&quot;).to[Int]
      val epoch = leaderIsrAndEpochInfo(&quot;leader_epoch&quot;).to[Int]
      val isr = leaderIsrAndEpochInfo(&quot;isr&quot;).to[List[Int]]
      val controllerEpoch = leaderIsrAndEpochInfo(&quot;controller_epoch&quot;).to[Int]
      val zkPathVersion = stat.getVersion
      LeaderIsrAndControllerEpoch(LeaderAndIsr(leader, epoch, isr, zkPathVersion), controllerEpoch)
    }
  }
</code></pre>
<p>LeaderAndIsrPartitionState定义在LeaderAndIsrRequest.json中,定义如下</p>
<pre><code class="language-json">  &quot;commonStructs&quot;: [
    { &quot;name&quot;: &quot;LeaderAndIsrPartitionState&quot;, &quot;versions&quot;: &quot;0+&quot;, &quot;fields&quot;: [
      { &quot;name&quot;: &quot;TopicName&quot;, &quot;type&quot;: &quot;string&quot;, &quot;versions&quot;: &quot;0-1&quot;, &quot;entityType&quot;: &quot;topicName&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The topic name.  This is only present in v0 or v1.&quot; },
      { &quot;name&quot;: &quot;PartitionIndex&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The partition index.&quot; },
      { &quot;name&quot;: &quot;ControllerEpoch&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The controller epoch.&quot; },
      { &quot;name&quot;: &quot;Leader&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;, &quot;entityType&quot;: &quot;brokerId&quot;,
        &quot;about&quot;: &quot;The broker ID of the leader.&quot; },
      { &quot;name&quot;: &quot;LeaderEpoch&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The leader epoch.&quot; },
      { &quot;name&quot;: &quot;Isr&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The in-sync replica IDs.&quot; },
      { &quot;name&quot;: &quot;ZkVersion&quot;, &quot;type&quot;: &quot;int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The ZooKeeper version.&quot; },
      { &quot;name&quot;: &quot;Replicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;0+&quot;,
        &quot;about&quot;: &quot;The replica IDs.&quot; },
      { &quot;name&quot;: &quot;AddingReplicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;3+&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The replica IDs that we are adding this partition to, or null if no replicas are being added.&quot; },
      { &quot;name&quot;: &quot;RemovingReplicas&quot;, &quot;type&quot;: &quot;[]int32&quot;, &quot;versions&quot;: &quot;3+&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;The replica IDs that we are removing this partition from, or null if no replicas are being removed.&quot; },
      { &quot;name&quot;: &quot;IsNew&quot;, &quot;type&quot;: &quot;bool&quot;, &quot;versions&quot;: &quot;1+&quot;, &quot;default&quot;: &quot;false&quot;, &quot;ignorable&quot;: true,
        &quot;about&quot;: &quot;Whether the replica should have existed on the broker or not.&quot; }
    ]}
  ]
</code></pre>
<h2><a class="header" href="#replica-sync副本同步" id="replica-sync副本同步">Replica sync(副本同步)</a></h2>
<p>在broker成为一个follower时候，会启动一个fetchThread，用于和partition leader同步消息
<img src="kafka/./replica-sync.svg" alt="replica-sync" /></p>
<h2><a class="header" href="#replica-leader-election" id="replica-leader-election">Replica Leader Election</a></h2>
<p>partition replica leader是由KafkaController来分配的.</p>
<p><img src="kafka/./replica-leader-election.svg" alt="replica-leader-election" /></p>
<p><img src="kafka/./elecLeaderForPartitions.svg" alt="electLeaderForPartitions" /></p>
<p>partion leader选择策略</p>
<pre><code class="language-scala">  def offlinePartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], uncleanLeaderElectionEnabled: Boolean, controllerContext: ControllerContext): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id)).orElse {
      if (uncleanLeaderElectionEnabled) {
        val leaderOpt = assignment.find(liveReplicas.contains)
        if (leaderOpt.isDefined)
          controllerContext.stats.uncleanLeaderElectionRate.mark()
        leaderOpt
      } else {
        None
      }
    }
  }
  def reassignPartitionLeaderElection(reassignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    reassignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  def preferredReplicaPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int]): Option[Int] = {
    assignment.headOption.filter(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id))
  }

  def controlledShutdownPartitionLeaderElection(assignment: Seq[Int], isr: Seq[Int], liveReplicas: Set[Int], shuttingDownBrokers: Set[Int]): Option[Int] = {
    assignment.find(id =&gt; liveReplicas.contains(id) &amp;&amp; isr.contains(id) &amp;&amp; !shuttingDownBrokers.contains(id))
  }
</code></pre>
<h1><a class="header" href="#ref-5" id="ref-5">Ref</a></h1>
<ol>
<li><a href="http://objcoding.com/2019/11/05/kafka-isr/">Kafka ISR 副本同步机制</a></li>
</ol>
<h1><a class="header" href="#kafka-controller-主要功能" id="kafka-controller-主要功能">Kafka Controller 主要功能</a></h1>
<p>kafka中会从broker server中选取一个作为controller，该controller通过ControllerChannelManager管理和每个broker通信的线程。</p>
<p>当zk中broker,topic, partion 等发生变动时，controller向每个broker发送消息, replica和partition 主要是通过replicaStateMachine和PartitionStateMachine来管理的
当replica或者partition leaderAndISR信息发生变动时候，controller通过这两个状态机，将状态的转换改为
相应的request请求，发送给broker。 </p>
<p>其中比较重要的请求是LeaderAndISR, 它指定了partition的leader和paritition in sync的replica list。</p>
<p>每个broker在zk中注册了ControllerChangeHandler，如果controller挂了，broker就会尝试去选举新的controller.</p>
<p><img src="kafka/./allpartion-overview.svg" alt="allpartionsoverview" /></p>
<p>controller会向broker发送三类请求: </p>
<ul>
<li>UpdateMetadataRequest: 更新元数据</li>
<li>LeaderAndIsrRequest: 创建分区，副本，leader和follower</li>
<li>StopReplicaRequest: 停止副本。</li>
</ul>
<p>controller和broker之间同步metadata</p>
<p>三类请求broker主要处理逻辑如下：</p>
<p><img src="kafka/./broker_update_metadata.svg" alt="broker_update_metadata" /></p>
<p>controller和broker之间处理IsrAndLeader请求</p>
<p><img src="kafka/./broker_handle_isr.svg" alt="borker-handle-isr" /></p>
<p>controller向broker发送stopReplica请求</p>
<p><img src="kafka/./broker-stop-replica.svg" alt="broker-stop-relica" /></p>
<h1><a class="header" href="#kafka-controller-channelmanager" id="kafka-controller-channelmanager">Kafka Controller: channelManager</a></h1>
<p>Controller和Broker之间采用队列来做异步通信,有专门的线程负责网络数据收发。</p>
<p>每次broker上线，Conntroller会新建一个RequestSendThread线程，当broker下线时候，会销毁该线程。</p>
<p>Controller和每个broker之间都有个RequestSendThread, Controller 将请求放到broker对应的请求队列中。
在RequestSendThread发送完请求，收到broker的响应之后，通过预先设置好的<code>sendEvent</code>回调，通过eventManager 采用异步的方式通知controller。</p>
<p><img src="kafka/./channel-manager.svg" alt="channel-manager" /></p>
<h2><a class="header" href="#kafka-controller-选举" id="kafka-controller-选举">Kafka Controller 选举</a></h2>
<p>每个kafka broker启动后, 会去zk中尝试创建ControllerZNode, 如果成功就会当选为controller。然后调用<code>onControllerFailover</code>开始controller的工作</p>
<ul>
<li>从zk中加载数据，刷新controllerContext中的各种cache.</li>
<li>在zk中注册broker, topic, patition等zk处理函数.</li>
<li>启动channelManager, 建立和其他broker之间通信channel</li>
<li>启动PartitionStateMachine和ReplicaStateMachine管理分区和副本状态.</li>
<li>启动kafkaScheduler，启动后台调度等</li>
</ul>
<p><img src="kafka/./controller-elect.svg" alt="controller-elect" /></p>
<h1><a class="header" href="#kafka-controller-zk监听" id="kafka-controller-zk监听">Kafka Controller zk监听</a></h1>
<p>在broker当选为controller之后，controller会在zk上注册一堆的handler， 处理broker/topic/partions等变化</p>
<pre><code class="language-scala">  private def onControllerFailover(): Unit = {
    info(&quot;Registering handlers&quot;)

    // before reading source of truth from zookeeper, register the listeners to get broker/topic callbacks
    val childChangeHandlers = Seq(brokerChangeHandler, topicChangeHandler, topicDeletionHandler, logDirEventNotificationHandler,
      isrChangeNotificationHandler)
    childChangeHandlers.foreach(zkClient.registerZNodeChildChangeHandler)
    val nodeChangeHandlers = Seq(preferredReplicaElectionHandler, partitionReassignmentHandler)
    nodeChangeHandlers.foreach(zkClient.registerZNodeChangeHandlerAndCheckExistence)
    //...other code
  }
</code></pre>
<h2><a class="header" href="#broker" id="broker">Broker</a></h2>
<p><code>BrokerChangeHandler</code>, 处理broker上线下线</p>
<p><img src="kafka/./controler-failover-zk-broker.svg" alt="controller-failover-zk-broker" /></p>
<h2><a class="header" href="#topic" id="topic">Topic</a></h2>
<h3><a class="header" href="#topic-change" id="topic-change">topic change</a></h3>
<p><img src="kafka/./topic-change.svg" alt="topic-change" /></p>
<h3><a class="header" href="#topic-delete" id="topic-delete">topic delete</a></h3>
<p><img src="kafka/./topic-delete.svg" alt="topic-delete" /></p>
<h2><a class="header" href="#isrchange" id="isrchange">Isrchange</a></h2>
<p>主要更新controller中的cache，并且controller发送sendUpdateMetadata通知所有的borker更新metadata.
<img src="kafka/./isr-change.svg" alt="isr-change" /></p>
<h2><a class="header" href="#logdirevent" id="logdirevent">LogDirEvent</a></h2>
<p><img src="kafka/./logdir-event.svg" alt="logdir-event" /></p>
<h2><a class="header" href="#replicaleaderelection" id="replicaleaderelection">ReplicaLeaderElection</a></h2>
<p><img src="kafka/./controller-process-replica-leader-election.svg" alt="replica-leader-election" /></p>
<h2><a class="header" href="#partitionreassignment" id="partitionreassignment">PartitionReassignment</a></h2>
<p><img src="kafka/./controller-partition-reasignment.svg" alt="partion-reassignment" /></p>
<h1><a class="header" href="#kafka-replica-assignment" id="kafka-replica-assignment">Kafka Replica Assignment</a></h1>
<h2><a class="header" href="#replica-迁移过程" id="replica-迁移过程">Replica 迁移过程</a></h2>
<p>缩写说明</p>
<ol>
<li>RS: replica set 所有的replica set</li>
<li>AR: add replica, 需要添加的replica</li>
<li>RR: remove replica,　需要删除的replica</li>
<li>TRS: target replica set, 要达到目标的replica set</li>
<li>ORS: target replica set, 原有的replica set</li>
</ol>
<p>具体迁移过程kafka代码中注释写的比较详细, 主要分为俩个阶段:</p>
<h4><a class="header" href="#phase-a" id="phase-a">Phase A</a></h4>
<p>如果AR没有在partition的ISR中，controller会发送NewReplica请求给AR的broker, 这些broker开始调用
replicaManager的makeFollowers, 启动Replicafetch线程和parititon leader同步，达到in-sync条件后，partition leader会将该broker加入到ISR中。</p>
<p>然后会触发controller在zk中注册的handler,开始下一步的迁移</p>
<h4><a class="header" href="#phase-b" id="phase-b">Phase B</a></h4>
<p>删除RR中的replica, 更新zk, 如果leader不在TRS中，controller需要发送LeaderAndIsr request给broker, 指定新的leader.</p>
<pre><code>   * Phase A (when TRS != ISR): The reassignment is not yet complete
   *
   *   A1. Bump the leader epoch for the partition and send LeaderAndIsr updates to RS.
   *   A2. Start new replicas AR by moving replicas in AR to NewReplica state.
   *
   * Phase B (when TRS = ISR): The reassignment is complete
   *
   *   B1. Move all replicas in AR to OnlineReplica state.
   *   B2. Set RS = TRS, AR = [], RR = [] in memory.
   *   B3. Send a LeaderAndIsr request with RS = TRS. This will prevent the leader from adding any replica in TRS - ORS back in the isr.
   *       If the current leader is not in TRS or isn't alive, we move the leader to a new replica in TRS.
   *       We may send the LeaderAndIsr to more than the TRS replicas due to the
   *       way the partition state machine works (it reads replicas from ZK)
   *   B4. Move all replicas in RR to OfflineReplica state. As part of OfflineReplica state change, we shrink the
   *       isr to remove RR in ZooKeeper and send a LeaderAndIsr ONLY to the Leader to notify it of the shrunk isr.
   *       After that, we send a StopReplica (delete = false) to the replicas in RR.
   *   B5. Move all replicas in RR to NonExistentReplica state. This will send a StopReplica (delete = true) to
   *       the replicas in RR to physically delete the replicas on disk.
   *   B6. Update ZK with RS=TRS, AR=[], RR=[].
   *   B7. Remove the ISR reassign listener and maybe update the /admin/reassign_partitions path in ZK to remove this partition from it if present.
   *   B8. After electing leader, the replicas and isr information changes. So resend the update metadata request to every broker.
   *
   * In general, there are two goals we want to aim for:
   * 1. Every replica present in the replica set of a LeaderAndIsrRequest gets the request sent to it
   * 2. Replicas that are removed from a partition's assignment get StopReplica sent to them
   *
   * For example, if ORS = {1,2,3} and TRS = {4,5,6}, the values in the topic and leader/isr paths in ZK
   * may go through the following transitions.
   * RS                AR          RR          leader     isr
   * {1,2,3}           {}          {}          1          {1,2,3}           (initial state)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3}           (step A2)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     1          {1,2,3,4,5,6}     (phase B)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {1,2,3,4,5,6}     (step B3)
   * {4,5,6,1,2,3}     {4,5,6}     {1,2,3}     4          {4,5,6}           (step B4)
   * {4,5,6}           {}          {}          4          {4,5,6}           (step B6)
   *
   * Note that we have to update RS in ZK with TRS last since it's the only place where we store ORS persistently.
   * This way, if the controller crashes before that step, we can still recover.
</code></pre>
<p><img src="kafka/./partition-replica-assignment.svg" alt="repartition-replica-assignment" /></p>
<h1><a class="header" href="#kafka-partitionreplica-state-machine" id="kafka-partitionreplica-state-machine">Kafka partition/replica state machine</a></h1>
<h2><a class="header" href="#replica-状态机" id="replica-状态机">Replica 状态机</a></h2>
<p><img src="kafka/./replica-statemachine.svg" alt="replica state machine" /></p>
<h3><a class="header" href="#replica-状态入口" id="replica-状态入口">replica 状态入口</a></h3>
<p><img src="kafka/./replica-target-state.svg" alt="replica target state" /></p>
<h2><a class="header" href="#partition-状态机" id="partition-状态机">Partition 状态机</a></h2>
<p><img src="kafka/./partition-statemachine.svg" alt="partition_statemachine" /></p>
<h3><a class="header" href="#partition-状态入口" id="partition-状态入口">partition 状态入口</a></h3>
<p><img src="kafka/./partition-target-state.svg" alt="partition_target_state" /></p>
<h1><a class="header" href="#txn-coordinator" id="txn-coordinator">Txn coordinator</a></h1>
<p>kafka streams中实现exactly once处理
read process write cycle</p>
<pre><code class="language-java">KafkaProducer producer = createKafkaProducer(
  “bootstrap.servers”, “localhost:9092”,
  “transactional.id”, “my-transactional-id”);

producer.initTransactions();

KafkaConsumer consumer = createKafkaConsumer(
  “bootstrap.servers”, “localhost:9092”,
  “group.id”, “my-group-id”,
  &quot;isolation.level&quot;, &quot;read_committed&quot;);

consumer.subscribe(singleton(“inputTopic”));

while (true) {
  ConsumerRecords records = consumer.poll(Long.MAX_VALUE);
  producer.beginTransaction();
  for (ConsumerRecord record : records)
    producer.send(producerRecord(“outputTopic”, record));
  producer.sendOffsetsToTransaction(currentOffsets(consumer), group);  
  producer.commitTransaction();
}
</code></pre>
<h2><a class="header" href="#dataflow" id="dataflow">Dataflow</a></h2>
<p><a href="https://www.confluent.io/blog/transactions-apache-kafka/">Transactions in Apache Kafka</a>中从整体上介绍了kafka中的事务处理流程,
摘抄如下：</p>
<ol>
<li>在A中producer和txn coordinator交互，获取唯一producerId，注册涉及到的partition等。主要发送请为InitProducerId, AddPartitionsToTxn, AddOffsetsToPartitions</li>
<li>在B中txn coordinator将事务各种状态写入日志中。</li>
<li>在C中producer正常向各个topic paritition写数据。</li>
<li>在D中coordinator开始两阶段提交，coordinator确保每个paritition将WriteMark写入成功。</li>
</ol>
<p><img src="kafka/./txn-dataflow.png" alt="tx-dataflow" /></p>
<h2><a class="header" href="#findcoordinator" id="findcoordinator">FindCoordinator</a></h2>
<p>首先producer发送FindCoordinator请求找到transcationId对应的coordinator. transactionId由client端提供，保证唯一性。
服务端会根据transactionId做hash，分配到相应的topic state 的paritition 中。
该parition 的leader即为这个事务的coordinator.</p>
<p><img src="kafka/./txn-find-coordinator.svg" alt="txn-find-coordinator" /></p>
<h2><a class="header" href="#initproducerid" id="initproducerid">InitProducerId</a></h2>
<p>生成全局唯一producerId, 每个transactionId对应着一个TransactionMetadata,
其中的topicPartitions 该事务涉及到的topic partition set.</p>
<p>服务端生成ProducerID时候，有个producerManager每次向zk申请一段的producerId区间，请求来了，先用改区间的id，如果用完了
就像zk再申请。这里面使用了expect zk version 来做分布式控制。避免申请的block被其他的txn coordinator覆盖了。</p>
<p><img src="kafka/./txn-producer-id.svg" alt="txn-producer-id" /></p>
<h2><a class="header" href="#addpartitionstotxn" id="addpartitionstotxn">AddPartitionsToTxn</a></h2>
<p>向事务添加Partitions, 或者提交当前消费的offset, 由于提交offset也是一种写入topic paritition行为，所以这边统一处理了。</p>
<p><img src="kafka/./txn-add-partition.svg" alt="txn-addPartitions" /></p>
<h2><a class="header" href="#endtxn" id="endtxn">endTxn</a></h2>
<p>最后producer发送endTxn请求， commit/abort 事务, coordinator开始两阶段提交。</p>
<h3><a class="header" href="#准备阶段preparecommitprepareabort" id="准备阶段preparecommitprepareabort">准备阶段：PrepareCommit/PrepareAbort</a></h3>
<p>将prepareCommit/PrepareAbort写入日志中, 写成功之后，coordinator会保证事务一定会被commit或者abort.</p>
<p><img src="kafka/./txn-prepare.svg" alt="txn-prepare" /></p>
<h3><a class="header" href="#提交阶段" id="提交阶段">提交阶段</a></h3>
<p>prepareCommit/preapreAbort日志写入成功后调用<code>sendTxnMarkersCallback</code>, coordinator 向事务中涉及到的broker发送WriteTxnMarker 请求，coordinator会一直尝试发送直到成功。
所有broker都响应成功后，会写入日志，并迁移到complete状态。</p>
<p><code>SendTxnMarkers</code>将请求放入队列中, 有个单独的InterBrokerThread线程负责从队列, 以及处理失败的队列中取出这些消息，然后将相同broker的请求batch起来，统一发送。</p>
<p><img src="kafka/./txn-commit.svg" alt="txn-commit" /></p>
<h3><a class="header" href="#broker对writemarkers请求的处理" id="broker对writemarkers请求的处理">broker对WriteMarkers请求的处理</a></h3>
<p><img src="kafka/./txn-write-markers.svg" alt="txn-write-markers" /></p>
<h2><a class="header" href="#txnimmigration" id="txnimmigration">TxnImmigration</a></h2>
<p>txn coordinator partition leader发生了变化，新的leader读取事务日志，加载到内存中，保存在变量<code>transactionMetadataCache</code>中.
对于PrepareCommit/PrepareAbort状态的事务会重新<code>SendTxnMarkers</code>请求</p>
<p><img src="kafka/./txn-immigration.svg" alt="txn-immigration" /></p>
<h2><a class="header" href="#事务状态机迁移" id="事务状态机迁移">事务状态机迁移</a></h2>
<p>状态迁移时候先<code>prepareTransionTo</code> 设置要转移到的Metadata状态, 然后调用appendTransactionToLog将事务写入日志，日志写入成功后
调用completeTransitionTo 迁移到目标状态</p>
<p><img src="kafka/./txn-state.svg" alt="txn-state" /></p>
<h2><a class="header" href="#事务日志消息格式" id="事务日志消息格式">事务日志消息格式</a></h2>
<p>事务日志中消息格式如下, 启动了log compaction</p>
<p><img src="kafka/./txn-message.svg" alt="txn-message" /></p>
<h2><a class="header" href="#ref-6" id="ref-6">Ref</a></h2>
<ol>
<li><a href="https://www.confluent.io/blog/transactions-apache-kafka/">Transactions in Apache Kafka</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Transactional Messaging in Kafka</a></li>
<li><a href="https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#heading=h.i4ub5zye01nh">Exactly Once Delivery and Transactional Messaging in Kafka</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka">Transactional Messaging in Kafka</a></li>
<li><a href="https://zhmin.github.io/2019/05/20/kafka-transaction/">Kafka 事务实现原理</a></li>
<li><a href="https://cloud.tencent.com/developer/article/1149669">Kafka设计解析8</a></li>
</ol>
<h1><a class="header" href="#draft-stream" id="draft-stream">Draft: Stream</a></h1>
<h2><a class="header" href="#streamgraphnode" id="streamgraphnode">StreamGraphNode</a></h2>
<p><img src="kafka/./stream-graph-node.svg" alt="stream graph node" /></p>
<h3><a class="header" href="#processor" id="processor">Processor</a></h3>
<p><img src="kafka/./stream-processor.svg" alt="stream processor" /></p>
<h3><a class="header" href="#processorcontext" id="processorcontext">ProcessorContext</a></h3>
<p><img src="kafka/./stream-processor-context.svg" alt="stream processor context" /></p>
<h2><a class="header" href="#stream-start" id="stream-start">Stream start</a></h2>
<p><img src="kafka/./stream-start.svg" alt="stream start" /></p>
<h2><a class="header" href="#questions-1" id="questions-1">Questions</a></h2>
<ol>
<li>DAG图是怎么建立起来的。</li>
<li>Kafka怎么调度DAG？怎么在不同线程，不同机器上部署？</li>
<li>DAG节点之间是怎么通信的？单纯通过kafka topic ?</li>
<li>怎么处理节点之间的依赖关系的?</li>
<li>Stream中的localstate, sharestate是怎么搞得，怎么保证故障恢复的。状态存储实现快速故障恢复和从故障点继续处理</li>
<li>Window join 具体指的是啥</li>
<li>KStream和KTable在kafka中是怎么表示的。</li>
<li>Kafka中的window有哪些？分别是怎么实现的？</li>
<li>through方法提供了类似Spark的Shuffle机制，为使用不同分区策略的数据提供了Join的可能</li>
</ol>
<p>KTable, KStream, KGroupedTable</p>
<p>StreamsBuilder</p>
<p>StreamGraphNode;
GlobalStoreNode;
StateStoreNode;
storeBuilder</p>
<p>writeToTopology</p>
<p>map/filter/groupBy/join(leftJoin, outerJoin)
queryableStoreName;</p>
<p>context.getStateStore</p>
<p>kafka Stream的并行模型中，最小粒度为Task，而每个Task包含一个特定子Topology的所有Processor。因此每个Task所执行的代码完全一样，唯一的不同在于所处理的数据集互补。</p>
<p>这里要保证两个进程的<code>StreamsConfig.APPLICATION_ID_CONFIG</code>完全一样。因为Kafka Stream将<code>APPLICATION_ID_CONFIG</code>作为隐式启动的Consumer的Group ID。只有保证APPLICATION_ID_CONFIG相同，才能保证这两个进程的Consumer属于同一个Group，从而可以通过Consumer Rebalance机制拿到互补的数据集。</p>
<p>State store被用来存储中间状态。它可以是一个持久化的Key-Value存储，也可以是内存中的HashMap，或者是数据库。Kafka提供了基于Topic的状态存储。</p>
<p>Topic中存储的数据记录本身是Key-Value形式的，同时Kafka的log compaction机制可对历史数据做compact操作，保留每个Key对应的最后一个Value，从而在保证Key不丢失的前提下，减少总数据量，从而提高查询效率。</p>
<h1><a class="header" href="#ref-7" id="ref-7">Ref</a></h1>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1149756">Kafka设计解析（七）- Kafka Stream</a></li>
<li><a href="https://www.orchome.com/335">Kafka Streams开发者指南</a></li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-kafka-streams/kafka-streams-internals-TaskManager.html">Kafka Streams Internal: TaskManager</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Architecture">Kafka Streams Architecture</a></li>
</ol>
<h1><a class="header" href="#leveldb" id="leveldb">leveldb</a></h1>
<p>记录一些学习leveldb源码笔记</p>
<pre><code>https://github.com/google/leveldb
</code></pre>
<h2><a class="header" href="#struct-and-alg" id="struct-and-alg">struct and alg</a></h2>
<h3><a class="header" href="#skiplist" id="skiplist">skiplist</a></h3>
<p>skiplist 应用的非常广泛，O(log n)的查找复杂度, O(log n)的查找复杂度, 下图是wiki上找到的示意图</p>
<p><img src="leveldb/./images/800px-Skip_list.svg.png" alt="skiplist" /></p>
<p><img src="leveldb/./images/800px-Skip_list_add_element-en.gif" alt="sliplist insert" /></p>
<h1><a class="header" href="#ref-8" id="ref-8">ref</a></h1>
<p>[1] https://en.wikipedia.org/wiki/Skip_list</p>
<h1><a class="header" href="#draft" id="draft">Draft</a></h1>
<h3><a class="header" href="#数据结构之间引用关系" id="数据结构之间引用关系">数据结构之间引用关系</a></h3>
<ol>
<li>Cache</li>
<li>Table</li>
<li>VersionSet</li>
<li>Env</li>
</ol>
<p><img src="leveldb/./dbinterface.svg" alt="dbInterface" /></p>
<h3><a class="header" href="#db-get" id="db-get">DB Get</a></h3>
<p><img src="leveldb/./db-get.svg" alt="db-get" /></p>
<h3><a class="header" href="#db-put" id="db-put">DB Put</a></h3>
<p><img src="leveldb/./db-put.svg" alt="db-put" /></p>
<h3><a class="header" href="#db-compact" id="db-compact">DB Compact</a></h3>
<p><img src="leveldb/./db-compact.svg" alt="db-compact" /></p>
<h3><a class="header" href="#table-builder" id="table-builder">Table builder</a></h3>
<p>memtable写入文件过程</p>
<p><img src="leveldb/./table-builder.svg" alt="table-builder" /></p>
<p>table format</p>
<ol>
<li>restart point的作用是啥？</li>
</ol>
<p><img src="leveldb/./table-format.svg" alt="table-format" /></p>
<h2><a class="header" href="#versionset" id="versionset">VersionSet</a></h2>
<p><img src="leveldb/./version.svg" alt="versionset" /></p>
<h3><a class="header" href="#manifest文件" id="manifest文件">Manifest文件</a></h3>
<h3><a class="header" href="#versionedit" id="versionedit">VersionEdit</a></h3>
<h2><a class="header" href="#todo" id="todo">TODO:</a></h2>
<h2><a class="header" href="#wal日志" id="wal日志">WAL日志</a></h2>
<h2><a class="header" href="#iterator" id="iterator">Iterator</a></h2>
<h2><a class="header" href="#bloom-filter" id="bloom-filter">Bloom Filter</a></h2>
<h2><a class="header" href="#ref-9" id="ref-9">Ref</a></h2>
<ol>
<li><a href="https://github.com/google/leveldb/blob/master/doc/table_format.md">table format</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/51360281">LevelDB设计与实现 - 读写流程</a></li>
</ol>
<h1><a class="header" href="#代码及模块间关系" id="代码及模块间关系">代码及模块间关系</a></h1>
<p><img src="leveldb/code-struct-overview.svg" alt="code-struct-overview" /></p>
<h2><a class="header" href="#具体细节" id="具体细节">具体细节</a></h2>
<p><img src="leveldb/./dbinterface.svg" alt="dbInterface" /></p>
<h1><a class="header" href="#leveldb-write-流程" id="leveldb-write-流程">LevelDB Write 流程</a></h1>
<h3><a class="header" href="#数据写入流程" id="数据写入流程">数据写入流程</a></h3>
<p>leveldb中数据写入流程如下:</p>
<ol>
<li>首先会将kv batch写入日志中，如果宕机了，能从日志中恢复过来，由于采用顺序写的方式，速度很快。</li>
<li>确保memtable的空间足够（没有超过一定大小限制），如果memtable没足够空间了，会新建一个memtable, 并将老的memtable转为
immtable，然后由后台压缩线程将immtable写入到level 0 文件。如果level 0 文件个数超过限制，也会触发background 压缩线程。</li>
<li>将kv batch插入memtable中, memtable的底层实现为skiplist, 插入时间复杂度为O(Log(n)),每个key,value插入都有自己的sequnceNumber.
用来控制版本号.</li>
</ol>
<p><img src="leveldb/./db-put-overview.svg" alt="db-put-overview" /></p>
<h3><a class="header" href="#写入细节" id="写入细节">写入细节</a></h3>
<ol>
<li>由MakeRoomsForWrite来保证memtable空间足够写入新的kv，如果immtable正在等待被写到文件中，或者level0文件个数超过阈值了，则需要阻塞等待后台线程处理完毕。由<code>backgroup_work_finished_signal_</code>condvar控制。</li>
<li>多线程写入时候，有个<code>writes_</code>队列做并发控制, writes_队列也使用condvar来控制，<code>writes_</code>队列开头的writer写完后，触发condvar，下个writer线程接着写。</li>
<li>immtable由<code>CompactMemtable</code>写入level 0文件</li>
<li>后台线程压缩时候，先使用<code>PickCompaction</code>选择需要合并压缩的sstable文件，然后使用<code>DoCompactionWork</code>做归并排序合并。</li>
<li>每次写入都会更新versionSet的LastSequnceNumber，用于版本控制,Sequnce越大，表明key,value值越新。</li>
</ol>
<p><img src="leveldb/./db-put.svg" alt="db-put" /></p>
<h3><a class="header" href="#wal-日志写入" id="wal-日志写入">WAL 日志写入</a></h3>
<p><img src="leveldb/./write_batch_internal.svg" alt="write_batch" /></p>
<h3><a class="header" href="#wal-日志恢复" id="wal-日志恢复">WAL 日志恢复</a></h3>
<p><img src="leveldb/./wal-log-recover.svg" alt="wal-log-recover" /></p>
<h1><a class="header" href="#leveldb-read流程" id="leveldb-read流程">LevelDB Read流程</a></h1>
<h3><a class="header" href="#数据读取流程" id="数据读取流程">数据读取流程</a></h3>
<ol>
<li>根据key和options中snapshot 拼接为looupKey</li>
<li>现在MemTable中查找，然后再immutable中查找，最后到level文件中查找。</li>
<li>通过version中的<code>files_</code>可以获得当前version中所有level file的列表。</li>
<li>level0中的文件key range有重叠，所有要每个文件都搜索。</li>
<li>其他level的通过fileMeta中记录的key range，定位到相应的sstable file.</li>
<li>文件操作：先从cache中查找，是否sstable的datablock index和bloomfilter已在内存中，如果不在的话，加载这些到内存中。cache以LRU方式来更新，淘汰。</li>
<li>先从datablock index中定位到相应的datablock和bloom filter,通过bloom filter快速查看key是否不存在，避免不必要的文件操作。</li>
<li>文件操作: 读取datablock到内存中，做二分查找, 将datablock放到缓存中。</li>
</ol>
<p><img src="leveldb/./db-get-overview.svg" alt="db-get-overview" /></p>
<h3><a class="header" href="#读取细节" id="读取细节">读取细节</a></h3>
<p>涉及到的模块说明:</p>
<ol>
<li>VersonSet负责维护version信息。</li>
<li>每个version中的<code>file_</code>数据成员，维护了每个层级的FileMetaData.</li>
<li>每个FileMetadata记录了该file的最大值和最小值，方便查找key,value时候，快速定位。</li>
<li>TableCache封装了Table和LRUCache逻辑。</li>
<li>Table封装了table加载，查找等逻辑。</li>
</ol>
<p><img src="leveldb/./db-get.svg" alt="db-get" /></p>
<h1><a class="header" href="#sstable-文件格式和读写" id="sstable-文件格式和读写">SSTable 文件格式和读写</a></h1>
<h2><a class="header" href="#table-format" id="table-format">Table format</a></h2>
<p>table文件分为Foot,metadataindex, dataIndex, metadat block, datablock这几块。</p>
<ol>
<li>Footer 48个字节，以Magic number为结尾。存储了指向metaDataIndex和DataIndex的BlockHandle（offset, size)</li>
<li>MetaBlock存储了bloomfilter 相关数据</li>
<li>DataIndexBlock 存储了每个block的lastKey，value为Datablock的blockHanle(offset和size)</li>
<li>MetaIndexBlock 中也是key,value形式，key为 <code>filter.filter_policy_name</code>，value为filterblockHandle, 当前只有bloomFilter</li>
<li>RestartPoint用于记录key shared共同前缀开始的位置。</li>
<li>每个DataBlock/IndexBlock除了原始数据，还包含了compressType(是否压缩）以及CRC32用于校验。</li>
</ol>
<p><img src="leveldb/./table-format.svg" alt="table-format" /></p>
<h2><a class="header" href="#table-write-流程" id="table-write-流程">Table write 流程</a></h2>
<p><img src="leveldb/./table-builder.svg" alt="table-builder" /></p>
<h2><a class="header" href="#table-读取流程" id="table-读取流程">Table 读取流程</a></h2>
<p>TableOpen中会读取文件的Footer, 读取indexBlock以及解析Metadatablock.</p>
<p><img src="leveldb/./table-read.svg" alt="Table-read" /></p>
<h1><a class="header" href="#versionset和manifest" id="versionset和manifest">Versionset和Manifest</a></h1>
<h2><a class="header" href="#manifest文件写入" id="manifest文件写入">Manifest文件写入</a></h2>
<p>version记录了当前每个level的各个文件的FileMetadata.</p>
<p>在压缩时候，每个level的FileMetadata可能会更改, 这种修改是用VersionEdit来表示的, 每次修改会将VersionEdit Encode写入日志中, 方便崩溃时候能够从Manifest日志文件中恢复。</p>
<p><img src="leveldb/./versionset-edit.svg" alt="versionset" /></p>
<h2><a class="header" href="#recover" id="recover">Recover</a></h2>
<p>Current文件内容记录了当前的manifest文件, 在DBOpen时候会去加载Mainfest文件，然后读取每个versionEditRecord
将它Decode为VersionEdit，然后一个个的apply，最终得到最后的version, 最后加入到VersionSet中。</p>
<p><img src="leveldb/./versionset-recover.svg" alt="versionset" /></p>
<p>遗留问题：
SequenceNumber和FilNumber这些是怎么保存的？</p>
<h1><a class="header" href="#compact" id="compact">Compact</a></h1>
<h3><a class="header" href="#pickcompaction" id="pickcompaction">PickCompaction</a></h3>
<p>选择要合并compact的FileMetaData</p>
<p><img src="leveldb/./pick-compaction.svg" alt="pick-compaction" /></p>
<h3><a class="header" href="#多路归并compact" id="多路归并compact">多路归并Compact</a></h3>
<p>将选择好的FilemetaData合并，输出到level+1层。通过versionEdit更改Version.</p>
<p><img src="leveldb/./do-compaction.svg" alt="db-compact" /></p>
<h1><a class="header" href="#iterator-迭代器" id="iterator-迭代器">Iterator 迭代器</a></h1>
<h2><a class="header" href="#iterator-继承关系" id="iterator-继承关系">Iterator 继承关系</a></h2>
<p><img src="leveldb/./iterator.svg" alt="iterator" /></p>
<h2><a class="header" href="#blockiter" id="blockiter">BlockIter</a></h2>
<p>Table中某个BlockData数据块的iter, 这里面有意思的restartPointer, restartPointer指向的record, key没有共享部分。
所以Seek时候先通过SeekToRestartPoint，找到合适的RestartPoint点，然后再使用ParseNextKey迭代遍历。</p>
<p><img src="leveldb/./block_iter.svg" alt="iterator" /></p>
<h2><a class="header" href="#levelfilenumiterator" id="levelfilenumiterator">LevelFileNumIterator</a></h2>
<p>用于遍历一组FileMetadat中定位target所在的FileMeta Index, 在TwoLevelIterator中作为index iter使用。</p>
<p><img src="leveldb/./level_file_num_iterator.svg" alt="level_file_num_iterator" /></p>
<h2><a class="header" href="#twoleveliterator" id="twoleveliterator">TwoLevelIterator</a></h2>
<p>双层迭代器，先通过index找到对应的block，调用<code>block_function</code>创建相应的block iterator.
增加了SkipEmptyData检查，当一个blockIter迭代完后，自动切换到下一个block iter.</p>
<p>TwoLevelIterator可以套娃:</p>
<ol>
<li>IndexBlockIter和DataBlockIter套在一起得到一个TableIterator</li>
<li>LevelFileNumIterator和TableIterator套在一次，得到某一层的Iterator.</li>
</ol>
<p><img src="leveldb/./two_level_iterator.svg" alt="two_level_iterator" /></p>
<h2><a class="header" href="#mergingiterator" id="mergingiterator">MergingIterator</a></h2>
<p>归并N个有序的iterator.</p>
<p><img src="leveldb/./merging_iterator.svg" alt="merging_iterator" /></p>
<h2><a class="header" href="#dbimplnewiterator" id="dbimplnewiterator">DBImpl::NewIterator</a></h2>
<ol>
<li>内存中的mm_和imm_分别作为一路iter，放到merging iter中</li>
<li>level0层由于table文件之间有overlap的，所以每个level0对应tableIterator作为一路放在merging itertor中。</li>
<li>level1 ~ levenN层 LevelFileumIterator和TableIterator通过TwoLevelIterator套在一起，得到某一层的iterator.</li>
</ol>
<p><img src="leveldb/./dbimpl-iterator.svg" alt="dbimpl-newIterator" /></p>
<h1><a class="header" href="#bloom-filter-1" id="bloom-filter-1">Bloom filter</a></h1>
<h2><a class="header" href="#filer-policy" id="filer-policy">filer policy</a></h2>
<p>leveldb中filter用于快速确定key是否不在table中, 一堆key经过一系列的hash计算后，可以得到
很小指纹数据。查询时候，可以根据这个指纹信息，快速排除key不存在的情况。</p>
<p><img src="leveldb/./bloom-filter.svg" alt="bloom-filter" /></p>
<p>计算keys对应的指纹数据：</p>
<pre><code class="language-cpp">for (int i = 0; i &lt; n; i++) {
  // Use double-hashing to generate a sequence of hash values.
  // See analysis in [Kirsch,Mitzenmacher 2006].
  uint32_t h = BloomHash(keys[i]);
  const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15);  // Rotate right 17 bits
  for (size_t j = 0; j &lt; k_; j++) {
    const uint32_t bitpos = h % bits;
    array[bitpos / 8] |= (1 &lt;&lt; (bitpos % 8));
    h += delta;
  }
</code></pre>
<p>match过程:</p>
<pre><code class="language-cpp">uint32_t h = BloomHash(key);
const uint32_t delta = (h &gt;&gt; 17) | (h &lt;&lt; 15);  // Rotate right 17 bits
for (size_t j = 0; j &lt; k; j++) {
  const uint32_t bitpos = h % bits;
  if ((array[bitpos / 8] &amp; (1 &lt;&lt; (bitpos % 8))) == 0) return false;
  h += delta;
}
return true;
</code></pre>
<h2><a class="header" href="#filter数据写入和读取流程" id="filter数据写入和读取流程">filter数据写入和读取流程</a></h2>
<h3><a class="header" href="#写入流程" id="写入流程">写入流程</a></h3>
<p>每个table的block数据的filter数据是写在一块的，通过一个<code>filter_offsets</code>来保存每个datablock对应的filter
在整个filter数据中的偏移和大小。</p>
<p>TableBuilder时候，每次开始新的一个datablock，都会调用filter的start new block， 然后Add Key，value时候，调用
AddKey, 创建key的指纹数据。</p>
<p>最后Table finish时候，写入filter data block数据，并且在metaindexblock中添加filter_policy_name和filter data block handle</p>
<h3><a class="header" href="#读取流程" id="读取流程">读取流程</a></h3>
<p>每个talbe Get时候，会使用ReadFilter加载该table的所有filterdata, 然后根据blockData的offset 找到该block对应的
filter数据，并使用该数据来判断key是不是不存在。</p>
<p><img src="leveldb/./filter-policy.svg" alt="filter-policy" /></p>
<h1><a class="header" href="#rocksdb" id="rocksdb">RocksDB</a></h1>
<h1><a class="header" href="#draft-1" id="draft-1">Draft</a></h1>
<h2><a class="header" href="#class之间关系" id="class之间关系">Class之间关系</a></h2>
<p><img src="rocksdb/./class-relations.svg" alt="class-relations" /></p>
<h2><a class="header" href="#write" id="write">Write</a></h2>
<ol>
<li>最终怎么写到了memTable中。</li>
<li>WAL写的流程是什么样？</li>
</ol>
<p><img src="rocksdb/./write.svg" alt="write" /></p>
<h2><a class="header" href="#writebatch" id="writebatch">WriteBatch</a></h2>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<h2><a class="header" href="#columnfamily" id="columnfamily">ColumnFamily</a></h2>
<ol>
<li>Blob 中value和key是怎么对的上的？</li>
<li>数据结构之间怎么串起来的。 </li>
</ol>
<h2><a class="header" href="#write-thread" id="write-thread">Write Thread</a></h2>
<p>Writer的状态
<img src="rocksdb/./write_thread_state.svg" alt="write thread state" /></p>
<p>write thread过程
Write group leader 负责写入WAL日志。
memtable可能由group leader写，也有可能由各个writer 并发写。</p>
<p>write thread是对写线程的抽象
<img src="rocksdb/./write_thread.svg" alt="write thread" /></p>
<p>write impl
<img src="rocksdb/./pipline_writeimpl.svg" alt="pipelined-write impl" /></p>
<h2><a class="header" href="#preprocesswrite" id="preprocesswrite">PreprocessWrite</a></h2>
<p><img src="rocksdb/./preprocess_write.svg" alt="preprocess write" /></p>
<h2><a class="header" href="#后台压缩" id="后台压缩">后台压缩</a></h2>
<p>MaybeScheduleFlushOrCompaction</p>
<p><img src="rocksdb/./flush_compaction.svg" alt="flush-compaction" /></p>
<p>后台线程压缩</p>
<p>compaction job之间是怎么划分的？怎么让不同线程去compact不同部分？</p>
<p><img src="rocksdb/./background-compaction.svg" alt="backgroup-compaction" /></p>
<h2><a class="header" href="#compaction-picker" id="compaction-picker">compaction picker</a></h2>
<h2><a class="header" href="#level-compaction-picker" id="level-compaction-picker">level compaction picker</a></h2>
<p>以下两张图摘自facebook wiki <a href="https://github.com/facebook/rocksdb/wiki/Leveled-Compaction">leveled-compaction</a></p>
<p><img src="rocksdb/./pre_l0_compaction.png" alt="level 0 compaction " /></p>
<p><img src="rocksdb/./pre_l1_compaction.png" alt="level 1 compaction" /></p>
<h1><a class="header" href="#column-family" id="column-family">Column Family</a></h1>
<ol>
<li>每个columnFamily有单独的Version, memtable以及imm memtable list</li>
<li>VersionStorageInfo 存储了属于该version的所有Filemetadata信息</li>
<li>读取时候，先从columnFaimly的memTable，然后imm list，然后version 中的各个level的文件</li>
<li>写时候，先写WAl日志，然后插入到memtable中，memtable在满时候，会转到imm list中, 然后由
后台线程flush到level0, 后台线程compact.</li>
</ol>
<p>rocks db中主要数据结构关系如下：</p>
<p><img src="rocksdb/./column-family-overview.svg" alt="column family overview" /></p>
<p>数据结构之间引用细节如下：</p>
<p><img src="rocksdb/./column_family.svg" alt="column family" /></p>
<h1><a class="header" href="#write-ahead-log" id="write-ahead-log">Write Ahead Log</a></h1>
<h3><a class="header" href="#writebatch-1" id="writebatch-1">WriteBatch</a></h3>
<p>put/delete等操作先写入writeBatch中</p>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<p>writeBatch中Record类型如下:</p>
<pre><code class="language-cpp">// WriteBatch::rep_ :=
//    sequence: fixed64
//    count: fixed32
//    data: record[count]
</code></pre>
<pre><code class="language-cpp">enum ValueType : unsigned char {
  kTypeDeletion = 0x0,
  kTypeValue = 0x1,
  kTypeMerge = 0x2,
  kTypeLogData = 0x3,               // WAL only.
  kTypeColumnFamilyDeletion = 0x4,  // WAL only.
  kTypeColumnFamilyValue = 0x5,     // WAL only.
  kTypeColumnFamilyMerge = 0x6,     // WAL only.
  kTypeSingleDeletion = 0x7,
  kTypeColumnFamilySingleDeletion = 0x8,  // WAL only.
  kTypeBeginPrepareXID = 0x9,             // WAL only.
  kTypeEndPrepareXID = 0xA,               // WAL only.
  kTypeCommitXID = 0xB,                   // WAL only.
  kTypeRollbackXID = 0xC,                 // WAL only.
  kTypeNoop = 0xD,                        // WAL only.
  kTypeColumnFamilyRangeDeletion = 0xE,   // WAL only.
  kTypeRangeDeletion = 0xF,               // meta block
  kTypeColumnFamilyBlobIndex = 0x10,      // Blob DB only
  kTypeBlobIndex = 0x11,                  // Blob DB only
  // When the prepared record is also persisted in db, we use a different
  // record. This is to ensure that the WAL that is generated by a WritePolicy
  // is not mistakenly read by another, which would result into data
  // inconsistency.
  kTypeBeginPersistedPrepareXID = 0x12,  // WAL only.
  // Similar to kTypeBeginPersistedPrepareXID, this is to ensure that WAL
  // generated by WriteUnprepared write policy is not mistakenly read by
  // another.
  kTypeBeginUnprepareXID = 0x13,  // WAL only.
  kMaxValue = 0x7F                // Not used for storing records.
};
</code></pre>
<h3><a class="header" href="#memtableinserter" id="memtableinserter">MemtableInserter</a></h3>
<p>MemTableInsertor 遍历writeBatch，将记录插入到memtable中,使用MemTableRep封装了skiplist和VectorRep这两种类型的memtable;</p>
<p><img src="rocksdb/./write-batch-iter.svg" alt="write batch iter" /></p>
<h3><a class="header" href="#writetowal" id="writetowal">WriteToWAL</a></h3>
<p>日志会被分片为固定大小kBlocksize, 太小的会被填充padding,太大的会被切分为first/mid/last等分片record</p>
<p>固定大小这个有什么优势吗？</p>
<p><img src="rocksdb/./write-to-wal.svg" alt="write to wal" /></p>
<h1><a class="header" href="#rocksdb-write流程" id="rocksdb-write流程">RocksDB Write流程</a></h1>
<h2><a class="header" href="#writebatch-2" id="writebatch-2">WriteBatch</a></h2>
<p><img src="rocksdb/./write-batch.svg" alt="write-batch" /></p>
<h2><a class="header" href="#preprocesswrite-1" id="preprocesswrite-1">PreprocessWrite</a></h2>
<h3><a class="header" href="#schedule-flush" id="schedule-flush">schedule flush</a></h3>
<p>schedule flush, 将满的memtable转变为immtable, 加到<code>flush_schedule_</code>队列中
由<code>BackgrounFlush</code>将immtable刷到dish上。</p>
<p><img src="rocksdb/./schedule_flushes.svg" alt="schedule flush" /></p>
<h2><a class="header" href="#write-thread-1" id="write-thread-1">Write thread</a></h2>
<p>Writer的状态</p>
<p><img src="rocksdb/./write_thread_state.svg" alt="write thread state" /></p>
<p>write 相关struct之间引用关系</p>
<p><img src="rocksdb/./write_struct.svg" alt="write struct" /></p>
<h1><a class="header" href="#backgroud-flush-and-compaction" id="backgroud-flush-and-compaction">Backgroud flush and compaction</a></h1>
<h2><a class="header" href="#maybescheduleflushorcompaction" id="maybescheduleflushorcompaction">MaybeScheduleFlushOrCompaction</a></h2>
<p><code>MaybeScheduleFlushOrCompaction</code>会使用线程池调度，最后在后台线程中调用<code>BackgroundFlush</code>
和<code>BackgrondCompaction</code>分别做memtable的flush和ssfile的compaction.</p>
<p><img src="rocksdb/./MaybeScheduleFlushOrCompaction.svg" alt="MaybeScheduleFlushOrCompaction" /></p>
<h2><a class="header" href="#后台线程调度schedule" id="后台线程调度schedule">后台线程调度Schedule</a></h2>
<p><img src="rocksdb/./schedule-bgthread.svg" alt="schedule-bgtread" /></p>
<h2><a class="header" href="#后台线程flush" id="后台线程flush">后台线程flush</a></h2>
<h3><a class="header" href="#生成flushrequest放入flush队列中" id="生成flushrequest放入flush队列中">生成flushRequest放入flush队列中</a></h3>
<p>向<code>flush_queue_</code>中放入FlushRequest的数据流程如下:</p>
<p><img src="rocksdb/./flush_queue_put.svg" alt="flush_queue_put" /></p>
<p>具体函数调用细节如下：</p>
<p><img src="rocksdb/./flush_queue_put_detail.svg" alt="flush queue put detail" /></p>
<h3><a class="header" href="#后台线程处理flush队列中请求" id="后台线程处理flush队列中请求">后台线程处理flush队列中请求</a></h3>
<p>后台线程执行<code>BackgroundFlush</code>从<code>flush_queue_</code>中取出FlushRequest转换为FlushJob. </p>
<p><img src="rocksdb/./flush-data-flow-overview.svg" alt="flush-data-flow-overview" /></p>
<p>cfd会被flush的条件</p>
<pre><code class="language-cpp">bool MemTableList::IsFlushPending() const {
  if ((flush_requested_ &amp;&amp; num_flush_not_started_ &gt; 0) ||
      (num_flush_not_started_ &gt;= min_write_buffer_number_to_merge_)) {
    assert(imm_flush_needed.load(std::memory_order_relaxed));
    return true;
  }
  return false;
}
</code></pre>
<p>最终调用<code>WriteLevel0Table</code> 将memtable写入磁盘中，具体调用关系如下:</p>
<p><img src="rocksdb/./background-flush.svg" alt="backgroud-flush" /></p>
<h2><a class="header" href="#后台线程compact" id="后台线程compact">后台线程compact</a></h2>
<h3><a class="header" href="#cfd放入compact队列" id="cfd放入compact队列">cfd放入compact队列</a></h3>
<p><img src="rocksdb/./background-compaction-put.svg" alt="background-compaction-put" /></p>
<h3><a class="header" href="#处理compact队列生成compactionjob" id="处理compact队列生成compactionjob">处理compact队列，生成compactionJob</a></h3>
<p>后台线程会通过<code>PickCompactionFromQueue</code> 去<code>compaction_queue_</code>中取出需要compact的ColumnFamilyData,
然后调用ComlumnFamilyData的PickCompaction 选择compactio的input level, output leve, 以及input files等，</p>
<p><img src="rocksdb/./background-compaction.svg" alt="backgroup-compaction" /></p>
<h4><a class="header" href="#多线程并发compact" id="多线程并发compact">多线程并发compact</a></h4>
<p>在compact Prepare中会将compactJob划分为不同的SubCompactionState，然后由多线程并发执行压缩</p>
<p><img src="rocksdb/./background-compaction-job.svg" alt="background-compaction-job" /></p>
<h4><a class="header" href="#compaction-picker-1" id="compaction-picker-1">Compaction Picker</a></h4>
<p>三种compaction style</p>
<p>Level Style Compaction</p>
<p>Universal Style Compaction</p>
<p>FIFO Style Compaction</p>
<h1><a class="header" href="#compaction-picker-2" id="compaction-picker-2">Compaction Picker</a></h1>
<h2><a class="header" href="#compaction生成流程" id="compaction生成流程">Compaction生成流程:</a></h2>
<ol>
<li>SetupInitialFiles 选择要compaction的level和input files</li>
<li>SetupOtherL0FilesIfNeeded和SetupOtherInputsIfNeeded补充选择和input files overlap的文件</li>
<li>最后GetCompaction 生成最终的Compaction然后重新CompuateScore用于下次Compact</li>
</ol>
<p><img src="rocksdb/./level-compaction-picker-overview.svg" alt="leveled compaction pick overview" /></p>
<p>SetupInitialFiles 初始选择的优先级顺序，当前一个选择为空时候，才会去选择下一个:</p>
<p><img src="rocksdb/./SetupInitalFiles-pri.svg" alt="SetupInitialFiles-pri" /></p>
<h3><a class="header" href="#compactionscore" id="compactionscore">CompactionScore</a></h3>
<h4><a class="header" href="#todo拆分为不同的子图" id="todo拆分为不同的子图">TODO拆分为不同的子图</a></h4>
<p><img src="rocksdb/./compaction-score.svg" alt="compaction score" /></p>
<h3><a class="header" href="#详细调用图如下" id="详细调用图如下">详细调用图如下</a></h3>
<p><img src="rocksdb/./level-compaction-picker.svg" alt="level-compaction-picker" /></p>
<h2><a class="header" href="#ref-10" id="ref-10">Ref</a></h2>
<ol>
<li>leveled-compaction: https://github.com/facebook/rocksdb/wiki/Leveled-Compaction</li>
<li>choose level compaction files: https://github.com/facebook/rocksdb/wiki/Choose-Level-Compaction-Files</li>
</ol>
<h1><a class="header" href="#read-流程" id="read-流程">read 流程</a></h1>
<h3><a class="header" href="#questions-2" id="questions-2">Questions</a></h3>
<p>SuperVersion ？ 为啥起这个名字？</p>
<h3><a class="header" href="#多级index" id="多级index">多级index:</a></h3>
<ol>
<li>ColumnFamily 根据Version中的<code>std::vector&lt;FileMetaData*&gt;</code> 定位到具体的Table。</li>
<li>Table根据<code>bloom filter</code>快速排出key不存在的case，如果key不存在，避免后续的磁盘操作。</li>
<li>Table根据<code>IndexBlock</code> 定位到对应的Datablock。</li>
<li>根据Datablock数据中的<code>restartPoint</code>列表二分查找，找到对应的restartPoint偏移, 进一步缩小查找区间。</li>
<li>在具体的<code>restartPoint</code>之间遍历查找具体的key</li>
</ol>
<p><img src="rocksdb/./table_read_index.svg" alt="table read index" /></p>
<h3><a class="header" href="#多级lru缓存" id="多级lru缓存">多级LRU缓存:</a></h3>
<ol>
<li>TableCache</li>
<li>DataBlockCache</li>
<li>RowCache</li>
</ol>
<p><img src="rocksdb/./table_read_cache.svg" alt="table read cache" /></p>
<p>详细调用关系：</p>
<p><img src="rocksdb/./dbimpl_get.svg" alt="db impl get" /></p>
<h1><a class="header" href="#blob" id="blob">Blob</a></h1>
<p>Questions:</p>
<ol>
<li>PinnableSlice 这个作用是啥</li>
<li>rocksdb的blob和pingcap的titan之间关系？实现逻辑？</li>
<li>Blob文件是怎么选择的</li>
</ol>
<p>Blob将key和value分来开存储。</p>
<pre><code class="language-cpp">// A wrapped database which puts values of KV pairs in a separate log
// and store location to the log in the underlying DB.
</code></pre>
<p><img src="rocksdb/./blob-db.svg" alt="blob db" /></p>
<h2><a class="header" href="#blob-log" id="blob-log">Blob Log</a></h2>
<p>blob log format</p>
<p><img src="rocksdb/./blob-log-format.svg" alt="blob log format" /></p>
<p>blob index</p>
<p><img src="rocksdb/./blob-index.svg" alt="blob index" /></p>
<h2><a class="header" href="#open" id="open">Open</a></h2>
<p>Blob open
<img src="rocksdb/./blob-open.svg" alt="blob open" /></p>
<h2><a class="header" href="#put" id="put">Put</a></h2>
<p>BlobPut</p>
<p><img src="rocksdb/./blob-put.svg" alt="blob put" /></p>
<h2><a class="header" href="#get" id="get">Get</a></h2>
<p>BlobGet
<img src="rocksdb/./blob-get.svg" alt="blob get" /></p>
<h1><a class="header" href="#transaction" id="transaction">Transaction</a></h1>
<h2><a class="header" href="#transaction-struct" id="transaction-struct">Transaction struct</a></h2>
<p><img src="rocksdb/./transaction-struct.svg" alt="transaction struct" /></p>
<h3><a class="header" href="#主要数据成员" id="主要数据成员">主要数据成员</a></h3>
<p>rocksdb中，每个事务主要有<code>track_keys_</code>和<code>write_batch_</code>这两个数据成员，</p>
<ul>
<li><code>track_keys_</code>用于跟踪管理该事务写操作涉及的key </li>
<li><code>write_batch_</code>用于记录事务最终的写结果。</li>
</ul>
<p>所有的悲观事务(pessimistic transaction), 通过<code>txn_db_impl_</code>指针共享 PessimisticTransactionDB，
从而共享全局的TransactionLockMgr,用来统一管理key的lock。</p>
<p><img src="rocksdb/./track-key.svg" alt="track key" /></p>
<h2><a class="header" href="#乐观事务" id="乐观事务">乐观事务</a></h2>
<p>在commit的时候才去检查key的冲突</p>
<p>一些问题：</p>
<ol>
<li>根据什么判断是否有冲突的？貌似是根据sequnceNumber，但是具体细节不太清楚</li>
<li><code>bucketed_locks_</code>的作用是啥？</li>
<li>CommitWithSerialValidate和 CommitWithParallelValidate这两者区别是啥？</li>
</ol>
<p><img src="rocksdb/./optimistic-transaction-commit.svg" alt="optimistic transaction commit" /></p>
<h2><a class="header" href="#悲观事务" id="悲观事务">悲观事务</a></h2>
<p>分为三种？</p>
<ol>
<li>writeCommitedTxn</li>
</ol>
<p>WriteCommitted, which means that the data is written to the DB, i.e., the memtable, only after the transaction is committed</p>
<ol start="2">
<li>WritePrepared</li>
<li>WriteUnpreparedTxnDB</li>
</ol>
<p><img src="rocksdb/./pessimistic-transaction.svg" alt="pessimistic transaction" /></p>
<h2><a class="header" href="#参考-2" id="参考-2">参考</a></h2>
<p><a href="https://github.com/facebook/rocksdb/wiki/WritePrepared-Transactions">Facebook WritePrepared</a></p>
<h1><a class="header" href="#optimistic-transaction" id="optimistic-transaction">Optimistic Transaction</a></h1>
<p>乐观事务在commit前，Write操作只会记录事务有哪些key, 不需要做加锁和key冲突检测，适合事务之间
write key重叠比较低的场景。</p>
<p>乐观事务在write时候，使用<code>tracked_keys</code>, 记录受影响的key以及该key的seq, </p>
<p><img src="rocksdb/./optimistic-transaction.svg" alt="optimistic transaction" /></p>
<p>在commit时候会遍历该<code>tracked_keys</code>, 对每个key查找当前db中该key的seq，然后和<code>tracked_key</code>中seq比较。
如果数据库中的seq比key的seq新，则认为发生了冲突。</p>
<p>不太理解这里面的<code>min_uncommited</code> 起了什么作用.</p>
<p><img src="rocksdb/./check-key-conflict.svg" alt="check key conflict" /></p>
<p>遍历<code>TransactionKeyMap</code>, 检查每个key的冲突</p>
<pre><code class="language-cpp">Status TransactionUtil::CheckKeysForConflicts(DBImpl* db_impl,
                                              const TransactionKeyMap&amp; key_map,
                                              bool cache_only) {
    //other code..
    //遍历迭代key_map
    for (const auto&amp; key_iter : keys) {
      const auto&amp; key = key_iter.first;
      const SequenceNumber key_seq = key_iter.second.seq;

      result = CheckKey(db_impl, sv, earliest_seq, key_seq, key, cache_only);

      if (!result.ok()) {
        break;
      }
    }

}
</code></pre>
<p>检查具体某个key的冲突</p>
<pre><code class="language-cpp">// min_uncommitted 默认值为 KMaxSequnceNumber
// snap_checker默认值为nullptr;
Status TransactionUtil::CheckKey(DBImpl* db_impl, SuperVersion* sv,
                                 SequenceNumber earliest_seq,
                                 SequenceNumber snap_seq,
                                 const std::string&amp; key, bool cache_only,
                                 ReadCallback* snap_checker,
                                 SequenceNumber min_uncommitted) {
  
  //...other code
    SequenceNumber seq = kMaxSequenceNumber;
    bool found_record_for_key = false;

    // When min_uncommitted == kMaxSequenceNumber, writes are committed in
    // sequence number order, so only keys larger than `snap_seq` can cause
    // conflict.
    // When min_uncommitted != kMaxSequenceNumber, keys lower than
    // min_uncommitted will not triggered conflicts, while keys larger than
    // min_uncommitted might create conflicts, so we need  to read them out
    // from the DB, and call callback to snap_checker to determine. So only
    // keys lower than min_uncommitted can be skipped.
    SequenceNumber lower_bound_seq =
        (min_uncommitted == kMaxSequenceNumber) ? snap_seq : min_uncommitted;

    // 去数据库中查找key的最新seq
    Status s = db_impl-&gt;GetLatestSequenceForKey(sv, key, !need_to_read_sst,
                                                lower_bound_seq, &amp;seq,
                                                &amp;found_record_for_key);

    if (!(s.ok() || s.IsNotFound() || s.IsMergeInProgress())) {
      result = s;
    } else if (found_record_for_key) {
      bool write_conflict = snap_checker == nullptr
                                ? snap_seq &lt; seq
                                : !snap_checker-&gt;IsVisible(seq);
      if (write_conflict) {
        result = Status::Busy();
      }
    }
  }
  return result;
}
</code></pre>
<p>一些问题：</p>
<ol>
<li>根据什么判断是否有冲突的？貌似是根据sequnceNumber，但是具体细节不太清楚</li>
<li><code>bucketed_locks_</code>的作用是啥？</li>
<li>CommitWithSerialValidate和 CommitWithParallelValidate这两者区别是啥？</li>
<li>key冲突检测是咋搞的</li>
<li>并行和顺序这个是怎么弄的</li>
</ol>
<h1><a class="header" href="#transaction-lock-mgr" id="transaction-lock-mgr">Transaction lock mgr</a></h1>
<p>TransactionLockMgr 用于管理悲观事务的key lock，所有的悲观事务，通过<code>txn_db_impl-&gt;lock_mgr_</code>指针共享
同一个lockmgr</p>
<h3><a class="header" href="#lockmap" id="lockmap">LockMap</a></h3>
<p>rocksdb中对于key lock做了多种优化</p>
<ol>
<li>首先根据ColumnFamilyId, 从LockMaps获得对应的LockMap</li>
<li>使用了thread local data来缓存全局的lock maps, 避免每次查询全局的lockmaps需要加锁</li>
<li>使用<code>GetStripe</code>把key做sharding获得相应的LockStripe,降低了锁冲突, 但是在同一个stripe中的key还是有并发等待问题.</li>
</ol>
<p><img src="rocksdb/./transaction-lock-level.svg" alt="transaction lock level" /></p>
<pre><code class="language-cpp">size_t LockMap::GetStripe(const std::string&amp; key) const {
  assert(num_stripes_ &gt; 0);
  return fastrange64(GetSliceNPHash64(key), num_stripes_);
}
</code></pre>
<p><code>GetLockMap</code>封装装了从thread local cache获取lockMap逻辑</p>
<pre><code class="language-cpp">std::shared_ptr&lt;LockMap&gt; TransactionLockMgr::GetLockMap(
    uint32_t column_family_id) {

  // First check thread-local cache
  if (lock_maps_cache_-&gt;Get() == nullptr) {
    lock_maps_cache_-&gt;Reset(new LockMaps());
  }

  auto lock_maps_cache = static_cast&lt;LockMaps*&gt;(lock_maps_cache_-&gt;Get());

  //首先从thread local cache中查找
  auto lock_map_iter = lock_maps_cache-&gt;find(column_family_id);
  if (lock_map_iter != lock_maps_cache-&gt;end()) {
    // Found lock map for this column family.
    return lock_map_iter-&gt;second;
  }

  //没找到的话，使用mutex访问全局LockMaps
  // Not found in local cache, grab mutex and check shared LockMaps
  InstrumentedMutexLock l(&amp;lock_map_mutex_);

  lock_map_iter = lock_maps_.find(column_family_id);
  if (lock_map_iter == lock_maps_.end()) {
    return std::shared_ptr&lt;LockMap&gt;(nullptr);
  } else {
    //插入到thread local cache中，方便下一次访问
    // Found lock map.  Store in thread-local cache and return.
    std::shared_ptr&lt;LockMap&gt;&amp; lock_map = lock_map_iter-&gt;second;
    lock_maps_cache-&gt;insert({column_family_id, lock_map});

    return lock_map;
  }
}
</code></pre>
<h3><a class="header" href="#获取释放key锁" id="获取释放key锁">获取/释放key锁</a></h3>
<p><img src="rocksdb/./transaction-lock-mgr.svg" alt="transaction lock mgr" /></p>
<h3><a class="header" href="#死锁检测" id="死锁检测">死锁检测</a></h3>
<p><img src="rocksdb/./transaction-lock-mgr-deadlock-detect.svg" alt="transaction lock mgr dead lock detect" /></p>
<h1><a class="header" href="#two-phase-commit" id="two-phase-commit">Two phase commit</a></h1>
<h2><a class="header" href="#write-commited-txn" id="write-commited-txn">Write Commited txn</a></h2>
<p>事务只有在提交之后，才会写入到db的memtable中，事务在数据库中读到的
kv都是提交之后的，这种需要在提交之前把所有的write kv操作保存在内存writeBatch中，
对于大的事务来说，内存是个瓶颈，另一方面，commit时候才集中的写入memtabe，这个延迟可能也无法忽略。</p>
<p><img src="rocksdb/./write-committed.svg" alt="write commited" /></p>
<p>WriteCommited 两阶段提交：</p>
<ul>
<li>Prepare阶段 将writebatch 写入WAL日志中,并将writeBatch中内容用<code>ktypeBeginPrepare(Xid)</code>, <code>kTypeEndPrepare(xid)</code> 括起来
由于只写到了WAL日志中,　其他事务看不到这个事务的修改</li>
<li>Commit阶段 向WAL日志写入commit 标记，比如<code>kTypeCommit(xid)</code> 并writeBatch中内容insert到memtable上，写入memtable之后，该事务的修改对其他事务就可以见了。
如果向WAL日志中写入<code>KtypeCommit(xid)</code>日志就挂了的话，下次recover时候，会重新从日志中恢复writeBatch，然后插入到memtabl中。</li>
</ul>
<p><img src="rocksdb/./two-phase-commit-write-batch.svg" alt="two-phase-commit-write-batch" /></p>
<pre><code class="language-cpp">Status WriteBatchInternal::MarkEndPrepare(WriteBatch* b, const Slice&amp; xid,
                                          bool write_after_commit,
                                          bool unprepared_batch) {
  // other code..
  // rewrite noop as begin marker
  b-&gt;rep_[12] = static_cast&lt;char&gt;(
      write_after_commit ? kTypeBeginPrepareXID
                         : (unprepared_batch ? kTypeBeginUnprepareXID
                                             : kTypeBeginPersistedPrepareXID));
  b-&gt;rep_.push_back(static_cast&lt;char&gt;(kTypeEndPrepareXID));
  PutLengthPrefixedSlice(&amp;b-&gt;rep_, xid);
  // other code..
}
</code></pre>
<h4><a class="header" href="#recover-1" id="recover-1">Recover</a></h4>
<p>事务日志会以writeBatch为单位写入到WAL日志中，恢复时MemtableInsetor会去遍历日志中的writeBatch,
将<code>BeginPrepare</code>....<code>EndPrepare(xid)</code>之间的kv操作插入到新的writeBatch中，
在遍历到<code>Commit(xid)</code>时候，将该writeBatch插入到memtable中，完成提交。</p>
<p><img src="rocksdb/./two-phase-commit-recover.svg" alt="two phase commit recover" /></p>
<h2><a class="header" href="#write-prepared-txn" id="write-prepared-txn">Write prepared txn</a></h2>
<p>没有commit，就把数据insert到db中，有以下几个问题需要解决:</p>
<ul>
<li>How do we identify the key/values in the DB with transactions that wrote them?</li>
<li>How do we figure if a key/value written by transaction <code>Txn_w</code> is in the read snapshot of the reading transaction <code>Txn_r</code>?</li>
<li>How do we rollback the data written by aborted transactions?</li>
</ul>
<p>在prepare阶段就插入memtalbe中.</p>
<p>CommitCache 用于判断是否提交了</p>
<p><img src="rocksdb/./two-phase-commit-write-preparedtxn.svg" alt="write unprepared" /></p>
<h2><a class="header" href="#write-unprepared-txn" id="write-unprepared-txn">Write unprepared txn</a></h2>
<h2><a class="header" href="#todo-1" id="todo-1">TODO:</a></h2>
<ol>
<li>write prepared txn和write unprepared txn这个具体逻辑还不是很清楚，只知道是把commit放到了一个cache里面。</li>
</ol>
<h1><a class="header" href="#tikv" id="tikv">tikv</a></h1>
<h1><a class="header" href="#draft-2" id="draft-2">Draft</a></h1>
<h2><a class="header" href="#tikv-启动流程" id="tikv-启动流程">TiKV 启动流程</a></h2>
<p><img src="tikv/draft/./startup.svg" alt="startup" /></p>
<h3><a class="header" href="#main-启动过程" id="main-启动过程">Main 启动过程</a></h3>
<ol>
<li>启动grpc服务</li>
<li>启动snapworker, lockmgr, status server, stats pool等
backgroud线程</li>
</ol>
<p><img src="tikv/draft/./start-main.svg" alt="start-main" /></p>
<h3><a class="header" href="#get过程" id="get过程">Get过程</a></h3>
<p>Trait Snapshot 这个定义了get, iter相关read接口</p>
<pre><code class="language-rs">pub trait Snapshot: Send + Clone {
    type Iter: Iterator;

    fn get(&amp;self, key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt;;
    fn get_cf(&amp;self, cf: CfName, key: &amp;Key) -&gt; Result&lt;Option&lt;Value&gt;&gt;;
    fn iter(&amp;self, iter_opt: IterOptions, mode: ScanMode) -&gt; Result&lt;Cursor&lt;Self::Iter&gt;&gt;;
    fn iter_cf(
        &amp;self,
        cf: CfName,
        iter_opt: IterOptions,
        mode: ScanMode,
    ) -&gt; Result&lt;Cursor&lt;Self::Iter&gt;&gt;;
    // The minimum key this snapshot can retrieve.
    #[inline]
    fn lower_bound(&amp;self) -&gt; Option&lt;&amp;[u8]&gt; {
        None
    }
    // The maximum key can be fetched from the snapshot should less than the upper bound.
    #[inline]
    fn upper_bound(&amp;self) -&gt; Option&lt;&amp;[u8]&gt; {
        None
    }

    /// Retrieves a version that represents the modification status of the underlying data.
    /// Version should be changed when underlying data is changed.
    ///
    /// If the engine does not support data version, then `None` is returned.
    #[inline]
    fn get_data_version(&amp;self) -&gt; Option&lt;u64&gt; {
        None
    }
}

</code></pre>
<h4><a class="header" href="#snapshot的实现者impl" id="snapshot的实现者impl">snapshot的实现者impl</a></h4>
<p><img src="tikv/draft/./snapshot-impl.svg" alt="snapshot impl" /></p>
<p>RocksSnapshot</p>
<pre><code class="language-rs">impl Snapshot for RocksSnapshot {
//...
}
</code></pre>
<p>RegionSnapshot 在RocksSnapshot上包装了一层？增加了啥功能呀？</p>
<pre><code class="language-rs">impl Snapshot for RegionSnapshot&lt;RocksSnapshot&gt; {
//...
}
</code></pre>
<p>Btree放在里面是干啥用的，和rocksdb做对照？</p>
<pre><code class="language-rs">impl Snapshot for BTreeEngineSnapshot {
//...
}
</code></pre>
<p><img src="tikv/draft/./kv_get.svg" alt="tikv get" /></p>
<h1><a class="header" href="#raft" id="raft">raft</a></h1>
<h1><a class="header" href="#raft-1" id="raft-1">Raft</a></h1>
<h2><a class="header" href="#leader-election" id="leader-election">Leader Election</a></h2>
<p>在raft中，主要有leader, candidate, follower三种状态, 一个cluster只有一个leader, leader负责处理client的写请求，然后
leader将日志push给各个follower。</p>
<p>leader通过心跳机制告诉follower自己还活着，当follower有一段时间没收到leader的心跳后，认为leader已经挂掉后，就转变为candidate，
发起投票请求，尝试成为leader。</p>
<h3><a class="header" href="#term-任期" id="term-任期">term: 任期</a></h3>
<p>在Raft中，任期扮演着逻辑时钟的角色，节点之间的请求和返回中都带上node当前的term。node在处理请求时，发现请求中的term比自己大，就
将自己term 改为该值，如果比自己小，就拒绝请求，并返回带上自己term。 </p>
<p>leader发送给follower的心跳中，如果收到了term比自己大的回复，那么leader就知道自己stale了，就会step down.</p>
<p>candidate在发起requestforvote时候，会将自己term +=1 , 然后经过一轮处理后，整个集群term</p>
<h3><a class="header" href="#appendentries" id="appendentries">AppendEntries</a></h3>
<p>AppendEntries 是由leader发送给follower的RPC请求，一方面用于同步日志，另一方面AppendEntries的log entriy可以为空，扮演着心跳的角色，
而心跳用于抑制follower 转变为candidate。</p>
<h3><a class="header" href="#requestforvote" id="requestforvote">RequestForVote</a></h3>
<p>follower 变为candidate之后，会将自己term + 1, 并且会发送RequestForVote请求给所有成员，开始选举，如果收到了大部分成员的投票，则成为
新的任期的leader。</p>
<h3><a class="header" href="#splitvote" id="splitvote">SplitVote</a></h3>
<p>为了解决有多个candidate 同时发起投票，然后每个candidate获得的选票都达不到大多数的问题，Raft采用了 random election timeout的机制，每个
candidate的election timout是个随机值，可以在很大程度上保证一段时间内只有一个candidate在request for vote</p>
<p><img src="raft/./raft-server-state.svg" alt="raft server state" /></p>
<h2><a class="header" href="#log-replication" id="log-replication">Log Replication</a></h2>
<p>一条日志，只有被复制到cluster中大部分server上时候，才会被认为是commited。被commited日志才能apply 到raft的state machine上。
leader自己的日志只能append,不能rewrite，不然后面的commited index就没啥用了。</p>
<p>leaderr发送给follower的心跳请求中带了当前leaderCommited index， follower根据这个信息来判断一条日志能安全的apply 到statemachine上。</p>
<p>每条日志都有term和index，如果两条日志的term和index是一致的，那么这两条日志就被认为是一致的。
新leader当选后，需要向follower push自己的日志。leader需要找到和follower日志共同的起点，然后从该点同步follower日志。</p>
<p>Leader维护了一个NextIndex数组，NextIndex[i]表示下一次要向follower发送日志的index。</p>
<p><img src="raft/./raft-sub-problem.svg" alt="raft sub problem" /></p>
<h1><a class="header" href="#pingcap-talent-plan-raft-lab" id="pingcap-talent-plan-raft-lab">pingcap talent plan raft lab</a></h1>
<p><img src="raft/./pingcap-raft-lab.svg" alt="pingcap-raft-lab" /></p>
<h1><a class="header" href="#mit6-824" id="mit6-824">mit6-824</a></h1>
<h1><a class="header" href="#gfs" id="gfs">GFS</a></h1>
<h2><a class="header" href="#questions-3" id="questions-3">Questions</a></h2>
<ol>
<li>master 和chunk之间是怎么互相自动发现的？</li>
<li>master 和chunk之间心跳信息具体内容是啥</li>
<li>master信息存在哪儿？master挂了？集群都挂？</li>
<li>Cache怎么解决失效的问题？</li>
<li>谁负责写入多个副本？</li>
<li>副本的一致性是怎么保证的</li>
<li>Atomic Append是咋搞的</li>
<li>写入流程是怎样的？</li>
</ol>
<p><img src="mit6-824/./gfs-arch.svg" alt="gfs arch" /></p>
<h3><a class="header" href="#chunksize" id="chunksize">ChunkSize</a></h3>
<p>chunksize 64MB的好处</p>
<ol>
<li>减轻client 和master的通信.</li>
<li>client和chunk server长时间通信,减少需要和多个chunk server网络通信</li>
</ol>
<p>缺点：</p>
<ol>
<li>小文件只有一个或几个chunk，容易造成成为热点</li>
</ol>
<h3><a class="header" href="#metadata" id="metadata">Metadata</a></h3>
<p>master 相当于一个路由表, master主要存储三种metadata</p>
<ol>
<li>the file and chunk namespace</li>
<li>mapping from files to chunk</li>
<li>location of each chunk replica
这三个信息都存储在内存中，前两个信息会通过operation log, 持久化存储到磁盘上
信息3没有存在磁盘上，master询问每个chunk server, 来构建这个信息.</li>
</ol>
<h2><a class="header" href="#写入流程-1" id="写入流程-1">写入流程</a></h2>
<h3><a class="header" href="#lease-and-mutation-order" id="lease-and-mutation-order">lease and mutation order</a></h3>
<p><img src="mit6-824/./gfs-write.svg" alt="gfs write" /></p>
<p>如果修改的区域跨chunk了,上面的lease机制无法保证对多个chunk的修改，有一致的修改顺序。</p>
<h3><a class="header" href="#atomic-record-append" id="atomic-record-append">Atomic record append</a></h3>
<p>这块没怎么看明白，好像是append时候，如果primary发现chunk size不够写，就直接先将当前chunk pad填满，并且
让secondary也pad，填充, 然后让client重试,为了避免过多的碎片，chunk append的record size 现在在<code>maxSize/4</code>
这样就避免了跨chunk写</p>
<h3><a class="header" href="#snapshot" id="snapshot">snapshot</a></h3>
<p><img src="mit6-824/./gfs-snapshot.svg" alt="gfs snapshot" /></p>
<p><img src="mit6-824/./gfs-snapshot-cow.svg" alt="gfs snapshot cow" /></p>
<h2><a class="header" href="#master-operation" id="master-operation">Master operation</a></h2>
<ol>
<li>namespace namespace operation </li>
<li>manages chunk replicas</li>
<li>placement decisions</li>
</ol>
<p><img src="mit6-824/./gfs-master-operation.svg" alt="gfs master operation" /></p>
<h1><a class="header" href="#raft-2" id="raft-2">Raft</a></h1>
<h1><a class="header" href="#tidb" id="tidb">tidb</a></h1>
<h1><a class="header" href="#tidb-学习资料整理" id="tidb-学习资料整理">TiDB 学习资料整理</a></h1>
<h2><a class="header" href="#参考资料" id="参考资料">参考资料</a></h2>
<p>本文主要摘自pingcap 如下几篇blog, 从整体上介绍了tidb/tikv的设计架构，以及为什么要这么设计，为了解决什么问题。
看完后能对tidb有个整体的认识。</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/25142743">TiDB 架构的演进和开发哲学</a></li>
<li><a href="https://pingcap.com/blog-cn/10-questions-tidb-structure/">十问 TiDB ：关于架构设计的一些思考</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-internal-1/">三篇文章了解 TiDB 技术内幕 - 说存储</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-internal-3/">三篇文章了解 TiDB 技术内幕 - 谈调度</a></li>
</ol>
<h2><a class="header" href="#tidb-整体架构" id="tidb-整体架构">TiDb 整体架构</a></h2>
<p><img src="tidb/./tidb-arch-2.png" alt="tidb-arch2" /></p>
<p>TiDB包含三大核心组件，TiDB/TiKV/PD, 组件之间通过GRPC通信, 各自功能如下：<a href="https://pingcap.com/blog-cn/tidb-operator-introduction/">TiDB Operator，让 TiDB 成为真正的 Cloud-Native 数据库</a></p>
<ol>
<li>TiDB Server：主要负责 SQL 的解析器和优化器，它相当于计算执行层，同时也负责客户端接入和交互。</li>
<li>TiKV Server：是一套分布式的 Key-Value 存储引擎，它承担整个数据库的存储层，数据的水平扩展和多副本高可用特性都是在这一层实现。</li>
<li>PD Server：相当于分布式数据库的大脑，一方面负责收集和维护数据在各个 TiKV 节点的分布情况，另一方面 PD 承担调度器的角色，根据数据分布状况以及各个存储节点的负载来采取合适的调度策略，维持整个系统的平衡与稳定。</li>
</ol>
<p>TiDB/TiKV 背后对应的论文基础<a href="https://pingcap.com/blog-cn/how-do-we-build-tidb/">How do we build TiDB</a>, Google Spanner/F1, Raft.</p>
<p><img src="tidb/./tidb-arch-overview.png" alt="tidb-arc-overview" /></p>
<p><img src="tidb/./tikv-overview.png" alt="tikv-overview" /></p>
<h3><a class="header" href="#tidb-1" id="tidb-1">tidb</a></h3>
<p><img src="tidb/./tidb-arch.png" alt="tidb" /></p>
<p>tidb开发选择从上往下开发，无缝兼容MYSQL协议。talk is cheap, show me the test，使用了大量的测试用例来保证正确性。</p>
<p><img src="tidb/./tidb-sql.png" alt="tidb-sql" /></p>
<h3><a class="header" href="#关系模型到-key-value-模型的映射" id="关系模型到-key-value-模型的映射">关系模型到 Key-Value 模型的映射</a></h3>
<p><a href="https://pingcap.com/blog-cn/tidb-internal-2/#%E4%B8%89%E7%AF%87%E6%96%87%E7%AB%A0%E4%BA%86%E8%A7%A3-tidb-%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95---%E8%AF%B4%E8%AE%A1%E7%AE%97">三篇文章了解 TiDB 技术内幕 - 说计算</a></p>
<pre><code class="language-SQL">CREATE TABLE User {
	ID int,
	Name varchar(20),
	Role varchar(20),
	Age int,
	PRIMARY KEY (ID),
	Key idxAge (age)
};
</code></pre>
<p>每行数据按照如下规则进行编码成 Key-Value pair：</p>
<pre><code>Key: tablePrefix{tableID}_recordPrefixSep{rowID}
Value: [col1, col2, col3, col4]
</code></pre>
<p>其中 Key 的 tablePrefix/recordPrefixSep 都是特定的字符串常量，用于在 KV 空间内区分其他数据。
对于 Index 数据，会按照如下规则编码成 Key-Value pair：</p>
<pre><code>Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue
Value: rowID
</code></pre>
<p>注意上述编码规则中的 Key 里面的各种 xxPrefix 都是字符串常量，作用都是区分命名空间，以免不同类型的数据之间相互冲突，定义如下：</p>
<pre><code>var(
	tablePrefix     = []byte{'t'}
	recordPrefixSep = []byte(&quot;_r&quot;)
	indexPrefixSep  = []byte(&quot;_i&quot;)
)
</code></pre>
<h3><a class="header" href="#tikv-1" id="tikv-1">tikv</a></h3>
<p><img src="tidb/./tidb-tikv.png" alt="tidb-tikv" /></p>
<p>TiKV 利用 Raft 来做数据复制，每个数据变更都会落地为一条 Raft 日志，通过 Raft 的日志复制功能，将数据安全可靠地同步到 Group 的多数节点中。
通过单机的 RocksDB，我们可以将数据快速地存储在磁盘上；通过 Raft，我们可以将数据复制到多台机器上，以防单机失效。数据的写入是通过 Raft 这一层的接口写入，而不是直接写 RocksDB。通过实现 Raft，我们拥有了一个分布式的 KV，现在再也不用担心某台机器挂掉了。</p>
<p><img src="tidb/./tikv-raft.png" alt="tikv-raft" /></p>
<p><img src="tidb/./tikv-region.png" alt="tikv-region" /></p>
<p>MVCC</p>
<p>很多数据库都会实现多版本控制（MVCC），TiKV 也不例外。设想这样的场景，两个 Client 同时去修改一个 Key 的 Value，如果没有 MVCC，就需要对数据上锁，在分布式场景下，可能会带来性能以及死锁问题。 TiKV 的 MVCC 实现是通过在 Key 后面添加 Version 来实现，简单来说，没有 MVCC 之前，可以把 TiKV 看做这样的：</p>
<h3><a class="header" href="#pd" id="pd">pd</a></h3>
<p>下面问题值得仔细思考。</p>
<ol>
<li>如何保证同一个 Region 的多个 Replica 分布在不同的节点上？更进一步，如果在一台机器上启动多个 TiKV 实例，会有什么问题？</li>
<li>TiKV 集群进行跨机房部署用于容灾的时候，如何保证一个机房掉线，不会丢失 Raft Group 的多个 Replica？</li>
<li>添加一个节点进入 TiKV 集群之后，如何将集群中其他节点上的数据搬过来?</li>
<li>当一个节点掉线时，会出现什么问题？整个集群需要做什么事情？如果节点只是短暂掉线（重启服务），那么如何处理？如果节点是长时间掉线（磁盘故障，数据全部丢失），需要如何处理？</li>
<li>假设集群需要每个 Raft Group 有 N 个副本，那么对于单个 Raft Group 来说，Replica 数量可能会不够多（例如节点掉线，失去副本），也可能会 过于多（例如掉线的节点又回复正常，自动加入集群）。那么如何调节 Replica 个数？</li>
<li>读/写都是通过 Leader 进行，如果 Leader 只集中在少量节点上，会对集群有什么影响？</li>
<li>并不是所有的 Region 都被频繁的访问，可能访问热点只在少数几个 Region，这个时候我们需要做什么？</li>
<li>集群在做负载均衡的时候，往往需要搬迁数据，这种数据的迁移会不会占用大量的网络带宽、磁盘 IO 以及 CPU？进而影响在线服务？</li>
</ol>
<p>作为一个分布式高可用存储系统，必须满足的需求，包括四种：</p>
<ol>
<li>副本数量不能多也不能少</li>
<li>副本需要分布在不同的机器上</li>
<li>新加节点后，可以将其他节点上的副本迁移过来</li>
<li>节点下线后，需要将该节点的数据迁移走</li>
</ol>
<p>作为一个良好的分布式系统，需要优化的地方，包括：</p>
<ol>
<li>维持整个集群的 Leader 分布均匀</li>
<li>维持每个节点的储存容量均匀</li>
<li>维持访问热点分布均匀</li>
<li>控制 Balance 的速度，避免影响在线服务</li>
<li>管理节点状态，包括手动上线/下线节点，以及自动下线失效节点</li>
</ol>
<h1><a class="header" href="#tidb-server-main-loop" id="tidb-server-main-loop">TiDB Server Main Loop</a></h1>
<p>跟着官方的tidb源码阅读博客，看了TiDB main函数，大致了解了一个SQL的处理过程</p>
<h2><a class="header" href="#conn-accept" id="conn-accept">conn accept</a></h2>
<p>下图显示了TiDB中Accept一个mysql连接的处理流程，对于每个新的conn, TiDB会启动一个goroutine来处理这个conn, 并按照Mysql协议，处理不同的mysql cmd。</p>
<p>对于Query语句，会session.Execute生成一个执行器，返回一个resultSet, 最后调用<code>writeResultset</code>, 从ResultSet.Next中获取结果，然后将结果返回给客户端。</p>
<p><img src="tidb/./tidb-server-main.svg" alt="tidb server main" /></p>
<h3><a class="header" href="#处理conn-loop" id="处理conn-loop">处理conn loop</a></h3>
<pre><code class="language-go">// Run reads client query and writes query result to client in for loop, if there is a panic during query handling,
// it will be recovered and log the panic error.
// This function returns and the connection is closed if there is an IO error or there is a panic.
func (cc *clientConn) Run(ctx context.Context) {
//other code..
for {
    // other code ...
		data, err := cc.readPacket()

    // other code ...
		if err = cc.dispatch(ctx, data); err != nil {
      // other code ...
    }
    // other code ...
}
}
</code></pre>
<h3><a class="header" href="#cmd-dispatch" id="cmd-dispatch">cmd dispatch</a></h3>
<pre><code class="language-go">// dispatch handles client request based on command which is the first byte of the data.
// It also gets a token from server which is used to limit the concurrently handling clients.
// The most frequently used command is ComQuery.
func (cc *clientConn) dispatch(ctx context.Context, data []byte) error {
//other code ...
	cmd := data[0]
	data = data[1:]
//other code ...
	dataStr := string(hack.String(data))

	switch cmd {
	case mysql.ComQuery: // Most frequently used command.
		if len(data) &gt; 0 &amp;&amp; data[len(data)-1] == 0 {
			data = data[:len(data)-1]
			dataStr = string(hack.String(data))
		}
		return cc.handleQuery(ctx, dataStr)
    //other case ...
  }
}
</code></pre>
<h2><a class="header" href="#sql-execute" id="sql-execute">SQL Execute</a></h2>
<p>TiDB中SQL执行过程如下</p>
<p><img src="tidb/./sql-to-resultset.svg" alt="sql-to-resultset" /></p>
<h3><a class="header" href="#sql-plan-optimize-制定查询计划以及优化" id="sql-plan-optimize-制定查询计划以及优化">SQL Plan Optimize: 制定查询计划以及优化</a></h3>
<p><img src="tidb/./sql-plan.svg" alt="sql-plan" /></p>
<h3><a class="header" href="#sql-build-executor" id="sql-build-executor">SQL build executor</a></h3>
<p>根据plan生成相应的executor</p>
<p><img src="tidb/./sql-executor.svg" alt="sql-executor" /></p>
<p>Executor interface如下, 使用了Volcano模型，接口用起来和迭代器差不多，采用Open-Next-Close套路来使用。</p>
<pre><code class="language-go">// Executor is the physical implementation of a algebra operator.
//
// In TiDB, all algebra operators are implemented as iterators, i.e., they
// support a simple Open-Next-Close protocol. See this paper for more details:
//
// &quot;Volcano-An Extensible and Parallel Query Evaluation System&quot;
//
// Different from Volcano's execution model, a &quot;Next&quot; function call in TiDB will
// return a batch of rows, other than a single row in Volcano.
// NOTE: Executors must call &quot;chk.Reset()&quot; before appending their results to it.
type Executor interface {
	base() *baseExecutor
	Open(context.Context) error
	Next(ctx context.Context, req *chunk.Chunk) error
	Close() error
	Schema() *expression.Schema
}
</code></pre>
<h3><a class="header" href="#executor-next" id="executor-next">executor Next</a></h3>
<h4><a class="header" href="#handlenodelay" id="handlenodelay">handleNoDelay</a></h4>
<p>不需要返回结果的立即执行</p>
<p><img src="tidb/./sql-nodelay-next.svg" alt="sql-nodelay-next" /></p>
<h4><a class="header" href="#recordset-driver" id="recordset-driver">RecordSet driver</a></h4>
<p><img src="tidb/./sql-recordset-driver.svg" alt="sql-recordset-driver" /></p>
<pre><code class="language-go">// RecordSet is an abstract result set interface to help get data from Plan.
type RecordSet interface {
	// Fields gets result fields.
	Fields() []*ast.ResultField

	// Next reads records into chunk.
	Next(ctx context.Context, req *chunk.Chunk) error

	// NewChunk create a chunk.
	NewChunk() *chunk.Chunk

	// Close closes the underlying iterator, call Next after Close will
	// restart the iteration.
	Close() error
}
</code></pre>
<p>RecordSet Next方法接口的实现.</p>
<pre><code class="language-go">// Next use uses recordSet's executor to get next available chunk for later usage.
// If chunk does not contain any rows, then we update last query found rows in session variable as current found rows.
// The reason we need update is that chunk with 0 rows indicating we already finished current query, we need prepare for
// next query.
// If stmt is not nil and chunk with some rows inside, we simply update last query found rows by the number of row in chunk.
func (a *recordSet) Next(ctx context.Context, req *chunk.Chunk) error {
	err := Next(ctx, a.executor, req)
	if err != nil {
		a.lastErr = err
		return err
	}
	numRows := req.NumRows()
	if numRows == 0 {
		if a.stmt != nil {
			a.stmt.Ctx.GetSessionVars().LastFoundRows = a.stmt.Ctx.GetSessionVars().StmtCtx.FoundRows()
		}
		return nil
	}
	if a.stmt != nil {
		a.stmt.Ctx.GetSessionVars().StmtCtx.AddFoundRows(uint64(numRows))
	}
	return nil
}
</code></pre>
<p>在writeResult时候不断调用RecordSet的Next方法，去驱动调用executor的Next;</p>
<pre><code class="language-go">// writeChunks writes data from a Chunk, which filled data by a ResultSet, into a connection.
// binary specifies the way to dump data. It throws any error while dumping data.
// serverStatus, a flag bit represents server information
func (cc *clientConn) writeChunks(ctx context.Context, rs ResultSet, binary bool, serverStatus uint16) error {
	data := cc.alloc.AllocWithLen(4, 1024)
	req := rs.NewChunk()
  //...
	for {
		// Here server.tidbResultSet implements Next method.
		err := rs.Next(ctx, req)
    /...
		rowCount := req.NumRows()
    //...
		for i := 0; i &lt; rowCount; i++ {
			data = data[0:4]
      /...
			if err = cc.writePacket(data); err != nil {
				return err
			}
      //...
    }
  }
	return cc.writeEOF(serverStatus)
}
</code></pre>
<h1><a class="header" href="#schema" id="schema">schema</a></h1>
<p><img src="tidb/./schema-kv.svg" alt="schema-kv" /></p>
<h1><a class="header" href="#plan" id="plan">Plan</a></h1>
<p><img src="tidb/./plan-struct.svg" alt="plan-struct" /></p>
<p><img src="tidb/./plan-builder.svg" alt="plan-builder" /></p>
<p><img src="tidb/./plan-tree.svg" alt="plan-tree" /></p>
<p><img src="tidb/./physical-join.svg" alt="physical-join" /></p>
<h2><a class="header" href="#hash-join-executor" id="hash-join-executor">hash join executor</a></h2>
<p><img src="tidb/./hash-join-executor.svg" alt="hashjoin-executor" /></p>
<h1><a class="header" href="#hash-join" id="hash-join">hash join</a></h1>
<p><img src="tidb/./Hash-Match-Join-Looping-1.gif" alt="hash-join-loop" /></p>
<p><img src="tidb/./hash-join-executor.svg" alt="hash-join-executor" /></p>
<h1><a class="header" href="#tidb-schema" id="tidb-schema">TiDB Schema</a></h1>
<h2><a class="header" href="#schema-作用" id="schema-作用">schema 作用</a></h2>
<p><img src="tidb/./schema-curd-to-kv.svg" alt="schema-curd-to-kv" /></p>
<h3><a class="header" href="#tidb中的映射" id="tidb中的映射">TiDB中的映射</a></h3>
<pre><code class="language-sql">CREATE TABLE User {
	ID int,
	Name varchar(20),
	Role varchar(20),
	Age int,
	PRIMARY KEY (ID),
	Key idxAge (age)
};
</code></pre>
<p>映射为如下的kv</p>
<pre><code>Key: tablePrefix{tableID}_recordPrefixSep{rowID}
Value: [col1, col2, col3, col4]
</code></pre>
<p>对于index数据映射如下:</p>
<pre><code>Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue
Value: rowID
</code></pre>
<p>unique index 数据映射如下:</p>
<pre><code>Key: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue_rowID
Value: null
</code></pre>
<p>涉及模块说明<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-2/">tidb源码概览</a></p>
<ol>
<li>infoschema:  SQL 元信息管理模块，另外对于 Information Schema 的操作，都会访问这里</li>
<li>meta:  利用 structure 包提供的功能，管理存储引擎中存储的 SQL 元信息，infoschema/DDL 利用这个模块访问或者修改 SQL 元信息</li>
<li>model: SQL 元信息数据结构，包括 DBInfo / TableInfo / ColumnInfo / IndexInfo 等</li>
<li>structure: 在 Transactional KV API 上定义的一层结构化 API，提供 List/Queue/HashMap 等结构</li>
</ol>
<h3><a class="header" href="#shema-数据结构" id="shema-数据结构">Shema 数据结构</a></h3>
<p>TiDB 使用Schema来将关系数据库中的table/index等映射到TiKV的kv存储中。 Schema本身也是以kv的形式保存在TiKV中的。
TiDB是无状态的，而且在TiDB内存中也加载这一份Schema。所以存在schema异步更新的问题。</p>
<p><img src="tidb/./model.svg" alt="model" /></p>
<h2><a class="header" href="#schema-存储加载修改" id="schema-存储加载修改">Schema 存储/加载/修改</a></h2>
<p>Schema在kv中的存储形式如下</p>
<pre><code class="language-go">// Meta structure:
//	NextGlobalID -&gt; int64
//	SchemaVersion -&gt; int64
//	DBs -&gt; {
//		DB:1 -&gt; db meta data []byte
//		DB:2 -&gt; db meta data []byte
//	}
//	DB:1 -&gt; {
//		Table:1 -&gt; table meta data []byte
//		Table:2 -&gt; table meta data []byte
//		TID:1 -&gt; int64
//		TID:2 -&gt; int64
//	}
//
</code></pre>
<p><img src="tidb/schema-save.svg" alt="schema-save" /></p>
<h3><a class="header" href="#schema-加载" id="schema-加载">schema 加载</a></h3>
<p><img src="tidb/./schema-load.svg" alt="schema-load" /></p>
<h2><a class="header" href="#schema-修改" id="schema-修改">schema 修改</a></h2>
<p>DDL（Data Definition Language): statements are used to define the database structure or schema.
DDL statements create, modify, and remove database objects such as tables, indexes, and users. </p>
<p>创建/修改/删除 schema逻辑如下:</p>
<p><img src="tidb/./ddl-schema-flow.svg" alt="ddl-schema-flow" /></p>
<h3><a class="header" href="#schema-state" id="schema-state">Schema state</a></h3>
<p>online schema change</p>
<h3><a class="header" href="#ddl-job" id="ddl-job">DDL Job</a></h3>
<p>TiDB 在同一时刻，只允许一个节点执行 DDL 操作。用户可以把多个 DDL 请求发给任何 TiDB 节点，但是所有的 DDL 请求在 TiDB 内部是由 owner 节点的 worker 串行执行的。</p>
<ul>
<li>worker：每个节点都有一个 worker 用来处理 DDL 操作。</li>
<li>owner：整个集群中只有一个节点能当选 owner，每个节点都可能当选这个角色。当选 owner 后的节点 worker 才有处理 DDL 操作的权利。owner 节点的产生是用 Etcd 的选举功能从多个 TiDB 节点选举出 owner 节点。owner 是有任期的，owner 会主动维护自己的任期，即续约。当 owner 节点宕机后，其他节点可以通过 Etcd 感知到并且选举出新的 owner。</li>
</ul>
<p>以上内容摘自<a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">4</a></p>
<h4><a class="header" href="#jobqueue" id="jobqueue">JobQueue</a></h4>
<h4><a class="header" href="#owner-election" id="owner-election">Owner election</a></h4>
<ol>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a></li>
<li><a href="https://github.com/ngaut/builddatabase/blob/master/f1/schema-change-implement.md">TiDB 的异步 schema 变更实现</a></li>
<li><a href="https://github.com/ngaut/builddatabase/blob/master/f1/schema-change.md">异步 schema 变更</a></li>
<li><a href="https://pingcap.com/blog-cn/tidb-source-code-reading-17/">TiDB 源码阅读系列文章（十七）DDL 源码解析</a></li>
</ol>
<h1><a class="header" href="#table" id="table">Table</a></h1>
<h2><a class="header" href="#table-interface" id="table-interface">table interface</a></h2>
<p><img src="tidb/./table.svg" alt="table" /></p>
<h2><a class="header" href="#tableid的分配" id="tableid的分配">TableID的分配</a></h2>
<p>tableID，PhyicalID</p>
<h2><a class="header" href="#从table-到kv" id="从table-到kv">从table 到kv</a></h2>
<pre><code class="language-go">var (
	tablePrefix     = []byte{'t'}
	recordPrefixSep = []byte(&quot;_r&quot;)
	indexPrefixSep  = []byte(&quot;_i&quot;)
)
</code></pre>
<pre><code class="language-go">// GenTableRecordPrefix composes record prefix with tableID: &quot;t[tableID]_r&quot;.
func GenTableRecordPrefix(tableID int64) kv.Key {
	buf := make([]byte, 0, len(tablePrefix)+8+len(recordPrefixSep))
	return appendTableRecordPrefix(buf, tableID)
}
</code></pre>
<pre><code class="language-go">// GenTableIndexPrefix composes index prefix with tableID: &quot;t[tableID]_i&quot;.
func GenTableIndexPrefix(tableID int64) kv.Key {
	buf := make([]byte, 0, len(tablePrefix)+8+len(indexPrefixSep))
	return appendTableIndexPrefix(buf, tableID)
}
</code></pre>
<pre><code class="language-go">
// GenTablePrefix composes table record and index prefix: &quot;t[tableID]&quot;.
func GenTablePrefix(tableID int64) kv.Key {
	buf := make([]byte, 0, len(tablePrefix)+8)
	buf = append(buf, tablePrefix...)
	buf = codec.EncodeInt(buf, tableID)
	return buf
}
</code></pre>
<h2><a class="header" href="#table的增删改查" id="table的增删改查">table的增删改查</a></h2>
<h3><a class="header" href="#addrecord" id="addrecord">AddRecord</a></h3>
<p><img src="tidb/./table_addRecord.svg" alt="table-AddRecord" /></p>
<h3><a class="header" href="#updaterecord" id="updaterecord">UpdateRecord</a></h3>
<p><img src="tidb/./table_updateRecord.svg" alt="table-updateRecord" /></p>
<h3><a class="header" href="#removerecord" id="removerecord">RemoveRecord</a></h3>
<p><img src="tidb/./table_removeRecord.svg" alt="table-RemoveRecord" /></p>
<h3><a class="header" href="#iterrecords" id="iterrecords">IterRecords</a></h3>
<p><img src="tidb/./table_iterRecords.svg" alt="table-IterRecords" /></p>
<h1><a class="header" href="#clickhouse" id="clickhouse">ClickHouse</a></h1>
<h1><a class="header" href="#server-main" id="server-main">Server Main</a></h1>
<ul>
<li><a href="clickhouse/server-main.html#server-main-%E4%B8%BB%E6%B5%81%E7%A8%8B">Server main 主流程</a>
<ul>
<li><a href="clickhouse/server-main.html#%E4%B8%BB%E5%BE%AA%E7%8E%AF">主循环</a></li>
<li><a href="clickhouse/server-main.html#sql-%E8%A7%A3%E6%9E%90%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B">SQL 解析执行流程</a>
<ul>
<li><a href="clickhouse/server-main.html#parse-sql">Parse SQL</a></li>
<li><a href="clickhouse/server-main.html#selectquery">SelectQuery</a></li>
<li><a href="clickhouse/server-main.html#queryplanstep">QueryPlanStep</a></li>
<li><a href="clickhouse/server-main.html#iprocessor">IProcessor</a></li>
<li><a href="clickhouse/server-main.html#iprocessor-%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%9B%BE">IProcessor 继承关系图</a></li>
</ul>
</li>
<li><a href="clickhouse/server-main.html#executor-%E6%89%A7%E8%A1%8Cpipeline">Executor: 执行pipeline</a>
<ul>
<li><a href="clickhouse/server-main.html#pipelineexecutor">PipelineExecutor</a></li>
<li><a href="clickhouse/server-main.html#pullingpipelineexecutor">PullingPipelineExecutor</a></li>
<li><a href="clickhouse/server-main.html#pullingasyncpipelineexecutor">PullingAsyncPipelineExecutor</a></li>
</ul>
</li>
<li><a href="clickhouse/server-main.html#iblockinputstream">IBlockInputStream</a>
<ul>
<li><a href="clickhouse/server-main.html#pipelineexecutingblockinputstream">PipelineExecutingBlockInputStream</a></li>
<li><a href="clickhouse/server-main.html#asynchronousblockinputstream">AsynchronousBlockInputStream</a></li>
</ul>
</li>
<li><a href="clickhouse/server-main.html#blockio">BlockIO</a></li>
</ul>
</li>
<li><a href="clickhouse/server-main.html#ref">Ref</a></li>
</ul>
<h2><a class="header" href="#server-main-主流程" id="server-main-主流程">Server main 主流程</a></h2>
<h3><a class="header" href="#主循环" id="主循环">主循环</a></h3>
<p>首先监听端口号，等待客户端连接， 
和客户端建立连接后,server然后不断从conn中读取packet, 
解析sql语句为AST树，然后创建plan pipeline
最后执行plan，将result set通过网络发送给客户端.</p>
<p><img src="clickhouse/./dot/server-main.svg" alt="server-main" /></p>
<h3><a class="header" href="#sql-解析执行流程" id="sql-解析执行流程">SQL 解析执行流程</a></h3>
<p>一条Query SQL在clickhouse中执行流程如下:</p>
<p><img src="clickhouse/./dot/execute-flow.svg" alt="execute-flow" /></p>
<h4><a class="header" href="#parse-sql" id="parse-sql">Parse SQL</a></h4>
<p>解析SQL，解析为AST树，然后创建对应的pipeline plan.</p>
<p><img src="clickhouse/./dot/execute-query.svg" alt="execute-query" /></p>
<h4><a class="header" href="#selectquery" id="selectquery">SelectQuery</a></h4>
<p>执行Select Query , 创建QueryPlan</p>
<p><img src="clickhouse/./dot/select-query.svg" alt="select-query" /></p>
<h4><a class="header" href="#queryplanstep" id="queryplanstep">QueryPlanStep</a></h4>
<p><img src="clickhouse/./dot/query-plan-step.svg" alt="query-plan-step" /></p>
<h4><a class="header" href="#iprocessor" id="iprocessor">IProcessor</a></h4>
<p>Processor is an element (low level building block) of a query execution pipeline.
It has zero or more input ports and zero or more output ports.</p>
<p>Blocks of data are transferred over ports.
Each port has fixed structure: names and types of columns and values of constants.</p>
<pre><code>src/Processors/IProcessor.h
</code></pre>
<h4><a class="header" href="#iprocessor-继承关系图" id="iprocessor-继承关系图">IProcessor 继承关系图</a></h4>
<p>CK中Iprocessor的继承关系图</p>
<pre><code class="language-cpp">class IProcessor
{
protected:
    InputPorts inputs;
    OutputPorts outputs;
}
</code></pre>
<p><img src="clickhouse/./dot/iprocessor.svg" alt="iprocessor" /></p>
<p><img src="clickhouse/./dot/transform.svg" alt="transform" /></p>
<h3><a class="header" href="#executor-执行pipeline" id="executor-执行pipeline">Executor: 执行pipeline</a></h3>
<h4><a class="header" href="#pipelineexecutor" id="pipelineexecutor">PipelineExecutor</a></h4>
<p>使用线程池执行pipline</p>
<p><img src="clickhouse/./dot/pipeline-executor.svg" alt="pipeline-executor" /></p>
<h4><a class="header" href="#pullingpipelineexecutor" id="pullingpipelineexecutor">PullingPipelineExecutor</a></h4>
<p>单线程同步执行？</p>
<pre><code class="language-cpp">/// Pulling executor for QueryPipeline. Always execute pipeline in single thread.
/// Typical usage is:
///
/// PullingPipelineExecutor executor(query_pipeline);
/// while (executor.pull(chunk))
///     ... process chunk ...
</code></pre>
<p><img src="clickhouse/./dot/pulling-pipeline-executor.svg" alt="pulling-pipeline-executor" /></p>
<h4><a class="header" href="#pullingasyncpipelineexecutor" id="pullingasyncpipelineexecutor">PullingAsyncPipelineExecutor</a></h4>
<p>多线程异步执行</p>
<pre><code class="language-cpp">/// Asynchronous pulling executor for QueryPipeline.
/// Always creates extra thread. If query is executed in single thread, use PullingPipelineExecutor.
/// Typical usage is:
///
/// PullingAsyncPipelineExecutor executor(query_pipeline);
/// while (executor.pull(chunk, timeout))
///     ... process chunk ...
</code></pre>
<p><img src="clickhouse/./dot/pulling-async-pipeline-executor.svg" alt="pulling-async-pipeline-executor" /></p>
<h3><a class="header" href="#iblockinputstream" id="iblockinputstream">IBlockInputStream</a></h3>
<h4><a class="header" href="#pipelineexecutingblockinputstream" id="pipelineexecutingblockinputstream">PipelineExecutingBlockInputStream</a></h4>
<p>封装了PullingPipelineExecutor和PullingAsyncPipelineExecutor, 实现了IBlockInputStream接口</p>
<p><img src="clickhouse/./dot/pipeline-executing-block-input-stream.svg" alt="pipeline-executing-block-input-stream" /></p>
<h4><a class="header" href="#asynchronousblockinputstream" id="asynchronousblockinputstream">AsynchronousBlockInputStream</a></h4>
<p>在另外一个线程中执行inner BlockInputStream</p>
<pre><code class="language-cpp">/** Executes another BlockInputStream in a separate thread.
  * This serves two purposes:
  * 1. Allows you to make the different stages of the query execution pipeline work in parallel.
  * 2. Allows you not to wait until the data is ready, and periodically check their readiness without blocking.
  *    This is necessary, for example, so that during the waiting period you can check if a packet
  *     has come over the network with a request to interrupt the execution of the query.
  *    It also allows you to execute multiple queries at the same time.
  */
</code></pre>
<p><img src="clickhouse/./dot/asynchronous-block-inputstream.svg" alt="asynchronous-block-inputstream" /></p>
<h3><a class="header" href="#blockio" id="blockio">BlockIO</a></h3>
<p>block-io getInputStream，读数据时执行plan</p>
<p><img src="clickhouse/./dot/block-io.svg" alt="block-io" /></p>
<h2><a class="header" href="#ref-11" id="ref-11">Ref</a></h2>
<ol>
<li><a href="https://cloud.tencent.com/developer/article/1602664">Clickhouse源码导读: 网络IO</a></li>
<li><a href="http://sineyuan.github.io/post/clickhouse-source-guide/">Clickhouse源码导读</a></li>
</ol>
<h1><a class="header" href="#block" id="block">Block</a></h1>
<ul>
<li><a href="clickhouse/block.html#block">Block</a></li>
<li><a href="clickhouse/block.html#blockinfo">BlockInfo</a></li>
<li><a href="clickhouse/block.html#icolumn">IColumn</a></li>
<li><a href="clickhouse/block.html#idatatype">IDataType</a></li>
</ul>
<h2><a class="header" href="#block-1" id="block-1">Block</a></h2>
<blockquote>
<p>A Block is a container that represents a subset (chunk) of a table in memory. It is just a set of triples: (IColumn, IDataType, column name). During query execution, data is processed by Blocks. If we have a Block, we have data (in the IColumn object), we have information about its type (in IDataType) that tells us how to deal with that column, and we have the column name. It could be either the original column name from the table or some artificial name assigned for getting temporary results of calculations.</p>
</blockquote>
<p>最基本的数据处理单元, 有点类似于Pandas的dataframe, 对应的基本操作有insert/erase</p>
<pre><code class="language-cpp">/** Container for set of columns for bunch of rows in memory.
  * This is unit of data processing.
  * Also contains metadata - data types of columns and their names
  *  (either original names from a table, or generated names during temporary calculations).
  * Allows to insert, remove columns in arbitrary position, to change order of columns.
  */
</code></pre>
<p><img src="clickhouse/./dot/block.svg" alt="block" /></p>
<h2><a class="header" href="#blockinfo" id="blockinfo">BlockInfo</a></h2>
<pre><code class="language-cpp">    /** is_overflows:
      * After running GROUP BY ... WITH TOTALS with the max_rows_to_group_by and group_by_overflow_mode = 'any' settings,
      *  a row is inserted in the separate block with aggregated values that have not passed max_rows_to_group_by.
      * If it is such a block, then is_overflows is set to true for it.
      */

    /** bucket_num:
      * When using the two-level aggregation method, data with different key groups are scattered across different buckets.
      * In this case, the bucket number is indicated here. It is used to optimize the merge for distributed aggregation.
      * Otherwise -1.
      */
</code></pre>
<h2><a class="header" href="#icolumn" id="icolumn">IColumn</a></h2>
<p>Cow: Copy on write shared Ptr</p>
<p>ICoumn存储数据</p>
<p>icolumn和idatatype 比较类似？他们两者分别负责什么功能?</p>
<p><img src="clickhouse/./dot/icolumn.svg" alt="iclolumn" /></p>
<h2><a class="header" href="#idatatype" id="idatatype">IDataType</a></h2>
<p>数据的序列化和反序列化</p>
<p><img src="clickhouse/./dot/idatatype.svg" alt="idatatype" /></p>
<h1><a class="header" href="#blockio-1" id="blockio-1">BlockIO</a></h1>
<p>Block的输入输出, 主要有BlockInputStream 和
BlockOutputStream, 输入输出的基本单位为Block</p>
<p>getHeader header的作用是啥？表明data的schema吗?</p>
<p><img src="clickhouse/./dot/blockio.svg" alt="blockio" /></p>
<h2><a class="header" href="#iblockinputstream-1" id="iblockinputstream-1">IBlockInputStream</a></h2>
<blockquote>
<p>The stream interface for reading data by blocks from the database.
Relational operations are supposed to be done also as implementations of this interface.
Watches out at how the source of the blocks works.
Lets you get information for profiling: rows per second, blocks per second, megabytes per second, etc.
Allows you to stop reading data (in nested sources).</p>
</blockquote>
<p>IBlockInputStream 主要接口 read, readPrefix, readSuffix</p>
<p>这个地方的limit, quta, 以及info之类的作用是什么?</p>
<p><img src="clickhouse/./dot/iblock-inputstream-func.svg" alt="iblock-inputstream-func" /></p>
<p>IBlockInputStream 继承关系</p>
<p><img src="clickhouse/./dot/iblockinputstream.svg" alt="iblockinputstream" /></p>
<p>TODO:</p>
<ol>
<li>TypePromotion 模板</li>
<li>Cow 模板</li>
</ol>
<h2><a class="header" href="#iblockoutputstream" id="iblockoutputstream">IBlockOutputStream</a></h2>
<blockquote>
<p>Interface of stream for writing data (into table, filesystem, network, terminal, etc.)</p>
</blockquote>
<p><img src="clickhouse/./dot/iblockoutputstream.svg" alt="iblockoutputstream" /></p>
<h1><a class="header" href="#godot" id="godot">Godot</a></h1>
<h1><a class="header" href="#godot-学习笔记" id="godot-学习笔记">godot 学习笔记</a></h1>
<ul>
<li><a href="godot/learning-note.html#node-tree">node tree</a></li>
<li><a href="godot/learning-note.html#node2d">Node2D</a></li>
<li><a href="godot/learning-note.html#node-%E8%99%9A%E5%87%BD%E6%95%B0">Node 虚函数</a></li>
<li><a href="godot/learning-note.html#instance-scene">Instance Scene</a></li>
<li><a href="godot/learning-note.html#signal">Signal</a>
<ul>
<li><a href="godot/learning-note.html#connect-signal">Connect signal</a></li>
<li><a href="godot/learning-note.html#emit-signal">Emit signal</a></li>
<li><a href="godot/learning-note.html#animatedsprite">AnimatedSprite</a></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#node-tree" id="node-tree">node tree</a></h2>
<ol>
<li>在tree中怎么快速定位到某个Node? 并转换为相应类型？</li>
<li>node之间怎么互相调用？</li>
<li>scene之间的过渡场景怎么搞？</li>
<li>目前有哪些node 各自负责干啥？</li>
</ol>
<p><img src="godot/./node-tree.svg" alt="node" /></p>
<h2><a class="header" href="#node2d" id="node2d">Node2D</a></h2>
<p><img src="godot/./node-type.svg" alt="node type" /></p>
<h2><a class="header" href="#node-虚函数" id="node-虚函数">Node 虚函数</a></h2>
<p>Rust中没有虚函数，是咋搞的</p>
<pre><code class="language-c#">public override void _EnterTree()
{
    // When the node enters the Scene Tree, it becomes active
    // and  this function is called. Children nodes have not entered
    // the active scene yet. In general, it's better to use _ready()
    // for most cases.
    base._EnterTree();
}

public override void _Ready()
{
    // This function is called after _enter_tree, but it ensures
    // that all children nodes have also entered the Scene Tree,
    // and became active.
    base._Ready();
}

public override void _ExitTree()
{
    // When the node exits the Scene Tree, this function is called.
    // Children nodes have all exited the Scene Tree at this point
    // and all became inactive.
    base._ExitTree();
}

public override void _Process(float delta)
{
    // This function is called every frame.
    base._Process(delta);
}

public override void _PhysicsProcess(float delta)
{
    // This is called every physics frame.
    base._PhysicsProcess(delta);
}
</code></pre>
<p><img src="godot/./node-callback.svg" alt="node callback" /></p>
<h2><a class="header" href="#instance-scene" id="instance-scene">Instance Scene</a></h2>
<p>先load scene, 然后将scene instance为node，可以放在场景里面</p>
<pre><code>var scene = GD.Load&lt;PackedScene&gt;(&quot;res://myscene.tscn&quot;); // Will load when the script is instanced.

//preload
var scene = preload(&quot;res://myscene.tscn&quot;) # Will load when parsing the script.

//instance
var node = scene.Instance();
AddChild(node);
</code></pre>
<h2><a class="header" href="#signal" id="signal">Signal</a></h2>
<p>可以在editor中connect. 也可以在代码中connect 信号和handler </p>
<p>带参数的Signal</p>
<pre><code>extends Node

signal my_signal(value, other_value)

func _ready():
    emit_signal(&quot;my_signal&quot;, true, 42)
</code></pre>
<h3><a class="header" href="#connect-signal" id="connect-signal">Connect signal</a></h3>
<pre><code>// &lt;source_node&gt;.connect(&lt;signal_name&gt;, &lt;target_node&gt;, &lt;target_function_name&gt;)
extends Node2D


func _ready():
    $Timer.connect(&quot;timeout&quot;, self, &quot;_on_Timer_timeout&quot;)


func _on_Timer_timeout():
    $Sprite.visible = !$Sprite.visible
</code></pre>
<h3><a class="header" href="#emit-signal" id="emit-signal">Emit signal</a></h3>
<p>定义和发射signal</p>
<pre><code>extends Node2D


signal my_signal


func _ready():
    emit_signal(&quot;my_signal&quot;)
</code></pre>
<h3><a class="header" href="#animatedsprite" id="animatedsprite">AnimatedSprite</a></h3>
<p><img src="godot/./dot/animated-sprite.svg" alt="" /></p>
<h1><a class="header" href="#tokio" id="tokio">tokio</a></h1>
<h1><a class="header" href="#executor-1" id="executor-1">Executor</a></h1>
<p>Executor中主要有<code>Executor</code>, <code>TypedExecutor</code>, <code>enter</code>, <code>DefaultExecutor</code>, <code>Park</code></p>
<ol>
<li>
<p><code>Executor</code>, <code>TypedExecutor</code>主要作用是spawn future，转换为相应的任务，然后去执行该任务，不断的poll future,直到future complete.</p>
</li>
<li>
<p><code>DefaultExecutor</code>作用，是将<code>tokio::spawn</code>的future转给当前默认的executor.</p>
</li>
<li>
<p><code>enter</code>　用于阻止在当前executor context中，再start一个executor</p>
</li>
<li>
<p><code>park</code>　是对线程<code>block/unblock</code>操作的抽象.</p>
</li>
</ol>
<p>原文如下（摘自tokio-executor/src/lib.rs)</p>
<ul>
<li>
<p>The [<code>Executor</code>] trait spawns future object onto an executor.</p>
</li>
<li>
<p>The [<code>TypedExecutor</code>] trait spawns futures of a specific type onto an
executor. This is used to be generic over executors that spawn futures
that are either <code>Send</code> or <code>!Send</code> or implement executors that apply to
specific futures.</p>
</li>
<li>
<p>[<code>enter</code>] marks that the current thread is entering an execution
context. This prevents a second executor from accidentally starting from
within the context of one that is already running.</p>
</li>
<li>
<p>[<code>DefaultExecutor</code>] spawns tasks onto the default executor for the current
context.</p>
</li>
<li>
<p>[<code>Park</code>] abstracts over blocking and unblocking the current thread.</p>
</li>
</ul>
<h2><a class="header" href="#executor-impl" id="executor-impl">Executor impl</a></h2>
<p>实现Executor接口的主要有current thread，task executor, default executor还有thread pool的executor.</p>
<p><img src="tokio/./executor.svg" alt="executor" /></p>
<h3><a class="header" href="#defaultexecutor" id="defaultexecutor">DefaultExecutor</a></h3>
<p>DefaultExecutor 扮演了入口的角色，会将spawn调用转发给thread local storage var的Executor;
<img src="tokio/./default-executor.svg" alt="default-executor" /></p>
<h3><a class="header" href="#current-thread" id="current-thread">current thread</a></h3>
<p>current thread executor 是单线程的executor。task spwan和execute是在同一线程上完成的。</p>
<p>代码中Entered和Borrow的作用是干啥的不太明白，感觉这块代码有点绕.</p>
<p>Entered和Borrow定义如下:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// A `CurrentThread` instance bound to a supplied execution context.
pub struct Entered&lt;'a, P: Park&gt; {
    executor: &amp;'a mut CurrentThread&lt;P&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>/// This is mostly split out to make the borrow checker happy.
struct Borrow&lt;'a, U&gt; {
    id: u64,
    scheduler: &amp;'a mut Scheduler&lt;U&gt;,
    num_futures: &amp;'a atomic::AtomicUsize,
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./current-thread-executor.svg" alt="current-thread-executor" /></p>
<h3><a class="header" href="#thread-pool-sender" id="thread-pool-sender">thread pool sender</a></h3>
<p>thread pool的sender使用future创建相应的task, 然后调用pool的<code>submit_external</code>提交任务</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    fn spawn(
        &amp;mut self,
        future: Pin&lt;Box&lt;dyn Future&lt;Output = ()&gt; + Send&gt;&gt;,
    ) -&gt; Result&lt;(), SpawnError&gt; {
        self.prepare_for_spawn()?;

        // At this point, the pool has accepted the future, so schedule it for
        // execution.

        // Create a new task for the future
        let task = Arc::new(Task::new(future));

        // Call `submit_external()` in order to place the task into the global
        // queue. This way all workers have equal chance of running this task,
        // which means IO handles will be assigned to reactors more evenly.
        self.pool.submit_external(task, &amp;self.pool);

        Ok(())
    }
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./thread_pool_sender.svg" alt="thread-pool-sender" /></p>
<h2><a class="header" href="#executor-setup" id="executor-setup">Executor setup</a></h2>
<p>thread local var <code>EXECUTOR</code>的设置过程</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>thread_local! {
    /// Thread-local tracking the current executor
    static EXECUTOR: Cell&lt;State&gt; = Cell::new(State::Empty)
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./executor-setup.svg" alt="executor-setup" /></p>
<p>在调用<code>tokio::spawn</code>时，会通过DefaultExecutor调用相应的Thread local storage中设置好的Executor</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>//tokio-executor/src/global.rs
pub fn spawn&lt;T&gt;(future: T)
where
    T: Future&lt;Output = ()&gt; + Send + 'static,
{
    DefaultExecutor::current().spawn(Box::pin(future)).unwrap()
}
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>//tokio-executor/src/global.rs
impl DefaultExecutor {
    #[inline]
    fn with_current&lt;F: FnOnce(&amp;mut dyn Executor) -&gt; R, R&gt;(f: F) -&gt; Option&lt;R&gt; {
        EXECUTOR.with(
            |current_executor| match current_executor.replace(State::Active) {
                State::Ready(executor_ptr) =&gt; {
                    let executor = unsafe { &amp;mut *executor_ptr };
                    let result = f(executor);
                    current_executor.set(State::Ready(executor_ptr));
                    Some(result)
                }
                State::Empty | State::Active =&gt; None,
            },
        )
    }
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#park" id="park">park</a></h1>
<p>park是对当前线程block和unblock操作的抽象, 和std的park/unpark操作来比，在线程被blocked的时候，可以去调用一些定制化的功能。</p>
<h2><a class="header" href="#park-impl" id="park-impl">Park impl</a></h2>
<p><img src="tokio/./park.svg" alt="park" /></p>
<h3><a class="header" href="#reactor-park" id="reactor-park">Reactor Park</a></h3>
<p>Reactor 相关数据结构如下, 
<img src="tokio/./reactor-park-struct.svg" alt="reactor-park-struct" /></p>
<p>Par接口的park/unpark操作主要依赖于mio的poll和SetReadness。
<img src="tokio/./reactor-park.svg" alt="reactor-park" /></p>
<h3><a class="header" href="#thread-pool-default-park" id="thread-pool-default-park">Thread pool default park</a></h3>
<p>线程池的default park主要依赖于croess beam的park和unpark</p>
<p><img src="tokio/./threadpool_default_park.svg" alt="threadpool_default_park" /></p>
<h3><a class="header" href="#parkthread" id="parkthread">ParkThread</a></h3>
<p>数据结构之间关系</p>
<p><img src="tokio/./park-thread-struct.svg" alt="park-thread-struct" /></p>
<p>接口调用关系</p>
<p><img src="tokio/./park-thread.svg" alt="park-thread" /></p>
<h1><a class="header" href="#tokio-thread-pool" id="tokio-thread-pool">tokio thread pool</a></h1>
<h2><a class="header" href="#schedule-1" id="schedule-1">schedule</a></h2>
<p>tokio 使用了crossbeam中的Queue, Stealer, Worker等来实现线程池，其中觉得有意思的地方时work stealing策略</p>
<p>每个task被分给worker的过程如下：有个pool.queue作为全局的task队列入口每次spawn task都会将task push到pool.queue中</p>
<p>worker run函数取task的逻辑如下：</p>
<ol>
<li>从自己的worker队列中去取任务.</li>
<li>如果自己队列中没任务，则从全局队列中，获取一批任务。</li>
<li>如果全局队列中也没任务，则随机的从其他的worker中steal一批任务。</li>
</ol>
<p>这样做的好处是，降低对全局队列的频繁加锁等操作，而且有steal机制，使得task可以比较均匀的被调度。</p>
<h3><a class="header" href="#task-spawn" id="task-spawn">task spawn</a></h3>
<p>task 从spawn到最后run的过程：</p>
<p><img src="tokio/./worker-steal.svg" alt="worker-steal" /></p>
<h3><a class="header" href="#task-wake" id="task-wake">task wake</a></h3>
<p><img src="tokio/./task-wake.svg" alt="task-wake" /></p>
<h3><a class="header" href="#worker-sleep" id="worker-sleep">worker sleep</a></h3>
<p>worker在sleep时候，会把自己push到pool的sleep_stack上, entry中的park/unpark负责线程的sleep和wake.</p>
<p><img src="tokio/./worker-sleep.svg" alt="worker sleep" /></p>
<h3><a class="header" href="#worker-run" id="worker-run">worker run</a></h3>
<p><img src="tokio/./worker-run.svg" alt="worker-run" /></p>
<h1><a class="header" href="#tokio-driver" id="tokio-driver">tokio driver</a></h1>
<p>Driver 简单来说，就是io event事件触发后，找到相应等待的task, 然后调用预设好的回调函数.</p>
<p>tokio中事件驱动主要靠<code>mio::poll</code>, 在像mio::register中注册一个event时，会带上一个token(token是在tokio中生成的）, driver根据该token建立到SchduleIo的映射，event触发的时候，就会调用schedulIo中预先定义好的方法。
然后事件被触发的时候,mio会把这个token带过来。</p>
<h2><a class="header" href="#task---mio-event" id="task---mio-event">task &lt;-&gt; mio event</a></h2>
<p>task和mio event通过token 建立关系，回调函数waker通过过Context包装, 传递给future的poll函数，当future需要等待某个事件时候，就会把事件和context关联起来。然后等事件被触发了，就调用context中预先设置好的waker.</p>
<p><img src="tokio/./task-token-event.svg" alt="task-token-event" /></p>
<h2><a class="header" href="#主要数据结构" id="主要数据结构">主要数据结构</a></h2>
<p><img src="tokio/./reactor-park-struct.svg" alt="reactor-park-struct" /></p>
<p><code>reactor::inner</code>中的<code>io_dispatch</code>表，用于记录事件token到ScheduleIO的一个映射关系.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>//reactor.rs
pub(super) struct Inner {
    /// The underlying system event queue.
    io: mio::Poll,

    /// ABA guard counter
    next_aba_guard: AtomicUsize,

    /// Dispatch slabs for I/O and futures events
    pub(super) io_dispatch: RwLock&lt;Slab&lt;ScheduledIo&gt;&gt;,

    /// Used to wake up the reactor from a call to `turn`
    wakeup: mio::SetReadiness,
}

<span class="boring">}
</span></code></pre></pre>
<p>ScheduledIo, 主要用于指向context </p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub(super) struct ScheduledIo {
    aba_guard: usize,
    pub(super) readiness: AtomicUsize,
    pub(super) reader: AtomicWaker,
    pub(super) writer: AtomicWaker,
}

<span class="boring">}
</span></code></pre></pre>
<p>Context 中的waker则定义了如何唤醒task, 对于threadpool 会去调用Task::Schedule方法，而对于current thread, 则会去调用Node.Notify</p>
<h2><a class="header" href="#context-注册过程" id="context-注册过程">context 注册过程</a></h2>
<p>首先ctx会在task run时候，被创建，然后传递给future_poll, 经过层层的poll_ready 之类的，注册到<code>Reactor::inner::io_dipatch</code>表中
注册的key会在<code>Reactor::inner::add_source</code>计算出来，然后传递给mio的register函数。</p>
<p>然后mio的poll函数在事件发生时，会将该token带上，在Reactor::dispatch中根据token找到相应的contex waker, 调用对应的wake函数。</p>
<h3><a class="header" href="#thread-pool-中-ctx-waker的创建" id="thread-pool-中-ctx-waker的创建">thread pool 中 ctx waker的创建</a></h3>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>//threadpool/task/mod.rs
    pub(crate) fn run(me: &amp;Arc&lt;Task&gt;, pool: &amp;Arc&lt;Pool&gt;) -&gt; Run {
    //...
            let waker = task::waker(Arc::new(Waker {
                task: me.clone(),
                pool: pool.clone(),
            }));

            let mut cx = Context::from_waker(&amp;waker);
    //...
    }
<span class="boring">}
</span></code></pre></pre>
<p>其中Waker定义如下, event经过dispatch 后, 最终会调用Task::Schedule.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// threadpool/waker.rs
impl ArcWake for Waker {
    fn wake_by_ref(me: &amp;Arc&lt;Self&gt;) {
        Task::schedule(&amp;me.task, &amp;me.pool);
    }
}
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#current-thread中ctx-waker的创建" id="current-thread中ctx-waker的创建">current thread中ctx waker的创建</a></h3>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    pub fn block_on&lt;F&gt;(&amp;mut self, mut future: F) -&gt; F::Output
    where
        F: Future,
    {
        // Safety: we shadow the original `future`, so it will never move
        // again.
        let mut future = unsafe { Pin::new_unchecked(&amp;mut future) };
        let waker = self.executor.scheduler.waker();
        let mut cx = Context::from_waker(&amp;waker);
        // ... other code
    }
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#event-token-scheduleio" id="event-token-scheduleio">event, token, scheduleIO</a></h3>
<p>tokio中通过token将event和scheduleIO关联起来</p>
<h4><a class="header" href="#token到scheduleio" id="token到scheduleio">token到ScheduleIO</a></h4>
<p>在<code>reactor::inner::add_source</code>中, 会在<code>io_dispatch</code>表中先创建一个ScheduleIO， key为aba_guard, 使用aba_guard计算出一个token, 
最后通过调用mio.register 将token和event关联起来, 这样就建立了ScheduleIO和event之间的关系.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// tokio-net/src/driver/reactor.rs
    pub(super) fn add_source(&amp;self, source: &amp;dyn Evented) -&gt; io::Result&lt;usize&gt; {
        // Get an ABA guard value
        let aba_guard = self.next_aba_guard.fetch_add(1 &lt;&lt; TOKEN_SHIFT, Relaxed);

        let key = {
            // Block to contain the write lock
            let mut io_dispatch = self.io_dispatch.write();

            if io_dispatch.len() == MAX_SOURCES {
                return Err(io::Error::new(
                    io::ErrorKind::Other,
                    &quot;reactor at max \
                     registered I/O resources&quot;,
                ));
            }

            io_dispatch.insert(ScheduledIo {
                aba_guard,
                readiness: AtomicUsize::new(0),
                reader: AtomicWaker::new(),
                writer: AtomicWaker::new(),
            })
        };

        let token = aba_guard | key;
        debug!(&quot;adding I/O source: {}&quot;, token);

        self.io.register(
            source,
            mio::Token(token),
            mio::Ready::all(),
            mio::PollOpt::edge(),
        )?;

        Ok(key)
    }
<span class="boring">}
</span></code></pre></pre>
<h4><a class="header" href="#scheduledio-到context" id="scheduledio-到context">ScheduledIo 到context,</a></h4>
<p>主要在Registration::inner::register中完成.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span>
<span class="boring">fn main() {
</span>    pub(super) fn register(&amp;self, token: usize, dir: Direction, w: Waker) {
        debug!(&quot;scheduling {:?} for: {}&quot;, dir, token);
        let io_dispatch = self.io_dispatch.read();
        let sched = io_dispatch.get(token).unwrap();

        let (waker, ready) = match dir {
            Direction::Read =&gt; (&amp;sched.reader, !mio::Ready::writable()),
            Direction::Write =&gt; (&amp;sched.writer, mio::Ready::writable()),
        };

        waker.register(w);

        if sched.readiness.load(SeqCst) &amp; ready.as_usize() != 0 {
            waker.wake();
        }
    }
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./task-event-detail.svg" alt="task-event-detail" /></p>
<h3><a class="header" href="#事件分发dispatch" id="事件分发dispatch">事件分发：dispatch</a></h3>
<p><code>reactor::poll</code>调用<code>mio::poll</code>来轮询是否有事件发生，如果有事件发生，则从mio的event中取出token,</p>
<p>然后调动dispatch, 调用相应的wake函数</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>//tokio-net/src/driver/reactor.rs

    #[cfg_attr(feature = &quot;tracing&quot;, tracing::instrument(level = &quot;debug&quot;))]
    fn poll(&amp;mut self, max_wait: Option&lt;Duration&gt;) -&gt; io::Result&lt;()&gt; {
        // Block waiting for an event to happen, peeling out how many events
        // happened.
        match self.inner.io.poll(&amp;mut self.events, max_wait) {
            Ok(_) =&gt; {}
            Err(e) =&gt; return Err(e),
        }

        // Process all the events that came in, dispatching appropriately

        // event count is only used for  tracing instrumentation.
        #[cfg(feature = &quot;tracing&quot;)]
        let mut events = 0;

        for event in self.events.iter() {
            #[cfg(feature = &quot;tracing&quot;)]
            {
                events += 1;
            }
            let token = event.token();
            trace!(event.readiness = ?event.readiness(), event.token = ?token);

            if token == TOKEN_WAKEUP {
                self.inner
                    .wakeup
                    .set_readiness(mio::Ready::empty())
                    .unwrap();
            } else {
                self.dispatch(token, event.readiness());
            }
        }

        trace!(message = &quot;loop process&quot;, events);

        Ok(())
    }
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>    fn dispatch(&amp;self, token: mio::Token, ready: mio::Ready) {
        let aba_guard = token.0 &amp; !MAX_SOURCES;
        let token = token.0 &amp; MAX_SOURCES;

        let mut rd = None;
        let mut wr = None;

        // Create a scope to ensure that notifying the tasks stays out of the
        // lock's critical section.
        {
            let io_dispatch = self.inner.io_dispatch.read();

            let io = match io_dispatch.get(token) {
                Some(io) =&gt; io,
                None =&gt; return,
            };

            if aba_guard != io.aba_guard {
                return;
            }

            io.readiness.fetch_or(ready.as_usize(), Relaxed);

            if ready.is_writable() || platform::is_hup(ready) {
                wr = io.writer.take_waker();
            }

            if !(ready &amp; (!mio::Ready::writable())).is_empty() {
                rd = io.reader.take_waker();
            }
        }

        if let Some(w) = rd {
            w.wake();
        }

        if let Some(w) = wr {
            w.wake();
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h1><a class="header" href="#tokio-io" id="tokio-io">tokio io</a></h1>
<p>Core I/O abstractions for the Tokio stack.</p>
<p>AsyncRead/AsyncWrite use nonblock IO</p>
<p><strong>non-blocking</strong>. All non-blocking I/O objects must return an error when
bytes are unavailable instead of blocking the current thread.</p>
<p>Would block error to future Not Ready poll</p>
<h2><a class="header" href="#asyncread" id="asyncread">AsyncRead</a></h2>
<ul>
<li><code>poll_read</code>: Attempt to read from the <code>AsyncRead</code> into <code>buf</code>.</li>
<li><code>poll_read_buf</code>: Pull some bytes from this source into the specified <code>BufMut</code>, returning how many bytes were read.</li>
</ul>
<p>AsyncReadExt An extension trait which adds utility methods to <code>AsyncRead</code> types.</p>
<p>This trait inherits from std::io::Read and indicates that an I/O object is non-blocking. All non-blocking I/O objects must return an error when bytes are unavailable instead of blocking the current thread.</p>
<p><img src="tokio/./async_read.svg" alt="async_read" /></p>
<h2><a class="header" href="#asyncwrite" id="asyncwrite">AsyncWrite</a></h2>
<ul>
<li><code>poll_write</code>:  Attempt to write bytes from <code>buf</code> into the object.</li>
<li><code>poll_write_buf</code>: Write a <code>Buf</code> into this value, returning how many bytes were written.</li>
<li><code>poll_flush</code>: Attempt to flush the object, ensuring that any buffered data reach their destination.</li>
<li><code>poll_shutdown</code>: Initiates or attempts to shut down this writer, returning success when the I/O connection has completely shut down.</li>
</ul>
<p><img src="tokio/./async_write.svg" alt="asycn_write" /></p>
<h3><a class="header" href="#tcp-stream" id="tcp-stream">tcp stream</a></h3>
<p>/// An I/O object representing a TCP stream connected to a remote endpoint.</p>
<p><img src="tokio/./tcp_stream_struct.svg" alt="tcp_stream_struct" /></p>
<p><img src="tokio/./tcp_stream.svg" alt="tcp_stream" /></p>
<h2><a class="header" href="#split" id="split">Split</a></h2>
<p>Split a single value implementing <code>AsyncRead + AsyncWrite</code> into separate
<code>AsyncRead</code> and <code>AsyncWrite</code> handles. 还不是太明白这个地方为啥需要lock ?类似与rw lock？</p>
<p>将一个stream分为reader, write部分，解决需要两次mut引用问题（src, dst)</p>
<p><img src="tokio/./split.svg" alt="split" /></p>
<p>调用<code>poll_read</code>, <code>poll_write</code>都会调用<code>poll_lock</code>, 此处的<code>poll_lock</code>并不会block线程。类似于spin lock。</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>// 类似与Spin Lock.
impl&lt;T&gt; Inner&lt;T&gt; {
    fn poll_lock(&amp;self, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Guard&lt;'_, T&gt;&gt; {
        if !self.locked.compare_and_swap(false, true, Acquire) {
            Poll::Ready(Guard { inner: self })
        } else {
            // Spin... but investigate a better strategy

            ::std::thread::yield_now();
            cx.waker().wake_by_ref();

            Poll::Pending
        }
    }
}

// 用于Mutex 
impl&lt;T&gt; Guard&lt;'_, T&gt; {
    fn stream_pin(&amp;mut self) -&gt; Pin&lt;&amp;mut T&gt; {
        // safety: the stream is pinned in `Arc` and the `Guard` ensures mutual
        // exclusion.
        unsafe { Pin::new_unchecked(&amp;mut *self.inner.stream.get()) }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2><a class="header" href="#copy" id="copy">Copy</a></h2>
<p>future copy实现了从reader异步write到write逻辑</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>impl&lt;R, W&gt; Future for Copy&lt;'_, R, W&gt;
where
    R: AsyncRead + Unpin + ?Sized,
    W: AsyncWrite + Unpin + ?Sized,
{
    type Output = io::Result&lt;u64&gt;;

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;io::Result&lt;u64&gt;&gt; {
        loop {
            // If our buffer is empty, then we need to read some data to
            // continue.
            if self.pos == self.cap &amp;&amp; !self.read_done {
                let me = &amp;mut *self;
                // 从reader中异步读取n个字节
                let n = ready!(Pin::new(&amp;mut *me.reader).poll_read(cx, &amp;mut me.buf))?;
                if n == 0 {
                    self.read_done = true;
                } else {
                    self.pos = 0;
                    self.cap = n;
                }
            }

            // If our buffer has some data, let's write it out!
            while self.pos &lt; self.cap {
                let me = &amp;mut *self;
                // 异步写n个字节到writer中
                let i = ready!(Pin::new(&amp;mut *me.writer).poll_write(cx, &amp;me.buf[me.pos..me.cap]))?;
                if i == 0 {
                    return Poll::Ready(Err(io::Error::new(
                        io::ErrorKind::WriteZero,
                        &quot;write zero byte into writer&quot;,
                    )));
                } else {
                    self.pos += i;
                    self.amt += i as u64;
                }
            }

            // If we've written al the data and we've seen EOF, flush out the
            // data and finish the transfer.
            // done with the entire transfer.
            if self.pos == self.cap &amp;&amp; self.read_done {
                let me = &amp;mut *self;
                // 最后写完了等待flush
                ready!(Pin::new(&amp;mut *me.writer).poll_flush(cx))?;
                return Poll::Ready(Ok(self.amt));
            }
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#buf-readerwritersream" id="buf-readerwritersream">buf reader/writer/sream</a></h3>
<h1><a class="header" href="#codec" id="codec">codec</a></h1>
<h2><a class="header" href="#transport" id="transport">Transport</a></h2>
<p>Codec</p>
<p>This is often known as “framing”: instead of viewing your connections as consisting of just bytes in/bytes out, you view them as “frames” of application data that are received and sent. A framed stream of bytes is often referred to as a “transport”.</p>
<h2><a class="header" href="#encodedecode-trait" id="encodedecode-trait">Encode/Decode Trait</a></h2>
<p>有点像序列化和反序列化</p>
<p>Encoder</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub trait Encoder {
    /// The type of items consumed by the `Encoder`
    type Item;

    /// The type of encoding errors.
    ///
    /// `FramedWrite` requires `Encoder`s errors to implement `From&lt;io::Error&gt;`
    /// in the interest letting it return `Error`s directly.
    type Error: From&lt;io::Error&gt;;

    /// Encodes a frame into the buffer provided.
    ///
    /// This method will encode `item` into the byte buffer provided by `dst`.
    /// The `dst` provided is an internal buffer of the `Framed` instance and
    /// will be written out when possible.
    fn encode(&amp;mut self, item: Self::Item, dst: &amp;mut BytesMut) -&gt; Result&lt;(), Self::Error&gt;;
}
<span class="boring">}
</span></code></pre></pre>
<p>Decoder</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>pub trait Decoder {
    type Item;
    type Error: From&lt;io::Error&gt;;
    fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;Self::Item&gt;, Self::Error&gt;;
    fn decode(&amp;mut self, src: &amp;mut BytesMut) -&gt; Result&lt;Option&lt;Self::Item&gt;, Self::Error&gt;;
    fn framed&lt;T: AsyncRead + AsyncWrite + Sized&gt;(self, io: T) -&gt; Framed&lt;T, Self&gt;
}
<span class="boring">}
</span></code></pre></pre>
<p><img src="tokio/./frame_trait.svg" alt="frame-trait" /></p>
<h2><a class="header" href="#framed" id="framed">framed</a></h2>
<p>frame write</p>
<p><img src="tokio/./frame_write.svg" alt="frame-write" /></p>
<h1><a class="header" href="#channel" id="channel">channel</a></h1>
<p>multi producer and single consumer for sendin values between tasks;</p>
<h2><a class="header" href="#data-struct" id="data-struct">data struct</a></h2>
<p><img src="tokio/./channel.svg" alt="channel" /></p>
<h2><a class="header" href="#function-call" id="function-call">function call</a></h2>
<p>函数调用</p>
<p><img src="tokio/./channel-call.svg" alt="channel-call" /></p>
<h1><a class="header" href="#task-waker" id="task-waker">task waker</a></h1>
<p><img src="tokio/./task-waker.svg" alt="task-waker" /></p>
<h2><a class="header" href="#atomic-waker" id="atomic-waker">atomic waker</a></h2>
<p><code>AtomicWaker</code> is a multi-consumer, single-producer transfer cell. The cell
stores a <code>Waker</code> value produced by calls to <code>register</code> and many threads can
race to take the waker by calling <code>wake</code>.</p>
<p>Because of this, the task will do one of two things.</p>
<ol>
<li>
<p>Observe the application state change that Thread B is waking on. In
this case, it is OK for Thread B's wake to be lost.</p>
</li>
<li>
<p>Call register before attempting to observe the application state. Since
Thread A still holds the <code>wake</code> lock, the call to <code>register</code> will result
in the task waking itself and get scheduled again.</p>
</li>
</ol>
<p><img src="tokio/./atomic-waker-state.svg" alt="atomic-waker-state" /></p>
<h1><a class="header" href="#python" id="python">python</a></h1>
<h1><a class="header" href="#records" id="records">records</a></h1>
<p>[github records]https://github.com/kennethreitz-archive/records</p>
<pre><code class="language-python">import records

db = records.Database('postgres://...')
rows = db.query('select * from active_users')    # or db.query_file('sqls/active-users.sql')
</code></pre>
<p><img src="python/records/./dot/records.svg" alt="records" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
